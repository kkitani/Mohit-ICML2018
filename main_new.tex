%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CCP-IRL}

% HACK REMOVE
%\usepackage[showframe]{geometry}% http://ctan.org/pkg/geometry

\usepackage[utf8]{inputenc}
\usepackage{authblk}

\usepackage{icml2018}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath,amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algorithmic}
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx

%PDF Info Is Required:
\icmltitlerunning{CCP-IRL}

% \author[1]{Mohit Sharma}
% \author[1]{Kris M. Kitani}
% \affil[1]{Robotics Institute, Carnegie Mellon University}
% \affil[1]{\texttt{\{mohits1, kkitani\}@cs.cmu.edu}}
% \author[]{Joachim Groeger}
% %\affil[2]{Amazon.com}
% \affil[2]{\texttt{jrg@joachimgroeger.com}}


% \pdfinfo{
% /Title (Inverse Reinforcement Learning with Conditional Choice Probabilities)
% /Author (
%     Mohit Sharma, 
%     %\texttt{mohits1@andrew.cmu.edu}
%     %\and
%     Kris M. Kitani, 
%     %\texttt{kkitani@cs.cmu.edu}
%     %\and
%     Joachim Groeger
%     %\texttt{joachimgroeger@gmail.com})
% }

% === Kris' Macros === %
\usepackage{mathtools}
\usepackage{bm}
\renewcommand{\vec}[1]{\mbox{\bm{$#1$}}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}

% === Mohit's Macros === %
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\def\MSHangBox#1{%^
\begin{minipage}[t]{\textwidth}% Top-hanging minipage, will align on
			       % bottom of first line
\begin{tabbing} % tabbing so that minipage shrinks to fit
~\\[-\baselineskip] % Make first line zero-height
#1 % Include user's text
\end{tabbing}%^
\end{minipage}} % can't allow } onto next line, as {WIDEBOX}~x will not tie.

\usepackage{color}
\definecolor{purple}{rgb}{0.58,0,0.83}

\begin{document}

\twocolumn[
\icmltitle{Inverse Reinforcement Learning with \\ Conditional Choice Probabilities}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{}
\icmlaffiliation{goo}{}
\icmlaffiliation{ed}{}

\icmlcorrespondingauthor{}{}
\icmlcorrespondingauthor{}{}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We make an important connection to existing results in econometrics to describe an alternative formulation of inverse reinforcement learning (IRL). In particular, we describe an algorithm using Conditional Choice Probabilities (CCP), which are maximum likelihood estimates of the policy estimated from expert demonstrations, to solve the IRL problem. Using the language of structural econometrics, we re-frame the optimal decision problem and introduce an alternative representation of value functions due to \cite{hotz}. In addition to presenting the theoretical connections that bridge the IRL literature between Economics and Robotics, the use of CCPs also has the practical benefit of reducing the computational cost of solving the IRL problem. Specifically, under the CCP representation, we show how one can avoid repeated calls to the dynamic programming subroutine typically used in IRL. 
%In the most general case, we show that it is possible to uniquely estimate the optimal reward function, with no parametric restrictions.
%We also derive a version of the CCP-IRL algorithm that can directly estimate a parametric form of the reward function.
We show via extensive experimentation on standard IRL benchmarks that CCP-IRL is able to outperform MaxEnt-IRL, with as much as a 5x speedup and without compromising on the quality of the recovered reward function.

\end{abstract} 

\section{Introduction}

The problem of extracting the reward function of a task given observed optimal behavior has been studied in parallel in both robotics and economics. In robotics this literature is collected under the heading "Inverse Reinforcement Learning" (IRL), \cite{Ng2000, abbeel2004apprenticeship}. The aim here is to learn a reward function that best explains demonstrations of expert behavior so that a robotic system can reproduce expert like behavior. Alternatively, in economics it is referred to as "structural econometrics" \cite{miller, pakes, rust_gmc} and is used to help economists better understand human decision making. Both of the fields are similar in that both seek to uncover a \emph{latent} reward function of an underlying Markov Decision Process (MDP).
Although both fields developed in parallel there has been very limited knowledge transfer between them. To alleviate this we make the connection between these two fields more explicit. We uncover different problem formulations which give rise to similar algorithms. Specifically, we show how the softmax value function in \cite{ziebart} is exactly similar to the ex-ante value function in \cite{rust_gmc} under Type-1 extremum assumption.


Additionally, one of the main challenges in IRL is the large computational complexity of current state of the art algorithms \cite{ziebart, Ratliff2006}. 
To infer the reward function of the underlying MDP, we need to \textit{repeatedly} solve this MDP at every step of a reward parameter optimization scheme.
% This is because in IRL we infer the reward function of the MDP by \textit{repeated} computation of the MDP solution for every loop of a reward parameter optimization scheme.
The MDP solution, which is characterized by a value function, requires a computationally expensive Dynamic Programming (DP) procedure. Unfortunately, solving this DP step repeatedly makes IRL algorithms computationally prohibitive.

% This problem of large computational complexity has also been studied in economics, with the ensuing literature collected under the name "Conditional Choice Probabilities".
This problem of large computational complexity has also been studied in economics \cite{hotz, su2012constrained, aguirregabiria2002swapping}.
Among the many works, Conditional Choice Probability (CCP) estimators \cite{hotz} are particularly interesting because of their computational efficiency.
CCP estimators use CCP values to estimate the reward function of the MDP.
The CCP values specify the optimal action for a state and are estimated from expert demonstrations.
These estimators are computationally efficient since they avoid the repeated computation of the DP step by using an alternative representation of the MDP's value function. 

%To avoid the repeated computation of the MDP, \cite{hotz} introduced an alternative representation of the value function such that it can be solved in closed form through the use of conditional choice probabilities (CCP).
% In this paper we leverage results from \cite{rust_gmc, hotz, magnac} to formulate an estimation routine for the reward function with CCPs, that avoids repeated calls to the solver of the full dynamic decision problem. The key insight from \cite{hotz} is that differences in current reward and future values between actions can be calculated from CCPs. This allows us to express future value functions in terms of difference value functions and therefore CCPs. Since CCPs are directly observed in the data, we can use this function representation to estimate the value function of the MDP at each step of the optimization process without solving the expensive dynamic programming (DP) formulation. This results in an algorithm whose overall computational time is comparable to a single MDP computation of a traditional gradient-based IRL method. 

In this work we introduce Conditional Choice Probability Inverse Reinforcement Learning (CCP-IRL) by incorporating CCPs into IRL. We leverage results from \cite{rust_gmc, hotz, magnac} to formulate an estimation routine for the reward function with CCPs, that avoids repeated calls to the solver of the full dynamic decision problem. We test the CCP-IRL algorithm on multiple different IRL benchmarks and compare the results to the state of the art IRL algorithm, MaxEnt-IRL \cite{ziebart}. We show that with CCP-IRL we can achieve up to 5$\times$ speedup without affecting the quality of the inferred reward function. Further, this speedup holds across large state spaces and increases for complex problems, such as, problems where value iteration takes much longer to converge. 

To summarize, in this work we make an important connection between economics and robotics to uncover different problem formulations that result in similar algorithms. We then use this connection to leverage the literature of conditional choice probabilities. For this, we propose a new IRL algorithm called CCP-IRL. We test the CCP-IRL algorithm on multiple different IRL benchmarks and show large speedup. We also discuss how the CCP approach is applicable to other important areas in IRL research.
Overall, we believe by explicitly pointing out the similarities and differences in the two fields, will help researchers better leverage results from the other field. We hope this will lead to more knowledge transfer between them.

% ==== OLD INTRO ====


% The field of Inverse Reinforcement Learning (IRL) \cite{Ng2000} \cite{abbeel2004apprenticeship} in robotics or computer vision is concerned with the the problem of learning a task given expert demonstrations for it. IRL solves this problem by finding a reward function under which the demonstrated trajectories are optimal. However, unfortunately most of the IRL algorithms are very expensive to run since they require solving a MDP problem in every loop of their iteration.

% IRL has also been a topic of study in economics for a number of decades under the heading of "structural econometrics" \cite{miller}, \cite{pakes} and \cite{rust_gmc}. Structural econometrics differs little from IRL in robotics or computer vision, in that both seek to uncover a latent reward function to either understand, forecast, anticipate or mimic behavior. The key conceptual departure in the structural econometrics literature is that the modeller allows for random action-dependent reward, which is observed by the demonstrator prior to taking an action but unobserved by us. 
% In particular, a random reward shock is introduced that affects current choices but has no future effect other than through current action. The rationale for this is to ensure that the model is not immediately contradicted by the data.
%In a standard Dynamic Discrete Choice model (a MDP with action shocks) the policy is a usually a deterministic function of the state.
%This implies that if we observed two experts' behaviors, if both were characterized by identical state variables, they should take the same action. This rarely happens in real behavioral data and the additive shock allows for the possibility of an unobserved factor to determine choice. 
% The introduction of shocks means that policies can now not be directly observed in the data. However, it is still possible to measure integrated policy functions, \emph{i.e.}, policy functions that are conditional on the observable state averaged across the unobserved shock. These integrated policy functions yield a probability of selecting an action conditional on a state.
% We refer to these probabilities as conditional choice probabilities or CCPs.
%From the perspective of the expert, policies are deterministic, however from the outside observer they are random.

% ==== Related Work ====
% Attempt to connect the different strategies for IOC to put CCP in a new box
%The task of IOC or IRL is to infer the \emph{reward function}, usually of an underlying MDP, from a set of demonstrated behavior. The key differences across methods are in their choice of objective function for the MDP/R model and the behaviors observed in the data. Most techniques work with conditional feature expectation functions \emph{e.g.} \cite{abbeel} use the feature matching technique. However, a major drawback of direct feature matching techniques is the non-uniqueness of the inversion from trajectories to the reward function. Alternatively, \cite{ziebart} consider a metric based on an empirical analogue of the Kullback-Leibler deviation, whose formulation leads to a unique solution under the maximum entropy criteria. The method performs gradient descent on the parameters of a linearized reward function and requires the complete MDP solution for each descent step. Instead of matching feature counts, \cite{Ratliff2006} attempt to maximize the difference between the reward (a function of feature counts) of an expert trajectory versus that of all other trajectories using a max-margin framework. However, the number of constraints (potentially infinite) that must be evaluated is large.
% and requires the use of cutting plane or (sub)gradient methods to solve. One can also attempt to directly match \emph{trajectories} \cite{mombaur2009identifying} instead of features. Such an approach results in a bi-level problem; one loop to improve reward parameters and another loop to solve the forward optimal control problem. 
%One can also attempt to match policies \cite{neu}. This approach also requires computing the complete solution MDP for every (natural) gradient step to improve the policy. 

% As we have noted, the core of most value-based IRL approaches is the \textit{repeated} computation of the MDP solution for every loop of a reward parameter optimization scheme. The solution of the MDP (\emph{i.e.}, the value function), is typically solved using the Bellman equations with dynamic programming (DP), known as the Value Iteration algorithm. Given the large cost of solving the DP problem at each step of optimization vastly limits the range of the above methods.

% In econometrics, to avoid the repeated computation of the value function, \cite{hotz} introduced an alternative representation of the value function such that it can be solved in closed form through the use of conditional choice probabilities (CCP). In this paper we leverage results from \cite{rust_gmc}, \cite{hotz} and \cite{magnac} to formulate an estimation routine with CCPs that (1) avoids repeated calls to the solver of the full dynamic decision problem and (2) ensures a unique mapping from trajectories to rewards. The key insight from \citeauthor{hotz} is that differences in current reward and future values between actions can be inverted from CCPs. This allows us to express future value functions in terms of difference value functions and therefore CCPs. Since CCPs are directly observed in the data we can use this function representation to estimate the value function at each step of the optimization process without needing to solve the expensive DP problem.

% This results in an IRL algorithm whose overall computational time is comparable to a single MDP computation of a traditional gradient-based IRL method. We test the above CCP-IRL algorithm on multiple different RL benchmarks and compare the results to state of the art IRL algorithm such as MaxEnt-IRL \cite{ziebart}. We show that with CCP-IRL we can get upto 5x speedup without affecting the quality of the inferred reward function. We also show that this speedup holds across large state spaces and increases for complex problems, such as, problems where the value iteration takes much longer to converge. 

% The econometric approach also provides a systematic method of determining under what conditions a unique mapping can be guaranteed from the observed distribution in the data to the reward function. For example, it can be shown that the reward function can only be estimated when fixing the discount factor and the distribution of the random reward shock.

% Another benefit of the CCP approach is when considering multi-agent settings where agents are strategic. In these settings there are generally multiple equilibria. As a result, it is not possible to guarantee that the numerical solution to the game will match the data. The CCP approach side-steps this issue since strategies are estimated directly from a single-path of play.

% Not sure how to weave this in... \cite{Levine2013} use second order methods to initialize many optimal policies (each conditioned on the initial state) and combine them to achieve a good global policy.

%We can construct different metrics in order to match the data. For example, we could directly look for parameters of the reward function that minimize the distance between the non-parametrically estimated CCPs and the CCPs implied by our model. Stack the CCPs for all actions and all states into vector $\mathbf{P}$ and stack the model implied CCPs in function $\Lambda(\mathbf{P};\theta)$ where $\theta$ are the reward parameters. Then we could find estimates $\hat{\theta}$ by
%\[
%\hat{\theta}=\arg\min_{\theta} [\mathbf{P}-\Lambda(\mathbf{P};\theta)]'[\mathbf{P}-\Lambda(\mathbf{P};\theta)]
%\]
%The solution to the above minimizes the Euclidean distance between the estimated and model-implied policies. This approach is similar in spirit to \cite{neu2007} who consider directly matching policy functions of experts. 
%One drawback however of their approach, is that it requires the complete solution MDP. \cite{neu2007} regress value functions onto actions and they use a Boltzman probability distribution, which leads to expressions for the probability distribution of actions similar to the CCPs derived under TIEV payoff shocks.

% Contribution


% Points to make: 
% (1) Structural Econometrics pre-dates IRL work, with the exception of Kalman 1964.
% (2) Main paragdigm in robotic learning is feature matching
% (3) Main paradigm in econ is ...


% Problem of non-uniqueness...
%The task of inverse optimal control (IOC) or inverse reinforcement learning (IRL) problem is to infer the reward function, usually of an underlying MDP, from a set of demonstrated behavior. The key differences across IRL or IOC methods are in their choice of distance metric between the MDP/R model and the behaviors observed in the data. Most techniques work with conditional feature expectation functions. \cite{abbeel2004apprenticeship} use the feature matching technique. However, a major drawback of existing IRL methods is the non-uniqueness of the inversion from trajectories to the reward function.

% The MaxEnt solution to IOC
%\cite*{ziebart} considers a metric based on an empirical analogue of the Kullback-Leibler deviation, whose formulation leads to a unique policy under the maximum entropy criteria. \cite{ziebart_phd} derives a mapping of Maximum Entropy (MaxEnt) methods into the space of Bellman-like operators. The method performs gradient descent on the parameters of a linearized reward function and requires that complete solution of the MDP for each descent iteration.



%\subsection{CCP IOC} % AKA our solution to everything!

% Our proposed method (need to be improved)
%In this paper, we provide specific conditions that ensure a unique reward function can be recovered. Interestingly, the MaxEnt Bellman updates are identical to the setup we describe with Type 1 Extreme Value (TIEV) payoff shocks. Our method is more general however, as any other distributional assumption on shocks leads to different Bellman updates.


 

%In this paper we leverage results from \cite{rust_gmc}, \cite{hotz} and \cite{magnac} to formulate an estimation routine with CCPs that resembles policy matching and that ensures a unique mapping from trajectories to rewards. Moreover, this mapping avoids the solution to the full dynamic decision problem. In particular, we can construct different metrics in order to match the data. For example, we could directly look for parameters of the reward function that minimize the distance between the non-parametrically estimated CCPs and the CCPs implied by our model. Stack the CCPs for all actions and all states into vector $\mathbf{P}$ and stack the model implied CCPs in function $\Lambda(\mathbf{P};\theta)$ where $\theta$ are the reward parameters. Then we could find estimates $\hat{\theta}$ by
%\[
%\hat{\theta}=\arg\min_{\theta} %[\mathbf{P}-\Lambda(\mathbf{P};\theta)]'[\mathbf{P}-\Lambda(\mathbf{P};\theta)]
%\]
%The solution to the above minimizes the Euclidean distance between the estimated and model-implied policies. This approach is similar in spirit to \cite{neu} who consider directly matching policy functions of experts. One drawback however of their approach, is that it requires the complete solution MDP. \cite{neu} regress value functions onto actions and they use a Boltzman probability distribution, which leads to expressions for the probability distribution of actions similar to the CCPs derived under TIEV payoff shocks. 


%\subsection{Related Work}

%General overview of IOC, stress the importance of R, cite computational cost when we want to know R. Behavior Cloning versus IOC.


%\subsection{Inverse Optimal Control}

%The classical inverse optimal control (IOC) or inverse reinforcement learning (IRL) problem assumes a known MDP excluding the reward function $r$ and a set of trajectories $\mathcal{D}$ from an expert who behaves optimally according to $r$. An IOC algorithm then searches for an $r$ which best describes the expert's observed behavior, according to some criterion function. 

%The major challenge facing IOC problems is there exists multiple reward functions which can induce the observed optimal trajectories (\cite{Ng2000}). For example, any set of trajectories is optimal for the all-zero rewards. To achieve a unique solution, \cite{Ng2000} proposed also maximizing the difference between the optimal and second-best action for every state. Likewise, \cite{ziebart} placed an entropy based prior over trajectories. Most value-based IOC approaches require that a recursive optimization is solved as a sub-routine, which can be intractable to solve for large or continuous state spaces.

%\textcolor{blue}{(KK: Don't forget to mention Max-Margin Planning...)}

%Alternatively, one may choose to forego searching for $r$ and instead directly look for a policy which mimics the observed behavior $\mathcal{D}$. \cite{abbeel} finds a policy which is guaranteed to perform well with the unobserved $r$.

%The line of work from \cite*{Ross2011}, \cite*{Ross2014} and \cite*{Sun2017} address the issue of different train and test trajectory distributions, by interleaving an action or cost-to-go oracle during training. \cite{Levine2013} use second order methods to initialize many optimal policies (each conditioned on the initial state) and combine them to achieve a good global policy.

%\subsection{Structural Analysis} % unsure of the exact econ term here for IOC...

%In the economics literature, \cite{rust_non_id} provided the same non-uniqueness result as \cite{Ng2000}. \cite{magnac} provide conditions that ensure a unique mapping, which we describe in detail in the main text.  \textcolor{red}{I suspect earlier economics papers also noticed this, does someone know the first?\textcolor{green}{added the firs non uniqueness result and references to magnac}}. They propose assuming a known reward for a single action in every state, a reasonable assumptions for problems where the ``non-action'' has zero or constant reward. They also address the game-theoretic problem of multiple agents acting optimally in a single MDP -- which describes many businesses and trading markets.
%\textcolor{green}{I am alright dropping references to games here.}





%==========================================================%
\section{IRL in Economics}

In this section, we first introduce the MDP formulation as used in the econometrics literature under the name "Dynamic Discrete Choice Model". Following this, we show how the optimality equation is formulated under these assumptions, and how the resulting optimization problems can be related to traditional IRL algorithms.

\subsection{Dynamic Discrete Choice Model}

A dynamic discrete choice (DDC) model (\emph{i.e.}, a discrete Markov decision process with action shocks) is defined as a tuple $(\mathcal{X,A}, T,r,\mathcal{E},F)$. 
We assume a discrete state space, although this is not strictly necessary (to avoid technical machinery out of the scope of this paper).
$\mathcal{X}$ is a countable set of states with a cardinality of $|\mathcal{X}|$. $\mathcal{A}$ is a finite set of actions with cardinality $|\mathcal{A}|$. $T$ is the transition function where $T(x'|x,a)$ is the probability of reaching state $x'$ given current state $x$ and action $a$. The reward function $r$ is a mapping $r:\mathcal{A}\times\mathcal{X}\rightarrow \mathbb{R}$.

Different from MDPs typically used in RL, each action also has a "payoff-shock" associated with it, that enters payoffs additively.
Intuitively, the shock variable accounts for the possibility that an agent takes a non-optimal behavior due to some unobserved factor of the environment or agent.
The vector of shocks is denoted $\vec{\epsilon} = [ \epsilon_{1} \cdots \epsilon_{|\mathcal{A}|} ]$ and $\vec{\epsilon}\in \mathbb{R}^{|\mathcal{A}|}$. Total rewards for action $a \in \mathcal{A}$ in state $x \in \mathcal{X}$ are therefore given by:
\begin{eqnarray}
r(a,x)+\epsilon_a.
\end{eqnarray}
. A shock value $\epsilon_a\in\mathbb{R}$ is often assumed to be distributed according to a Gumbel or Type 1 Extreme Value (TIEV) distribution,
\begin{align}
F(\epsilon_a)=e^{-e^{-\epsilon_a}}
\end{align}
We will see that the use of a TIEV distribution is numerically convenient for the following derivations. However, alternative algorithms can be derived for other functional forms. Each shock $\epsilon_a$ is independently and identically drawn from $F(\epsilon_a)$. This ensures that state transitions are conditionally independent. All serial dependence between $\epsilon_{t}$ and $\epsilon_{t+1}$ is transmitted through $x_{t+1}$. \cite{rust_theory} proves the existence of optimal stationary policies in this setting.

\subsection{Bellman Optimality Equation Derivation}

Consider a system currently in state $(x_t,\vec{\epsilon}_t)$, where $\vec{\epsilon}_t$ is a vector of shock values. The decision problem is to select the action that maximizes the payoff:
\begin{align}
\begin{split}
V(x_t,\vec{\epsilon}_t) & = \max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& \qquad + \beta \cdot E_{x_{t+1},\vec{\epsilon}_{t+1}|x_t,a} \left[V(x_{t+1},\vec{\epsilon}_{t+1})\right] \big\} \\
\end{split}
\end{align} 
where $V$ is the value function, $\beta$ is the discount factor and $\epsilon_{at} \in \vec{\epsilon}_t$ is the shock value when selecting action $a$ at time $t$.


Given the conditional independence assumption of the shock variable described previously, we can separate the integration of $x_{t+1}$ and $\vec{\epsilon}$. Define the \emph{ex-ante} value function (i.e., $V$ prior to the revelation of the values of $\epsilon$) as:
\begin{eqnarray}\label{eq:def_exante}
\overline{V}(x_t)
\triangleq 
E_{\vec{\epsilon}_t} \left[ V(x_t, \vec{\epsilon}_t) \right],
\end{eqnarray}
that is, the expectation of the value function with respect to the shock distribution. Using this notation and conditional independence, we can write the original decision problem as:
% NOTE: 
% Cannot use \left \right to dynamically scale {} 
% This trick doesn't seem to work (https://tex.stackexchange.com/questions/49890/linebreak-between-left-and-right)
%
\begin{align}
\begin{split}
V(x_t,\vec{\epsilon}_t) & =\max_{a\in\mathcal{A}} \big\{ \vphantom{V} r(x_t,a)+\epsilon_{at}  \\
&  \, +\beta \cdot E_{x_{t+1}|x_t,a} \left[ \overline{V}(x_{t+1}) \right] \vphantom{r(x_t, a)} \big\}.
\nonumber
\end{split}
\end{align}


The \emph{ex-ante} value function also follows a Bellman-like equation:
\begin{align} \label{eq:exantebellman}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]
\end{split}
\end{align}

Assuming TIEV distribution for the shock values, one obtains the following expression for the \emph{ex-ante} value functions as shown by \cite{rust_gmc}:
\begin{align} \label{eq:exanterust}
\begin{split}
\overline{V}(x_t) &=\ln\left[\sum_{a\in\mathcal{A}} \exp\left(r(x_t,a)+\beta \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \right)\right] \\
& \qquad +\gamma,
\end{split}
\end{align}
where $\gamma$ is Euler's constant. The expectation of the maximum is equal to the average of expected value functions, conditional on choosing action $a$ with $\vec{\epsilon}$ integrated using the TIEV density. Weights in the average are given by the CCPs of choosing action $a$.

% E(max(a,b)) = Pr(a>b)E(a|a>b)+Pr(b>a) E(b|b>a)

Notice that the above is exactly the recursive representation of the Maximum Causal Entropy IOC algorithm as derived in Theorem 6.8 in \cite{ziebart_phd}. In our setting, the soft-max recursion is a consequence of Bellman's optimality principle in a setting with a separable stochastic payoff shock with a TIEV distribution, while in \cite{ziebart_phd} the authors derive the recursion from an information-theoretic perspective that enforces a maximum causal entropy distribution over trajectories.

%===============================================================%
\subsection{Conditional Choice Probability}

We will now show how it is possible to efficiently recover the optimal value function, and consequently the underlying reward function, using the DDC model. The key insight is that the optimal value function can be directly estimated from observed state-actions pairs (Conditional Choice probabilities), observed over a \textit{large} set of expert demonstrations. When this assumption holds the optimal value function can be represented as a linear function of the CCPs and efficiently computed for different parameter values without solving the DP problem iteratively.

Since an outside observer does not have access to the shock ($\vec{\epsilon}$), the underlying deterministic policy of the expert $\sigma(a | x,\epsilon)$ is not directly measurable. However, if we average decisions across trajectories conditioned on the same state variables we are able to identify the integrated policy. We denote this integrated policy by $\sigma(a|x)\in[0,1]$, the \emph{conditional choice probability} (CCP) of an action being chosen conditioned on state $x$: 
\begin{eqnarray}
\sigma(a|x_t)\triangleq E_{\vec{\epsilon}}\left[ \mathbf{1}\{a\ \textrm{is optimal in state }x_t\}\right],
\end{eqnarray}
where $\mathbf{1}\{\}$ is the indicator function. The event in the indicator function is equivalent to the event:
\begin{align}
\begin{split}
& \left\{r(x_t,a)+\epsilon_{at}+\beta E_{x_{t+1}|a,x_t} \overline{V}(x_{t+1})\geq \right. \\
& \quad \left. r(x_t,a')+\epsilon_{a't}+\beta E_{x_{t+1}|a',x_t} \overline{V}(x_{t+1}),\ \forall a'\neq a \right\}
\end{split}
\end{align}

Expanding the expectation under the TIEV assumption on the shock variable allows CCPs to be solved in closed-form:
\begin{align} \label{eq:ccps}
\begin{split}
\sigma(a|x_t)=\frac{\exp\left(r(x_t,a)+\beta E_{x_{t+1}|x_t,a} \overline{V}(x_{t+1})\right)}{\sum_{a'\in\mathcal{A}} \exp\left(r(x_t,a')+\beta E_{x_{t+1}|x_t,a'} \overline{V}(x_{t+1})\right)}
\end{split}
\end{align}
Notice that \eqref{eq:ccps} is identical to the definition of the policy of the MaxEnt formulation in \cite{ziebart_phd}, which is derived from an entropic prior on trajectories. The CCP is derived by integrating out the TIEV shock variable. 
% In its simplest form, the CCPs can be computed directly from $N$ expert trajectories each with $T_i$ time periods:  $\mathcal{D} = \{(a_{it},x_{it})_{t=0}^{T_i}:i=1,\dots,N\}$ in tabular form. An initial maximum likelihood estimate can be computed by maintaining a table over state-action pair occurrences. 


\begin{figure*}[ht]
\begin{minipage}[t]{0.45\textwidth}
  %\vspace{0pt}  
  \begin{algorithm}[H]
    \caption{CCP-IRL algorithm} \label{algo:ccp_irl_algorithm}
    \begin{algorithmic}[1]
        %\Procedure{CCP-IRL}{$\mu_D,f, S, A, T, \gamma$}
        \STATE {\bfseries Input:} $\mu_D,f, S, A, T, \gamma$
        \STATE $\theta^{(0)} \gets$ init\_weights
        \STATE $M \gets \left[I-\sum_{a}(S(a) \lambda) *\left[ \beta T(a)  \right]\right]^{-1}$ 
        \STATE $\tilde{\epsilon} \gets \gamma - \log S(a)$
        \FOR{$i\gets 1, n$}
            \STATE $R^{(i)} \gets \theta^T f$
            \STATE $V^{(i)} \gets M \times \sum_{a}{S(a) \times \left[ R^{(i)} +\tilde{\epsilon}\right]}$
            \STATE $\pi_{\theta}^{(i)}(a|x) \gets e^{V^{(i)}(x_a) - V^{(i)}(x)}$
            \STATE $E[\mu^{(i)}] \gets $ FORWARD PASS()
            \STATE $\theta^{(i)} \gets \theta^{(i-1)} - \alpha \times (\mu_D - E[\mu^{(i)}])$
        \ENDFOR
    \end{algorithmic}
  \end{algorithm}
\end{minipage}%
\qquad
\begin{minipage}[t]{0.45\textwidth}
  %\vspace{0pt}
  \begin{algorithm}[H]
    \caption{Forward Pass} \label{algo:forward_pass_algorithm}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} policy $\pi_{\theta(a|x)}$
        \STATE $D^{(0)}(x) \gets P(x_i = x_{initial})$
        \FOR{$i\gets 1, n$}
            \STATE $D^{(i-1)}(x_{goal}) \gets 0$
            \STATE $D^{(i)}(x) \gets D^{(i)}(x) + \pi_{\theta}(a|x') D^{(i-1)}(x')$
        \ENDFOR
        \STATE $D(x) \gets \sum_{i}D^{(i)}(x)$ 
        \STATE \textbf{return} $D(x)$
    \end{algorithmic}
  \end{algorithm}
\end{minipage}
\end{figure*}


\section{Conditional Choice Probability - Inverse Reinforcement Learning}

Our aim in Inverse Reinforcement Learning is to find the parameterized reward function $r(\theta)$ for the given MDP/R. We now show how we can leverage the \textit{non-parameteric} estimates of choice conditional probabilities to efficiently estimate the parameters ($\theta$) of the reward function. 

First, we look at the alternative representation of the ex-ante value function which can be derived from the CCP representation.
Using this alternative representation, we will see how we can avoid solving the original MDP using the expensive dynamic programming formulation for every update of $\theta$.

%Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
%\begin{align}\label{eq:vamax}
%\begin{split}
%V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
%&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \\
%& \qquad \times E_{\vec{\epsilon}_{t+1}} \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\} \\
%&=r(a, x_t) + \sum_{j=1}^{T-t}\beta^{j}E_{x_{t+j}|a_t,x_t} \\
%& \qquad \big[r(x_{t+j}, a_{t+j})+ E_{\epsilon_{t+j}|a_t, x_t}\left[\epsilon(x_{t+j}, a_{t+j})\right] \big]
%\end{split}
%\end{align}

% Previously, we have seen that given the ex-ante value function we can derive the optimal policy estimates using \eqref{eq:ccps}.
% We now derive an alternate representation for the ex-ante value function.

\subsection{CCP-IRL estimation}

Returning to the definition of ex-ante value function \eqref{eq:def_exante} we know that,

\begin{align} \label{eq:exantebellman_1}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& \qquad +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big].\\
\end{split}
\end{align}
\cite{hotz} show that if we can get \textit{consistent} CCP estimates from the data, the above equation \eqref{eq:exantebellman_1} can be estimated as,
\begin{align} \label{eq:exantebellman_2}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\sum_{a\in\mathcal{A}} \sigma(a|x_t) \big\{r(x_t,a)+\epsilon_{at} \\
& \qquad +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]\\
\end{split}
\end{align}
Now, defining the expected shock given that action $a$ is optimal as $\tilde{\epsilon}(a|x_t) = E_{\vec{\epsilon}}\left( \epsilon_{at} |a\ \textrm{is optimal in state } x_t\right)$, we can rewrite \eqref{eq:exantebellman_2} as,
\begin{align} \label{eq:exantebellman_3}
\begin{split}
\overline{V}(x_t) & = \sum_{a\in\mathcal{A}} \sigma(a|x_t) \Big[r(x_t,a)+\tilde{\epsilon}(a|x_t) \\
& \qquad +\beta \sum_{x_{t+1}\in\mathcal{X}} T(x_{t+1}|x_t,a) \overline{V}(x_{t+1})\Big]
\end{split}
\end{align}

It was shown further that $\tilde{\epsilon}(a|x_t)$ depends on CCPs and distribution of $\epsilon$ only. They prove that the mapping between CCPs and choice specific value function is \emph{invertible}. Using this inverse mapping and assuming TIEV distribution for $\epsilon$, we get $\tilde{\epsilon}(a|x_t) = \gamma - \log \sigma(a|x_t)$.

% We can now use Monte Carlo approximations of the expected value functions. We use the CCPs, and transition probabilities to simulate $S$ trajectories each with $T$ decision nodes. The steps are:
% \begin{enumerate}
%     \item Draw an initial state $x_0$
%     \item Draw an action using CCPs associated with realized state $x_0$, call this $a_0$
%     \item Given action $a_0$ and state $x_0$ draw future state $x_1$. 
%     \item Continue this process until we have $T$ decision nodes.
%     \item Repeat this $S$ times
%     \item Store all trajectories.
% \end{enumerate}
% We can then approximate the expected value functions, conditional on a parameter guess on the reward function, as:
% \begin{eqnarray}
% \frac{1}{S}\sum_{s=1}^S\sum_{t=0}^T\beta^t [r(a_t,x_t)+\gamma-\ln \sigma(a_t|x_t)]
% \end{eqnarray}


From \eqref{eq:exantebellman_3} we can see that, excluding the unknown reward function, all other terms can be estimated from CCPs. We will now derive a closed form solution for the reward function estimation procedure. For this, we stack the ex-ante value function over all states,
\begin{align} \label{eq:exantebellman_4}
    \begin{split}
    \overline{\mathbf{V}}=\sum_{a}\mathbf{S}(a) \times \left[\mathbf{R}(a)+\tilde{\bm{\epsilon}}(a)+\beta \mathbf{T}(a) \overline{\mathbf{V}}\right]
    \end{split}
\end{align}
where:
\[
\overline{\mathbf{V}}=\left[\begin{array}{c}\overline{V}(x_1)\\\vdots\\\overline{V}(x_{|\mathcal{X}|}\end{array}\right]
\]
\[
\mathbf{R}(a)=\left[\begin{array}{c}r(x_1,a)\\\vdots\\ r(x_{|\mathcal{X}|},a)\end{array}\right]
\]

\[
\mathbf{T}(a)=\left[\begin{array}{ccc}
T(x_1|x_1,a),\dots,T(x_{|\mathcal{X}|},x_1,a)\\
\vdots\\
T(x_1|x_{|\mathcal{X}|},a),\dots,T(x_{|\mathcal{X}|},x_{|\mathcal{X}|},a)\\
\end{array}\right]
\]

\[
\mathbf{S}(a)=\left[\begin{array}{c}\sigma(a|x_1)\\ \vdots\\ \sigma(a|x_{|\mathcal{X}|})\end{array} \right]
\]

\[
\tilde{\bm{\epsilon}}(a)=\left[\begin{array}{c}\tilde{\epsilon}(a|x_1)\\ \vdots \\ \tilde{\epsilon}(a|x_{|\mathcal{X}|})\end{array}\right]
\]



Notice that \eqref{eq:exantebellman_4} is linear in ex-ante value function ($\overline{\mathbf{V}}$). Thus we can write a closed form solution for it.
First, rearranging the terms we get,
\begin{align}
    \begin{split}
    \overline{\mathbf{V}}-\sum_{a}\mathbf{S}(a) *\left[ \beta \mathbf{T}(a) \overline{\mathbf{V}}\right]=\sum_{a}\mathbf{S}(a) *\left[ \mathbf{R}(a)+\tilde{\bm{\epsilon}}(a)\right]
    \end{split}
\end{align}

Defining $\lambda$ as a $1\times|\mathcal{X}|$ vector of ones.
We can now write the closed form solution for $\overline{\mathbf{V}}$ as,
\begin{align} \label{eq:exante_inversion}
\begin{split}
\overline{\mathbf{V}} &=\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1} \\
& \qquad \times \left[\sum_{a}\mathbf{S}(a) *\left[ \mathbf{R}(a)+\tilde{\bm{\epsilon}}(a)\right]\right]
\end{split}
\end{align}

%The above is the value function representation used by \cite{pese} and discussed in \cite{arcidiacono}.
Notice that the inverse matrix in \eqref{eq:exante_inversion} is similar to the inverse matrix required for value function estimation in value iteration (when written in matrix form). Hence, it is not more expensive than solving the MDP problem once. Additionally, we later discuss ways to approximate the matrix inverse.
Further, for linear reward parameters there exists a particularly convenient form of the above formulation. We dervie that formulation in the supplementary material.

% For linear rewards we can write $\overline{\mathbf{V}} = \mathbf{Z}^T\theta + \mathbf{E}$ where $Z$ is the feature matrix for all states. Let $\mathbf{W} = [\mathbf{Z}, \mathbf{E}]$ we can then calculate these as 
% \begin{align}\label{eq:W_linear_contraction}
% \mathbf{W} = \sum_{a}S(a) \times \left\{\left[\mathbf{z}(a), \bm{\tilde{\epsilon}}(a)\right] + \beta \mathbf{T}(a)\mathbf{W}\right\}
% \end{align}
% The $\mathbf{W}$ defined in the \eqref{eq:W_linear_contraction} is a contraction mapping and hence can easily be estimated using dynamic programming. Notice, this is again equivalent to solving the MDP problem once.

% \textbf{Linear Parameters:} We now derive a more convenient form for linear reward parameters. We can rewrite \eqref{eq:exantebellman_4} as

% \begin{align}
% \begin{split}
% \overline{\mathbf{V}} &= \sum_{a}\mathbf{S}(a)\times \mathbf{V}(a) \\
% \mathbf{V}(a) &= \mathbf{R}(a) + \tilde{\bm{\epsilon}}(a) + \beta\mathbf{T}(a)\sum_{a'}\mathbf{S}(a')\mathbf{V}(a')
% \end{split} 
% \end{align}

% Assuming linear parameters we can now write $\mathbf{R}(a) = \mathbf{Z}(a)^T\theta$. This allows us to simplify the above equation to $\mathbf{V}(a)=\mathbf{\hat{Z}}(a)^T\theta + \hat{\bm{\epsilon}}(a)$, where we define $\mathbf{\hat{Z}}(a)$ and $\hat{\bm{\epsilon}}(a)$ as

% \begin{align}
% \begin{split}
% \hat{z}(a, x) &= z(a,x) + \beta\sum_{x'\in \mathcal{X}}T(x'|x,a)\sum_{a'}\sigma(a'|x')\hat{z}(a',x') \\
% \hat{\epsilon}(a, x) &= \tilde{\epsilon}(a,x) + \beta\sum_{x'\in \mathcal{X}}T(x'|x,a)\sum_{a'}\sigma(a'|x')\hat{\epsilon}(a',x')
% \end{split} 
% \end{align}

\subsection{Algorithm}

We now discuss the CCP-IRL algorithm and how it avoids repeatedly solving the original MDP problem.
The pseudo-code for CCP-IRL is given in Algorithm~\ref{algo:ccp_irl_algorithm}, where $\mu_D$ is expert's feature expectations and $f$ are features at every state.
Notice that the only quantity dependent on $\theta$ in \eqref{eq:exante_inversion} is $\mathbf{R}(a, \theta)$.
Thus, to estimate $\mathbf{\overline{V}}$ using \eqref{eq:exante_inversion} we calculate $\mathbf{R}(a, \theta)$ for every $\theta$ value \emph{i.e.}, at every step of the iteration (Line 6).
But the inverse matrix $\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations (Line 3).
This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}.
Given this inverse matrix computing $\mathbf{\overline{V}}$ at any $\theta$ requires simple matrix operations (Line 7), which allows us to avoid solving the MDP using dynamic programming at every step of the iteration.

We also note how to calculate the initial CCP estimates ($\mathbf{S}$). In their simplest form, the initial CCP estimates can be computed directly from $N$ expert trajectories each with $T_i$ time periods:  $\mathcal{D} = \{(a_{it},x_{it})_{t=0}^{T_i}:i=1,\dots,N\}$ in tabular form. An initial maximum likelihood estimate can be computed by maintaining a table over state-action pair occurrences.

%Lines 8-10 we calculate the gradient for the rewad parameters using the IRL policy, which is estimated from the choice specific value function under maximum entropy distribution. These lines and Algorithm~\ref{algo:forward_pass_algorithm} are explained in \cite{kitani2012activity}.

% We will now look at how using the above formulation for the ex-ante value function \eqref{eq:exante_inversion} avoids us from repeatedly solving the original MDP problem.
% The pseudo-code for CCP-IRL is given in Algorithm~\ref{algo:ccp_irl_algorithm}.
% Notice that the only quantity dependent on $\theta$ in \eqref{eq:exante_inversion} is $\mathbf{R}(a, \theta)$.
% Thus, to estimate $\mathbf{\overline{V}}$ using \eqref{eq:exante_inversion} we calculate $\mathbf{R}(a, \theta)$ for every $\theta$ value \emph{i.e.}, at every step of the iteration (Line 6).
% But the inverse matrix $\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations.
% This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}. 
% Given this inverse matrix computing the ex-ante value function at any $\theta$ requires simple matrix operations which allows us to avoid solving the MDP using dynamic programming at every step of the iteration.\footnote{
% \cite{hotz2} derive a simulation approach that can avoid matrix inversions. Specifically, it is possible to simulate $S$ action state pairs with $\tau$ periods using the CCPs and the transition probabilities. These paths together with the fact that $\tilde{\epsilon}(a|x_t)$ is only a function of the CCPs, allows us to estimate the value function for a candidate value $\theta$. The simualation results must only be computed once and can be used for every $\theta$ candidate. The value function estimates can then be used to find $\theta$ that matches the data.}
%\textbf{Calculate CCP:} As mentioned previously we can get conditional choice probability estimates using a simple frequency estimator or a Kernel estimator for large state spaces. Given these estimates we can find the reward parameters ($\theta$) using an MLE estimator.

%The main advantage of the above estimation procedure lies in its computation complexity. Recall, that in Maximum Entropy IRL \cite{ziebart2010modeling} we have to solve the value iteration (backwards Dynamic Programming) problem at every iteration. Comparatively in the above formulation we only need to estimate $\tilde{z}(a, x_t)$ once and we can use different estimates of $\theta$ at each iteration. Thus, the above algorithm greatly reduces the computational burden of Maximum Entropy formulation by removing the backwards pass from each iteration.


% =============== CCP (Non-parameteric) ==============

%\subsection{Choice-Specific Value Function Estimation}

%We will now show how the value function is related to the CCPs by introducing the choice-specific value function. Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
%\begin{align}
%V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
%&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) E_{\vec{\epsilon}_{t+1}}
%\max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\}\label{eq:vamax}
%\end{align}
%Define the value function difference for action $a$ and $a_0$ as:
%\begin{align}
%\Delta_a(x)\triangleq V_a(x)-V_{a_0}(x).
%\end{align}
%The base action $a_0$ is selected, one action for each state, for which the immediate reward is normalized to zero. Now add and subtract $V_{a_0}(x_{t+1})$ within the max operator in (\ref{eq:vamax}), introduce the short-hand $T^{x_{t+1}}_{x_t,a}$, and expanding the expectation with TIEV yields:
%\begin{eqnarray}\label{eq:cond_choice_v}
%V_a(x_t)=r(a,x_t)+\beta \sum_{x_{t+1}} T^{x_{t+1}}_{x_t,a} \left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\exp(\Delta_{a'}(x_{t+1}))\right]+\gamma\right]\label{eq:vax}
%\end{eqnarray}
%We now have the choice-specific value function defined in terms of the value function difference $\Delta_{a}$.



%\subsection{Connecting Choice-Specific Value Function to CCPs}
%
%We will now proceed to expand the residual in terms of a ratio of CCPs. Recalling the definition of CCPs and choice-specific value function from Equations (\ref{eq:ccps}) and (\ref{eq:vax}):
%\begin{eqnarray}
%\sigma(a|x_t)=\frac{\exp\left(V_a(x_t)\right)}{\sum_{a'} \exp\left(V_{a'}(x_t)\right)}.
%\end{eqnarray}
%

%Multiplying and dividing by $\exp(-V_{a_0}(x_t))$ leads to
%\begin{eqnarray}
%\sigma(a|x_t)=\frac{\exp(V_a(x_t)-V_{a_0}(x_t))}{\sum_{a'} \exp(V_{a'}(x_t)-V_{a_0}(x_t))}=\frac{\exp(\Delta_a(x_t))}{\sum_{a'} \exp(\Delta_{a'}(x_t))}.
%\end{eqnarray}


%It is clear from the above that value function differences between action $a$ and $a'$ are equal to:
%\begin{eqnarray}
%\frac{\sigma(a'|x_t)}{\sigma(a_0|x_t)}=
%\left[
%\frac
%  {\exp(\Delta_{a'}(x_t))}
%  {\sum_{a''} \exp(\Delta_{a''}(x_t))}
%\right]
%\left[\frac{1}{\sum_{a''} \exp(\Delta_{a''}(x_t))}\right]^{-1}=\exp(\Delta_{a'}(x_t))
%\end{eqnarray}
%%
%
%
%Using the CCP ratio, the value function for the base action $a_0$ can be expressed as:
%\begin{eqnarray}
%V_{a_0}(x_t) 
%= 
%\beta\sum_{x_{t+1}}
%T(x_{t+1}|x_t,a_0) 
%\left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\frac{\sigma(a'|x_{t+1})}{\sigma(a_0|x_{t+1})}\right]+\gamma\right]
%\end{eqnarray}\label{eq:va0}
%
%\subsection{Solving for the Optimal Value Function}

%Since we have expressed the choice-specific (a base action) value function in terms of just the transition function and CCPs, we can now estimate $V_{a_0}$ as a fixed point of the above linear equation -- without the knowledge of the full value function or the reward function. In particular, stacking the value function  $V_{a_0}$ for all states yields a $m_x\times 1$ vector:
%\begin{eqnarray}
%\mathbf{V}_0\equiv \left[\begin{array}{c} V_{a_0}(x_1)\\V_{a_0}(x_2)\\\vdots\\ V_{a_0}(x_{m_x})\end{array}\right]
%\end{eqnarray}
%and $\mathbf{T}$ is the transition matrix with entry $T(x_{j}|x_m,a_0)$ for the $m$th row and the $j$th column. 

%Also define the $m_x\times 1$ vector $\mathbf{\Gamma}$ as:
%\begin{align}
%\mathbf{\Gamma} \equiv \left[\begin{array}{c} 
%\ln(1+\sum_{a'\neq a_0}\sigma(a'|x_1)/\sigma(a_0|x_1))\\
%\vdots\\
%\ln(1+\sum_{a'\neq a_0}\sigma(a'|x_{m_x})/\sigma(a_0|x_{m_x}))\\
%\end{array}\right].
%\end{align}
%
%Then we can write system of equations in matrix form as:
%\begin{align}
%\mathbf{V}_0 = \beta \mathbf{T} [\mathbf{V}_0 + \mathbf{\Gamma}]
%\label{eq:v0_fp}
%\end{align}
%which leads to
%\begin{align}
%\mathbf{V}_0 = [I-\beta \mathbf{T}]^{-1}\beta \mathbf{T} \mathbf{\Gamma}.
%\label{eq:v0_inv}
%\end{align}

%The above is the unique fixed point. The value function for every state can then be estimated as:
%\begin{eqnarray}
%V_a(x_t)=\ln(\sigma(a|x_t))-\ln(\sigma(a_0|x_t))+V_{a_0}(x_t).\label{eq:v_backout}
%\end{eqnarray}
%

%\subsection{Computational Complexity}
%In this way, we are able to avoid the need for dynamic programming and solve for the value function directly by solving the linear system.
%\textcolor{red}{(Matt: Need to explain how this is different than typical linear system formulation for policy evaluation (see http://incompleteideas.net/sutton/book/ebook/node41.html). Usually DP methods are faster than solving the system of $|X|$ linear equations for large $|X|$.)} \textcolor{blue}{(Empirically the CCP IOC is much faster. What is happening here? The state space is not big enough to see the computational cost of the linear system overtake the DP or is the GD outer loop killing MaxEnt IOC? Matt, please feel free to add your analysis and thoughts here.)}
%\textcolor{green}{I think the cost is coming re-solution of the DP at every possible parameter guess. Is that getting down by GD?}
%\textcolor{red}{Matt: MaxEnt IOC essentially has to perform value iteration for each gradient calculation. If I'm interpreting CCP IOC correctly, Eq 27 only needs to be evaluated once for every state (i.e. it avoids the need for an outer loop)?} 
%\textcolor{green}{Matt exactly, no iterations on the value function for every parameter value only once.} 
%\textcolor{red}{(Matt: Got it. Then the computational complexity should be $\mathcal{O}(|X|^3 + |X||A|)$ (first term for solving the linear system in eq 27, second term for eq 28). In CCP IOC, wouldn't using DP for Eq 27 reduce the $|X|^3$ term?)}
%\textcolor{purple}{SR: So this makes sense to me, but doesn't take into account the actual CCP computation -- this assumes we have already computed CCPs and are using those to compute the value function. We need this, plus what it takes to compute CCPs (which should be $\Omega(|X||A|^2NT)$ from the top of my head, because of the kernel smoothing in eq 10)}

%\subsection{Backing Out the Reward Function}

%We can go further to estimate the reward values $r(a, x_t)$ for $a$ and every state $x_t$ without the need for a costly gradient descent algorithm. We can do this by simply backing out the reward values using equation \ref{eq:va}:
%\begin{eqnarray}\label{eq:reward}
%r(a,x_t)
%=
%V_a(x_t)
%- 
%\beta\sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) 
%\left\{
%V_{a_0}(x_{t+1})
%+
%\ln
%  \left(
%    1 + \sum_{a'\in\mathcal{A}}\frac{\sigma(a|x_{t+1})}{\sigma(a_0|x_{t+1})}
%  \right)
%\right\}.
%\end{eqnarray}
%
%This proves that reward functions can be recovered without any further assumptions aside from the existence of one choice generating zero payoffs. 
%
%\textcolor{blue}{(Add an algorithm box !!!)}

%\subsection{Reward Function Transfer}
%The reward function is a vector of payoffs associated with every action at every state. This function can then be transferred to new settings. In order to determine optimal actions in new scenes we must re-sove the optimization problem with the new features and the recovered reward function. Denote the estimated reward function as $\hat{r}(x,a)$ The solution can be found using a slightly modified value function iteration approach using the smoothed value function:
%\begin{eqnarray} \label{eq:exanterust}
%\hat{\overline{V}}(x_t)=\ln\left[\sum_{a\in\mathcal{A}} \exp\left(\hat{r}(x_t,a)+\beta \cdot E_{x_{t+1}|a} \left[
%~\hat{\overline{V}}(x_{t+1})~
%\right]
%\right)\right]+\gamma,
%\end{eqnarray}
%where $\hat{\overline{V}}(x_t)$ is defined as in equation (\ref{eq:def_exante}) substituting the estimated reward function $\hat{r}(x,a)$.
%which defines a smoothed Bellman operator:
%\begin{eqnarray} \label{eq:exanterust}
%\Psi_\rho(\hat{\overline{V}})(x_t)=\rho\ln\left[\frac{1}{\rho}\sum_{a\in\mathcal{A}} \exp\left(\hat{r}(x_t,a)+\beta \cdot E_{x_{t+1}|a} \left[
%~\hat{\overline{V}}(x_{t+1})~
%\right]
%\right)\right]+\gamma,
%\end{eqnarray}
%where $\rho$ is a smoothing parameter. \cite{rust_theory} proves that this mapping is a contraction. Using the above with the estimated reward function it is possible to derive optimal policies for new settings. 
%\subsection{Relaxing Base Action Assumption}
%The CCP approach does not rely on the existence of a base action. There are a number of alternative assumptions that allow us to leverage the CCP representations. For example, if we are willing to make parametric assumptions on the reward function, e.g. linear or non-linear function of state variables, we can relax the base action assumption and allow for non-zero reward. These results are discussed in detail in \cite{magnac}. 
%


\subsection{Complexity Analysis}
The main computation in CCP-IRL is to estimate the inverse matrix in \eqref{eq:exante_inversion}. This formulation is similar to solving the MDP problem once in value iteration (in matrix formulation).\cite{?Putterman}. 
In contrast the main computation in MaxEnt-IRL is solving the same MDP problem multiple times.
However, unlike MaxEnt-IRL where we need to \textit{repeatedly} solve the MDP using dynamic programming we only need to estimate the inverse matrix \textit{once}.
Once found, estimating the MDP in CCP-IRL involves simple matrix computations and hence involve no significant computation overhead.

Thus, assuming a total of $N$ iterations for the entire MaxEnt-IRL convergence and $T$ iterations for each backwards recursion, MaxEnt-IRL takes a total of $O(N\times T \times|A|\times|S|)$ \cite{ziebart_phd}. For CCP-IRL assuming the matrix inversion can be performed with state of the art matrix inversion method, we get a corresponding runtime of $O(|S|^{2.4}+T\times|A|\times|S|)$. This complexity can be further reduced to $O(T\times|A|\times|S|)$ for linear reward formulations. We derive this formulation in the supplementary section.

We also look at how for large state spaces, we can avoid using matrix inversion and rather estimate the inverse matrix \eqref{eq:exante_inversion} using successive approximations.
Defining $A \equiv \left(I- \beta \sum_{a}(\mathbf{S}(a) \lambda) *\left[ \mathbf{T}(a)  \right]\right)^{-1}$ we can write it as $A = (I - \beta F)^{-1}$ where $F$ is used as a shorthand for notational convenience. Premultiplying both sides with $(I - \beta F)$ we get $(I - \beta F)\times A = I$ which finally gives us $A = I + \beta F A$. We can now use this last equation to estimate the invese matrix $A$ by successive approximations. From a computational perspective this can be much more efficient compared to estimating the inverse directly. Next, we will empirically show the above computational gains in CCP-IRL as well as discuss the expert data requirements for CCP-IRL.


% The main computation in CCP-IRL is to estimate $\tilde{Z}(a, x_t)$ and $\tilde{E}(a, x_t)$. For a finite or infinite horizon model with linear parameters for the reward function, we can implement this using backward recursion similar to Algorithm 2 in \cite{kitani2012activity}. Once $\tilde{Z}$ and $\tilde{E}$ have been estimated, we can calculate the $V(a, x_t, \theta)$ at different parameter values using simple matrix computations. Thus during the entire gradient descent we perform the expensive backwards recursion process only once. Assuming a total of $N$ iterations for the entire MaxEnt-IRL convergence and $T$ iterations for each backwards recursion MaxEnt-IRL takes a total of $O(N\times T \times|A|\times|S|)$ \cite{ziebart_phd}. These are reduced to $O(T\times|A|\times|S|)$ for linearly parameterized CCP-IRL. For non-linear parameterization, to avoid estimating $\tilde{Z}(a, x_t, \theta)$ at every $\theta$ value, we pre-compute the inverse matrix in \eqref{eq:w_inversion_non_linear} once. Thus the main computation again reduces to a non-linear function approximation at every step and a one time matrix inversion. Thus we get a corresponding runtime of $O(|S|^{2.4}+T\times|A|\times|S|)$ as compared to $O(N\times T \times(|A|\times|S|)$ for MaxEnt-IRL.

% Let $m_v$ be the number of steps for the value iteration to converge. Then, \emph{each gradient computation} requires

% Once the MLE is computed, CCP IOC requires a single policy evaluation (Eq. \ref{eq:v_backout}) and a single reward function backout (Eq. \ref{eq:reward}). Policy evaluation can be approached naively by solving the system of $|\mathcal{X}|$ linear equations directly (requiring $\mathcal{O}(|\mathcal{X}|^{2.4})$ time). More typically, this would be solved with fixed point iteration, which requires $\mathcal{O}(|\mathcal{X}|m_p)$, where $m_{p}$ is the number of steps for policy evaluation to converge.  We refer the reader to \cite{Sutton2017} for more details. The reward function backout in Eq. \ref{eq:reward} requires $\mathcal{O}(|\mathcal{X}||\mathcal{A}|)$ time.

% The CCP maximum likelihood estimation depends on the nature of the state and action space. For discrete actions and states, the tabular frequency estimation is linear in the number of observed state-action pairs $\mathcal{O}(NT)$. For the continuous case, this depends on the choice of likelihood function, though for a naive non-parametric MLE, this may require $\mathcal{O}((NT)^2)$ for all observed state-action pairs.

% Compare this to MaxEnt IOC in the discrete case, a form of policy gradient descent for imitation learning, which requires a procedure similar to value iteration for every gradient computation. Let $m_v$ be the number of steps for the value iteration to converge. Then, \emph{each gradient computation} requires $\mathcal{O}(|\mathcal{X}|^2|\mathcal{A}|m_v)$ operations. As we expect $m_v$ and $m_p$ to be similar, a single gradient update of MaxEnt IOC is more computationally complex than the entire CCP IOC algorithm.
% \textcolor{red}{(Someone else familiar with MaxEnt IOC: This is a key take-home message, could you verify it?)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Experiments}

%In this section our aim is to empirically validate the above theoretical results \textcolor{blue}{(KK: To vague. Be clear and upfront.)}. 
In this section we empirically validate,
(1) the computational efficiency of CCP-IRL and (2) the underlying assumptions of consistent CCP estimates \emph{i.e.,} we show the data requirement for CCP-IRL.
To this end we evaluate the performance of CCP-IRL on three standard IRL tasks. Since CCP-IRL is an extension of MaxEnt-IRL \cite{ziebart}, we use it as a baseline method to compare our results on the benchmark tasks. Previously, both linear \cite{ziebart} \cite{ziebart2010modeling} and non-linear \cite{wulfmeier2015maximum} formulations of MaxEnt-IRL have been used to estimate the reward functions. Hence, we discuss results for both formulations of CCP-IRL. For the former, we focus on problems of navigation in a traditional Gridworld setting with stochastic dynamics, while for the latter we choose the Objectworld task as described in \cite{Levine2013}. We will also like to note that given the additional complexity in applying MaxEnt-IRL to continuous domains we only focus on discrete tasks in our experiments.
% For each experimental scenario, we follow the pattern of: (1) Generate a few demonstration trajectories for the task; (2) Apply IRL (MaxEnt or CCP) algorithm to learn the reward function; and (3) Compare the learned reward function against the true reward. 
For comparative analysis, we use both qualitative and quantitative results.
For qualitative analysis, we directly compare the visualizations of the inferred reward functions for both CCP-IRL and MaxEnt-IRL.
For quantitative comparison, we use negative log likelihood (NLL) \cite{kitani2012activity} and expected value difference (EVD) \cite{levine2011nonlinear} as the evaluation criterion. NLL is a probabilistic comparison metric and evaluates the likelihood of a path under the predicted policy. For a policy ($\pi$), NLL is defined as,
\begin{align}
NLL(\pi) = E_{\pi(a|s)}\big[-\log \prod_{t} \pi(a_{t}|s_{t}) \big]
\end{align}
% Unfortunately, NLL cannot capture the underlying stochastic nature of the expert policy since it assumes deterministic transitions in expert trajectories.
As another metric of success, similar to related works \cite{Levine2013, wulfmeier2015maximum}, we use expected value difference (EVD). EVD measures the difference between the optimal and learned policy by comparing the value function obtained with each policy under the \textit{true reward} distribution.
Further, to verify the computational improvement using CCP-IRL, we observe the time taken by each algorithm as well as the number of iterations it takes for each algorithm to converge. We show that our algorithm is able to achieve similar qualitative and quantitative performance with \textit{much less} computational time.
%For computational analysis all experiments were run on a PC with an Intel Xeon CPU E5-2660 v3 2.60 GHz (10 cores) processor with an NVIDIA TITAN X (Pascal) GPU.


\subsection{Gridworld: Evaluating Linear Rewards} 

To show the computational efficiency of CCP-IRL for linear parameterizations we focus on the standard gridworld experiments, since it is easy to formulate linear reward function in this setting. We empirically validate the computational efficiency of CCP-IRL. Additionally, we qualitatively compare the reward functions inferred by both algorithms. Finally, we discuss how CCP-IRL requires consistent CCP estimates, which in turn depend on the amount of expert demonstrations available. Since CCP estimates are calculated from expert trajectories we observe how CCP-IRL's performance depends on the amount of expert trajectories.

\begin{table}[t]
\centering
\def\arraystretch{1.0}% 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
N & Cell Size & MaxEnt (sec) & CCP (sec) & Speedup \\\hline

32 & 8 & 635.63 & \textbf{266.18} & $3\times$ \\
32 & 4 & 584.30 & \textbf{283.81} & $2\times$ \\
64 & 8 & 3224.97 & \textbf{1024.42} & $3\times$ \\
\hline
\end{tabular}
\caption{\textbf{Computation time} (averaged over multiple runs) comparison between MaxEnt and CCP for gridworld settings. Each experiment was run for 50 iterations.}
\label{table:table_results_macro_cells}
\end{table}



\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/macro_cells/grid_16_macro_2_lr_05/ll_ccp_vs_maxent_per_traj.pdf}}&
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/macro_cells/grid_16_macro_2_lr_05/evd_multiple_tries.pdf}}
    \end{tabular}
    \caption{Results for gridworld of size 16 with macro-cells of size 2. Left: Minimum NLL results with varying number of trajectories. Right: Expected Value Difference results. For few trajectories CCP-IRL shows much larger variance as compared to MaxEnt-IRL.}
    \label{fig:img_maxent_vs_ccp_gridworld_macro_cell}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{ccc}
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/true_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/maxent_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/ccp_reward_2_trim.pdf}} \\
    True Reward & MaxEnt & CCP \\
    \end{tabular}
    \caption{ Reward distribution for macro cells with gridsize 8 and macro cell size 2 using 10 trajectories. Dark - high reward, Light - low reward. }
    \label{fig:img_reward_map_gridworld_macro_cell}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{c}
    \MSHangBox{\includegraphics[width=0.4\textwidth]{images/gridworld/fixed_target_wind_30/timeit_maxent_vs_ccp_grid_32_per_discount.pdf}}
  \end{tabular}
    \caption{Computation time (in seconds and averaged over multiple runs) comparison between MaxEnt and CCP for Gridworld with gridsize of 32. Each experiment was run for 50 iterations. }
    \label{fig:img_gridworld_maxent_vs_ccp_time_discount}
\end{figure}

For this we use the macro-cell Gridworld environment \cite{abbeel2004apprenticeship}. This is slightly more complex form of the standard RL gridworld navigation problem and provides a good basis to compare the two algorithms. 
In this setting, the $N \times N$ grid is divided into non-overlapping square regions (macro-cells). Each region contains multiple grid cells and each cell in a region shares the same reward. Similar to \cite{abbeel2004apprenticeship}, for every region we select a positive reward $r \in (0, 1)$ with probability of $0.1$ and $r = 0$ with probability $0.9$. This reward distribution leads to positive rewards in few macro cells which results in interesting policies to be learned and hence requires more precise CCP estimates to match expert behavior.

Since all the cells in the same region share the same reward, our feature representation is a one-hot encoding of the region that cell belongs to \emph{e.g.}, in a grid world of size $N=64$ and macro cell of size 8 we have 64 regions and thus each state vector is of size 64. As before, we assume a stochastic wind with probability $0.3$ and the agent can move only in four directions. 

We analyze the performance of both algorithms given different amounts of expert trajectories.
Figure \ref{fig:img_maxent_vs_ccp_gridworld_macro_cell} compares the NLL and EVD results against increasing number of expert trajectories.
As seen above, both algorithms show poor performance given very few trajectories (< 20 trajectories).
However, with moderate number of trajectories MaxEnt-IRL approaches expert behavior while CCP-IRL is still comparatively worse.
Finally, with sufficiently large number of trajectories (> 60) both algorithms converge to expert behavior. 
CCP-IRL's poor performance with few expert demonstrations reflect its dependence on sufficient amount of input data.
Since CCP estimates are calculated from input data CCP-IRL needs a sufficient (relatively larger than MaxEnt-IRL) amount of trajectories to get consistent CCP estimates.
% This is expected given that we need sufficient amount of trajectories to get consistent CCP estimates. 
% This requirement for more input trajectories can be solved for robotic tasks with autonomous data aquisition.

% These results show that CCP-IRL's performance on the input trajectories to get consistent CCP estimates.  since consistent CCP estimates require sufficient input trajectories 
%of CCPs which in turn are directly inferred from the data.
%which requires a sufficient number of trajectories to arrive at consistent estimates.
%Many robotic tasks use simulations or autonomous techniques to gather data, in which case the requirement for large number of trajectories can be easily satisfied. 

We also qualitatively compare the rewards inferred by both algorithms given few trajectories in 
Figure \ref{fig:img_reward_map_gridworld_macro_cell}. Notice that the darker regions in the true reward are similarly darker for both algorithms. Thus, both algorithms are able to infer the general reward distribution. However, MaxEnt-IRL is able to match the true reward distribution at a much finer level (since less discrepancy compared to the true reward) and hence the underlying policy more closely as compared to CCP-IRL.
Thus, given few input trajectories MaxEnt-IRL performs better than our proposed CCP-IRL algorithm.
% inferred reward function is worse as compared to MaxEnt-IRL.

%Finally we compare the computation cost for both algorithms across large state spaces ($|S| \in \{10^3, 10^4\}$) in Table \ref{table:table_results_macro_cells}.
%As before, CCP-IRL is almost 2x as fast as MaxEnt-IRL across state spaces. Additionally, for the last two rows the only difference in the experiments is in terms of the feature size, since we get similar performance improvement for both settings this indicates that the speedup in CCP-IRL is independent of the dimension of $z(a, x_t)$. This is important since in CCP-IRL the backwards recursion happens on the $z(a, x_t)$ vector while in MaxEnt-IRL it happens on the scalar reward which might give an impression of the former being much more computationally expensive.

We verify the computation advantage for CCP-IRL across large state spaces ($|S| \in \{10^3, 10^4\}$) in Table~\ref{table:table_results_macro_cells}. As seen before, CCP-IRL is atleast 2$\times$ faster than MaxEnt-IRL. Also, its computational efficiency increasing for larger state spaces.
%than MaxEnt-IRL across different sized state spaces.

% Additionally, we also observe the affect of feature dimension on computation complexity. Notice the last two rows of Table \ref{table:table_results_macro_cells} have same state space but different feature sizes. Given these differences we still get similar (~2x) performance improvement for CCP-IRL which indicates that this speedup is largely independent of the dimension of $z(a, x_t)$ \eqref{eq:choice_specific_linear_param}. This is important since in CCP-IRL the backwards recursion happens on the $z(a, x_t)$ vector while in MaxEnt-IRL it happens on the scalar reward which might give an impression of the former being much more computationally expensive.

\begin{table}[t]
\centering
\def\arraystretch{1.4}% 
\begin{tabular}{|c|c|c|c|}
\hline
Experiment Setting & MaxEnt & CCP & Speedup \\\hline

Grid size: 16, C = 2 & 1622.63 & \textbf{296.43} & $5\times$ \\
Grid size: 32, C = 2 & 9115.50 & \textbf{1580.22} & $6\times$ \\
Grid size: 16, C = 8 & 2535.38  & \textbf{545.95} & $5\times$ \\
Grid size: 32, C = 8 & 19445.66 & \textbf{4799.02} & $4\times$ \\
\hline
\end{tabular}
\caption{Computation time (in seconds and averaged over multiple runs) comparison between MaxEnt and CCP for Objectworld. Each experiment was run for the same number of iterations with similar settings. }
\label{table:table_results_objectworld}
\end{table}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.23\textwidth]{images/objectworld/grid_16_object_8/test_ll.pdf}}
    \MSHangBox{\includegraphics[width=0.23\textwidth]{images/objectworld/grid_16_object_8/evd_maxent_vs_ccp.pdf}}
  \end{tabular}
    \caption{Results on ObjectWorld with gridsize of 16 and 2 colors. Left: NLL results on test trajectories. Right: Expected Value Difference results. }
    \label{fig:img_objectworld_maxent_vs_ccp_lr_01}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/objectworld/transfer/ccp_no_var_maxent_no_var.pdf}}
    \MSHangBox{\includegraphics[width=0.24\textwidth]{images/objectworld/timeit_maxent_vs_ccp_grid_16_per_iter.pdf}}
  \end{tabular}
    \caption{Left: Results for transfer experiment using MaxEnt and CCP formulation on Objectworld with gridsize of 16 and 2 colors. Right: Time variance between MaxEnt-IRL and CCP-IRL with increasing number of iterations. As expected CCP-IRL shows little computation increase with larger number of iterations.}
    \label{fig:img_objectworld_maxent_vs_ccp_time_results}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{ccc}
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/true_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/maxent_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/ccp_reward_trim.pdf}} \\
    True Reward & MaxEnt & CCP \\
    \end{tabular}
    \caption{ Reward distribution for Objectworld with gridsize 8 using 30 trajectories and 5 colors. Dark - low reward, Light - high reward. We also plot the inner and outer color of each object. \textit{Pink} - color 1 \textit{Orange} - color 2, other colors are distractors. \textit{Figure best viewed in electronic version.} }
    \label{fig:img_reward_map_objectworld}
\end{figure}

\subsection{Objectworld: Evaluating Non-Linear Rewards}

We now look at CCP-IRL's performance when the true reward function is a non-linear parameterization of the feature vector.
For this, we use the Objectworld \cite{levine2011nonlinear} environment since the reward function is a non-linear function of state features.
Similar to related work \cite{wulfmeier2015maximum}, we use a Deep Neural Network (DNN) as the non-linear function approximator.
% Similar to previous works \cite{wulfmeier2015maximum} we use a Deep Neural Network as the non-linear function approximator for the reward function.
As before, we verify both (1) the computational advantage provided by CCP-IRL (DeepCCP-IRL) and (2) the data requirement for CCP-IRL in the above scenario.

The Objectworld environment consists of a grid of $N \times N$ states. At each state the agent can take 5 actions, including movement in 4 directions and staying in place. Spread through the grid are random objects, each with an inner and outer color. Each of these colors is chosen from a set of $C$ colors. The reward for each cell(state) is positive if the cell is within distance 3 of color 1 and distance 2 of color 2, negative if only within distance 3 of color 1 and zero in all other cases. For our feature vector we use a continuous set of values $x \in \mathbb{R}^{2C}$, where $x_i$ and $x_{i+1}$ is the shortest distance from the state to the \emph{i'th} inner and outer color respectively. Since the reward is only dependent on two colors, features for other colors act as distractors. 

We use DeepMaxEnt-IRL \cite{wulfmeier2015maximum} as the baseline, using similar deep neural network architecture for both algorithms. Precisely, we use a 2-layer feed-forward network with rectified linear units. We use the Adam \cite{kingma2014adam} optimizer with the initial learning rate set to $10^{-3}$.

We quantitatively analyze the performance of our proposed DeepCCP-IRL algorithm. 
Figure \ref{fig:img_objectworld_maxent_vs_ccp_lr_01} compares the NLL and EVD results for both algorithms. Notice that as observed before, with few expert trajectories both algorithms perform poorly. However, DeepMaxEnt-IRL matches expert performance with moderate number of trajectories ($\approx 20$), while DeepCCP-IRL requires relatively large number of trajectories ($\approx 40$).
This is expected since CCP-IRL requires larger number of expert trajectories to get consistent CCP estimates.

Also, we qualitatively look at the inferred reward to verify how well the DNN is able to approximate the non-linear reward. Figure \ref{fig:img_reward_map_objectworld} plots the inferred rewards against the true reward function. Notice that both algorithms capture the non-linearities in the underlying reward function and consequently match the expert behavior. Thus, a deep neural network suffices as a non-linear function approximator for CCP-IRL. 

We now analyze the computation gain in the non-linear case. Table \ref{table:table_results_objectworld} shows the computation time for different sized state spaces and different sized feature vectors. Notice that DeepCCP-IRL is almost 5$\times$ as fast as DeepMaxEnt-IRL across small and large state spaces. Thus we see that CCP-IRL provides a much larger computation advantage for the non-linear case, which we believe is because the objectworld MDP problem is more complex than the above grid world experiments. 
This results in both algorithms requiring larger number of iterations until convergence which leads to a large computational increase for DeepMaxEnt-IRL as compared to DeepCCP-IRL. 
This computational increase with larger number of iterations is also shown in Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} (Right).
Notice that as the number of iterations increase, our proposed DeepCCP-IRL algorithms shows minor computational increase as compared to DeepMaxEnt-IRL. 
Thus, for significantly complex MDP problems which require large number of iterations our proposed CCP-IRL algorithm should require much less computation time compared to MaxEnt-IRL.
% This results in each DP step taking longer for Objectworld which leads to large computation time for DeepMaxEnt-IRL.
%Also, as seen in Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} (Right), the computation gain for CCP-IRL holds across different number of iterations. This shows that the performance gain from CCP-IRL is largely independent of the number of iterations required to converge.

% Additionally, to better understand the above computation gain we plot the time taken against number of iterations in figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results}. As expected, DeepCCP-IRL shows negligible increase while DeepMaxEnt-IRL shows an almost exponential increase in computation time. Since DeepMaxEnt-IRL needs to solve the original MDP at every iteration step.

% We next look at the data requirement for DeepCCP-IRL. Notice Figure \ref{fig:img_objectworld_maxent_vs_ccp_lr_01}, as before we see that with fewer trajectories DeepCCP-IRL performs much worse compared to DeepMaxEnt-IRL. As expected this is a direct consequence of not having consistent CCP estimates given the insufficient amount of input trajectories.

% As seen above both DeepMaxent-IRL and DeepCCP-IRL show similar performance for both metrics with increasing number of trajectories. Thus both algorithms are able to recover the underlying reward function given sufficient trajectories. As before we see that with fewer trajectories DeepCCP-IRL performs much worse compared to DeepMaxEnt-IRL which is expected given that we need sufficient data for consistent CCP estimates. Also, Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} shows the results for the transfer experiment which reinforce the need for more data for better results.


%\textcolor{blue}{(KK: You mention the transfer experiment in the beginning but there is no discussion of it in this section... right?)}

%\textcolor{blue}{(KK: The entire section has almost no qualitative analysis. No visualization of the reward function that is learned. No visualization of transfer of the reward function. No visualization of trajectories. No visualization of the actual gridworld for object world.)}


%\textcolor{blue}{(KK: There should be a section on how the state space affects the results both in terms of accuracy and computational cost. I think that it is in the graphs but there is no explict section on it. The reader wants to know if we get the same speed up for large state spaces.)}

%\textcolor{blue}{(KK: Also, what about the entire discussion on the number of examples and coverage of the state space? What do the results say about this? Do we need examples from the entire space for CCP-IOC to work?)}

%\subsection{State and Action Representations}

%The traditional MaxEnt IOC model has taken the approach of state-based policy estimation using locations as states and directions as actions (N, NE, E, SE, and so on). In our CCP-based estimation, we instead choose to define states as the discrete feature vectors at each location $s = f(x,i)$, where $\vec{x}$ is the location vector $[x~y]$, and $i$ is a trajectory. Feature vectors change from trajectory to trajectory - as such, our state space contains a separate state for each trajectory-location pair; the number of states at which we estimate CCPs grows linearly with the number of trajectories. 

%Actions are feature vectors (states) that are exactly one grid position away from the current state. Note that the cardinality of the action space does not change as related to MaxEnt IOC but, instead, our interpretation (as a future state) of what an action is.
%
%We make the decision to redefine states and actions as above with the intent that rewards \textcolor{blue}{KK: SR why are we talking about rewards in the state and action section?}\textcolor{purple}{SR: KK wanting the reward to be a function of features is \emph{the reason} we choose to redefine states and actions.}\textcolor{blue}{KK: you don't have to argue with me on this point...I wrote the paper. It's the writing that needs to be fixed so that the connection is clear. I don't think that its obvious that rewards are functions of features in this paper.}, as in \cite{kitani2012activity}, are functions of feature vectors - \emph{not} locations; and that features of future states impact current agent choice - \emph{not} direction of movement \textcolor{blue}{KK: SR this is because the constant feature provides this implicitly when using value iteration. Direction does matter.}. This mirrors the underlying reasoning of \cite{kitani2012activity} but does so by explicitly defining the relationship between features and rewards \textcolor{blue}{KK: SR the explicit relationship is never made clear in terms of theory/implementation details.}. 
%
%\subsection{Action Equivalence and Base Action Representation}
%
%Because \textcolor{blue}{KK: don't start with 'because'} action space is an $F$-dimension real-valued vector \textcolor{blue}{KK: SR be careful about `space' and `vector' are being used}, we cannot practically use equivalence \textcolor{blue}{KK: SR what do you mean here by equivalence? why is it not practical?} in CCP estimation as in Equation \ref{eq:ccp-est}. Instead, we use a loosened version of equivalence \textcolor{blue}{KK: SR why is this ok?} where two actions are equivalent \textcolor{blue}{KK: SR why do we need this equivalence?} when the L1 distance \textcolor{blue}{KK: SR why L1?} between them is less than $a_{bin}$ \textcolor{blue}{KK: SR use a different variable, overloaded use with actions, sounds like this is a scalar threshold right?}. The consistency of this estimator \textcolor{blue}{KK: SR new term here `consistency of estimator' which is not defined} is guaranteed as long as $a_{bin}\rightarrow 0$ while the size of the sample increases without bound. This is identical to requirements all non-parametric estimators must fulfill for consistency.
%
%% \textcolor{purple}{(SR: JG, a couple sentences here about why this loosened version of equivalence does not compromise statistical soundness would be good)}
%
%We treat base action $a_0$ in a similar manner\textcolor{blue}{KK: SR remind the reader of what the base action is.}. In particular, we do not necessarily have an action at each state that is equivalent to $a_0$ \textcolor{blue}{KK: SR why?}; because a non-zero CCP is required for the action $a_0$ at every state \textcolor{blue}{KK: SR why?} for which we want a reward estimate \textcolor{blue}{KK: SR don't need to mention at this point? it breaks the logical flow}, we choose an arbitrary action $a_0$ that has non-zero CCP in a maximal number of states \textcolor{blue}{KK: SR the meaning of maximal number of states is not clear.}. But because \textcolor{blue}{KK: `But because' ... you're killing me!} we cannot rely on real-valued vector equivalence\textcolor{blue}{KK: SR unclear what this means?}, we use the most similar action (within an L1 bin size $a_{bin}$ of $a_0$), and call it $\tilde{a_0}$ \textcolor{blue}{KK: SR you explain what you are doing in a mechanical way but the reader is going to have a hard time understanding why this is the right thing to do. A research paper is in many ways an argumentative piece of writing, protocols and procedures need to be carefully motivated}.
%
%
%
%\subsection{CCP-based Reward Function Estimation}
%\label{sec:ccp-exp}
%
%\textcolor{blue}{KK: SR to be consistent, let's use CCP IOC.}
%
%For training our CCP-based model \textcolor{blue}{KK: SR what are we training exactly? remind the reader, we are trying to estimate the reward function right? That's the goal of IOC}, we use $N = 14$ ground truth observed trajectories across the state space \textcolor{blue}{KK: SR the state space has been redefined above. I think that you mean the locations of the scene. The state space is now all possible feature vectors right conditioned on the trajectory ID?} with $F = 34$ feature maps for each trajectory\textcolor{blue}{KK: SR is it for each trajectory or for each scene?} and compute the conditional choice probabilities for each state as follows:\textcolor{blue}{KK: SR ops, missing something here.}
%
%Instead of quantizing the space of $F$-dimensional real-valued states for computation of the conditional choice probabilities (which would cause the complexity to grow exponentially in state-space size), we instead compute point estimates of CCPs using Equation \ref{eq:ccp-est} at all present states \textcolor{blue}{KK: SR `present states' needs to be explained?} (assuming that each location-trajectory pair \textcolor{blue}{KK: SR another new term 'location-trajectory pair' that needs to be explained?} is a unique feature vector and, therefore, is a unique state \textcolor{blue}{(KK: SR not clear what this means? unique in what sense? you mean you can't visit the same state twice?)}), interpolating as necessary for other states.
%\textcolor{blue}{(KK: SR probably a more succint way to write this paragraph is to say that you are going to use a continuous distribution over the state space using KDE)}
%
%\textcolor{blue}{(KK: SR I think that this section needs to be fleshed out with mathematical notation because there is too much uncertainty in the text. It will be super helpful to the reader to be more concrete. I think that the state space for the CCP is actually location but internally you are using the feature vector representation...is that correct? You can make this concrete by writing out the actual KDE you use in terms of a feature vector.)}
%
%\textcolor{blue}{(KK: SR ok so this is going to confuse people. Above you say that the state representation is not location anymore but rather features vectors. This gives the impression that x= feature vector. Now we are talking about (x,y) locations again. This part needs to be clear. The state of the policy is still location but the internal representation for the KDE is a feature vector right?)} We do this \textcolor{blue}{(KK: SR this?)} computation for all locations $(x,y)$ in each trajectory $t$ (for each action possible for each state), which gives us an $N m_x \times A$ matrix of CCPs, with rows summing to 1 \textcolor{blue}{(KK: SR need to describe why N affects the size of the CCP table. From the theory section, CCP is just an integration of state-actions pairs over many trajectories. Nothing about storing a CCP for each trajectory)}. Because \textcolor{blue}{(KK: SR remove)} each state's CCP estimation is independent of other states, we can parallelize across state estimation; we do so by assigning one row of location space per thread\textcolor{blue}{(KK: SR this is too much detail)}. 
%
%\textcolor{blue}{(KK: SR actually now that I've read the entire section. This portion is actually about building the CCP table and the reward estimation. Consider new headings. At any rate, the reader at this point is thinking... so when are we getting to the actual experiments. There seems to be a lot of engineering involved in all of this.)}
%
%\textcolor{blue}{(KK: stopping here at the moment.)}
%
%We can then compute $\mathbf\Gamma$ and $\mathbf T$, the transition matrix. For computing $\mathbf\Gamma$, we use action $\tilde{a_0}$ \textcolor{blue}{(KK: this variable needs to be clarified above.)}. Because the transitions we are using are deterministic, we use an $m_x \times m_x$ matrix where element $a_{i,\tilde{a_0}}$ is 1 and all other elements are 0, where $\tilde{a_0}$ is the future feature vector (state) reached by choosing action $\tilde{a_0}$ from state $i$. \textcolor{blue}{(KK: mx is the cardinality of the state space right (number of pixels)? This is actually a big waste of memory if you are actually storing the mx x mx matrix since the transitions are sparse. minor point.)}
%
%To estimate the zero-value function \textcolor{blue}{(KK: this is a new term, I think this is called the choice-specific value function for the base action in the paper. Need to be consistent to avoid confusion.)}, we use Equation \ref{eq:v0_fp}. We choose to use this instead of the inversion in Equation \ref{eq:v0_inv} because of the size of the state space \textcolor{blue}{(KK: be explicit, you mean that the state space is too big right?.)} - it is faster and more practical to reach a fixed point on the zero-reward action value function. \cite{rust_theory} proves that this mapping has a unique fixed point. We use repeated matrix multiplication to reach an equivalence with an epsilon of L1-norm $\tau$ \textcolor{blue}{(KK: need to be explained. Write out the recursion equation for the reader to make it clear.)}.
%
%We can then simply compute the value function using Equation \ref{eq:v_backout}, for each state-action pair, and back out the reward function.
%
%\begin{table}[t]
%    \centering
%\begin{tabular}{cc}
%    \begin{tabular}{r|l}
%{\bf Parameter} & {\bf Value} \\
%\hline
%$h$ & 0.8 \\
%$a_0 ((x,y),t)$ & ((80,130),0) \\
%$\beta$ & 0.95 \\
%$\tau$ & $10 \times 10^{-10}$ \\
%$a_{bin}$ & 0.3
%    \end{tabular}
%    
%    \hspace{2.0cm}
%    
%    \begin{tabular}{|c|c|c|}
%    \hline
%    Forecasting & Max-Ent IOC & CCP-IOC \\
%    \hline
%    walk (A) & 1.623 & 2.032 \\
%    walk (B) & - & - \\
%    \hline
%    \end{tabular}
%    
%\end{tabular}
%    \caption{Caption??}
%    \label{tab:my_label}
%\end{table}
%
%
%
%%\subsection{Pedestrian Trajectory Forecasting Results}
%%
%\textcolor{blue}{(KK: Please fill out this part...)}
%
%VIRAT 1 dataset forecasting log loss (MaxEnt IOC vs CCP IOC)
%
%VIRAT 2 dataset forecasting log loss (MaxEnt IOC vs CCP IOC)
%
%
%
%\subsection{CCP-based Reward Function Subsampling Analysis}
%
%\textcolor{blue}{(KK: This experiment is not motivated well. Why do we need to do this experiment? explain to the reader for improved readability.)}
%We also perform subsample analysis \textcolor{blue}{(KK: please explain what you mean by subsample analysis.)} on the CCP-based reward function using random subsamples of state-action pairs. These pairs do not necessarily come from the same trajectory $t$ \textcolor{blue}{(KK: why is this important?)}. Using each subsample, we can determine an L2 distance to the "true" reward estimation function \textcolor{blue}{(KK: Why is L2 appropriate here.)}, derived from the entire dataset. We do so using the following specifications:
%\begin{enumerate}
%\item[-] $\eta = 1000$ size subsamples of state-action pairs
%\item[-] $N_\eta = 1000$ subsampling episodes
%\item[-] Parameters as in CCP-based Reward Function Estimation \textcolor{blue}{(KK: Not clear what this referring to.)}
%\end{enumerate}
%
%We begin subsampling analysis by first computing the ``true'' estimate for the reward function. We do this by running the algorithm specified in Section \ref{sec:ccp-exp} with all data and all states.
%
%After computing the ``true'' reward function estimate for the full set of states, we randomly subsample $\eta$ state-action pairs from the observed trajectories. It is worth noting that these state-action pairs do not necessarily consist a single trajectory.\textcolor{blue}{(KK: why is this worth noting...twice.)}
%
%Using the $\eta$ state-action pairs, we recompute an estimate for the reward function for present states (by using the algorithm specified in Section \ref{sec:ccp-exp}). This gives us a partial estimate of the ``true'' reward function $\hat R$. We can calculate an L2 norm approximation from $\hat R$ to $R$ by assuming that all non-present states in $\hat R$ have equivalent reward values to those states in $R$.
%
%Doing the above norm calculation over $N_\eta$ episodes gives us an estimate of the variation in the reward function estimator.\textcolor{blue}{(KK: Results are missing and discuss needs to follow.)}

\section{Related Work \& Discussion}

Past work in scaling IRL for large spaces has looked at discrete and continuous spaces separately. This is because applying Max-Ent IRL to continuous spaces is much more challenging and requires its own set of assumptions. Trivially, we could discretize the continuous space and then apply MaxEnt-IRL algorithm directly. However, this scales exponentially with respect to the state space and is thus infeasible in practice. Hence, more sophisticated algorithms are required to extend MaxEnt-IRL to continuous spaces. 

For discrete spaces, past work such as, \cite{huang2015approximate} has mainly focused on approximating the value iteration step in IRL. However, since the MDP problem is still being solved at every step of an iterative procedure this does not scale to very large spaces. For continuous domains, most algorithms use local approximation to estimate the partition function in MaxEnt-IRL. Giving up on global optimality allows them to scale their algorithm to high dimensional domains \cite{levine2012continuous, kalakrishnan2013learning, finn2016guided}.
%Similarly, Kalakrishnan et~al. \yrcite{kalakrishnan2013learning} use local optimality with path integral for policy improvement to scale IRL to manipulation tasks. Recently, Finn et~al. \yrcite{finn2016guided} combined local sample based approximation of MaxEnt-IRL with policy learning under unknown dynamics for high dimensional manipulation tasks.

Alternately, some IRL algorithms have focused on a smaller subset of MDP problems. Todorov \yrcite{todorov2007linearly} introduced a new class of MDPs which are linearly solvable. Dvijotham and Todorov \yrcite{dvijotham2010inverse} present an efficient IRL algorithm for these MDPs. However, this new class of MDP's only solve a restricted form of general MDP formulation and is thus not applicable in all scenarios. Particularly, it is not clear how well these MDPs would generalize to problems where the reward (and as a consequence the value function) is a non-linear function of the states.
%In contrast, our work makes no such approximation of the general MDP framework and is thus applicable everywhere.

% In contrast our work focuses on the simpler discrete setting but with much larger state space.

In contrast to previous work our CCP-IRL algorithm makes no assumption on the MDP framework and is thus generally applicable. 
Also, we focus on discrete problem settings to introduce the key insights from the CCP framework in a well understood paradigm.
Although, the CCP approach is applicable to situations with continuous state spaces, continuous actions or non-stationary settings, given the complexity of applying MaxEnt-IRL to continuous spaces, it is not yet clear how can CCPs be effectively applied to high-dimensional continuous control tasks in robotics. We leave this for future work. 

% CCP framework helps in the unidentification of the reward formulation
% CCP formulation helps in using other empirial models developed by economists e.g. what if we use a different form of human optimality.

\textbf{Discussion:} 
We now discuss some of extensions of the CCP framework which are relevant in robotics and AI research.  
The CCP framework provides methods to investigate the well known under-identification problem in IRL. Only recently, have researchers in AI focused on the unidentifiability problem in IRL. In \cite{amin2017repeated} the authors focus on identification guarantees when the agent can choose the task rewards in a fixed environment setting. In contrast, CCP approach clarifies necessary assumptions to ensure a ``unique" mapping from trajectories to the reward functions \cite{magnac}. CCP approach is also used in multi-agent settings where strategic situations involve multiple equilibria, this often necessitates some equilibrium selection rules. With CCPs and a dataset on a single path of play, it is possible to estimate rewards without having to solve the game and to correctly select the equilibrium in the data \cite{pese}.

Finally, we look at the data requirement for CCP-IRL. A complete theoretical analysis of sample complexity of CCPs is beyond the scope of our initial work. However, we would like to refer the reader to Aguirregabiria and Mira \yrcite{aguirregabiria2002swapping} which discuss some sample properties of CCP estimators.
More importantly, the authors show that one-step CCP estimators (such as CCP-IRL) can have large bias with insufficient data. 
To reduce this bias they introduce a successive iteration procedure which results in a sequence of estimators. They further show that this iteration until convergence will realize the original MaxEnt-IRL estimator.

\section{Conclusion}

% Edit later... place holder

We have described an alternative framework for inverse reinforcement learning (IRL) problems that avoids value function iteration or backward induction. In IRL problems, the aim is to estimate the reward function from observed trajectories of a Markov decision process (MDP). We first analyze the decision problem and introduce an alternative representation of value functions due to \cite{hotz}. These representations allow us to express value functions in terms of empirically estimable objects from action-state data and the unknown parameters of the reward function. We then show that it is possible to estimate reward functions with few parametric restrictions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{references}
\bibliographystyle{icml2018}

% \clearpage
% \section*{Appendix for "Conditional Choice Probability Inverse Optimal Control"}


% \section{Identification Of Reward Functions}
% In econometrics, ``identification" refers to the process of determining whether observed distributions in the data can be mapped into a unique set of parameters of a model. Identification assumes the investigator has access to an infinitely large data set and is not constrained by estimation error. The question in this setting is whether it is mathematically possible to construct a unique mapping from data to the parameters of interest. In the MDP setting, we can determine whether data on states and actions is sufficient to identify the reward function and to what extent parametric assumptions are required. \cite{pese} prove that the solution to a MDP can be represented as an equation system that is linear in the reward function (See Lemma 1 in that paper). Identification then reduces to determining whether a linear equation system has a unique solution. In particular, \cite{pese} provide conditions for identification in multi-agent settings, which are very similar to single-agent decision problems.

% We present one type of identification argument following \cite{bajari} where we assume that one action has a zero payoff in every state. 
% We will now show how the value function is related to the CCPs by introducing the choice-specific value function. Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
% \begin{align}
% V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
% &=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) E_{\vec{\epsilon}_{t+1}}
% \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\}\label{eq:vamax_id}
% \end{align}
% Define the value function difference for action $a$ and $a_0$ as:
% \begin{align}
% \Delta_a(x)\triangleq V_a(x)-V_{a_0}(x).
% \end{align}
% The base action $a_0$ is selected, one action for each state, for which the immediate reward is normalized to zero. Now add and subtract $V_{a_0}(x_{t+1})$ within the max operator in (\ref{eq:vamax}), introduce the short-hand $T^{x_{t+1}}_{x_t,a}$, and expanding the expectation with TIEV yields:
% \begin{eqnarray}\label{eq:cond_choice_v}
% V_a(x_t)=r(a,x_t)+\beta \sum_{x_{t+1}} T^{x_{t+1}}_{x_t,a}  \\ 
% \left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\exp(\Delta_{a'}(x_{t+1}))\right]+\gamma\right]\label{eq:vax}
% \end{eqnarray}
% We now have the choice-specific value function defined in terms of the value function difference $\Delta_{a}$.



% \subsection{Connecting Choice-Specific Value Function to CCPs}

% We will now proceed to expand the residual in terms of a ratio of CCPs. Recalling the definition of CCPs and choice-specific value function from Equations (\ref{eq:ccps}) and (\ref{eq:vax}):
% \begin{eqnarray}
% \sigma(a|x_t)=\frac{\exp\left(V_a(x_t)\right)}{\sum_{a'} \exp\left(V_{a'}(x_t)\right)}.
% \end{eqnarray}


% Multiplying and dividing by $\exp(-V_{a_0}(x_t))$ leads to
% \begin{eqnarray}
% \sigma(a|x_t)=\frac{\exp(V_a(x_t)-V_{a_0}(x_t))}{\sum_{a'} \exp(V_{a'}(x_t)-V_{a_0}(x_t))}=\frac{\exp(\Delta_a(x_t))}{\sum_{a'} \exp(\Delta_{a'}(x_t))}.
% \end{eqnarray}


% It is clear from the above that value function differences between action $a$ and $a'$ are equal to:
% \begin{eqnarray}
% \frac{\sigma(a'|x_t)}{\sigma(a_0|x_t)}=
% \left[
% \frac
%   {\exp(\Delta_{a'}(x_t))}
%   {\sum_{a''} \exp(\Delta_{a''}(x_t))}
% \right]
% \left[\frac{1}{\sum_{a''} \exp(\Delta_{a''}(x_t))}\right]^{-1}=\exp(\Delta_{a'}(x_t))
% \end{eqnarray}
% %
% %
% %
% Using the CCP ratio, the value function for the base action $a_0$ can be expressed as:
% \begin{eqnarray}
% V_{a_0}(x_t) 
% = 
% \beta\sum_{x_{t+1}}
% T(x_{t+1}|x_t,a_0) 
% \left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\frac{\sigma(a'|x_{t+1})}{\sigma(a_0|x_{t+1})}\right]+\gamma\right]
% \end{eqnarray}\label{eq:va0}

% \subsection{Solving for the Optimal Value Function}

% Since we have expressed the choice-specific (a base action) value function in terms of just the transition function and CCPs, we can now estimate $V_{a_0}$ as a fixed point of the above linear equation -- without the knowledge of the full value function or the reward function. In particular, stacking the value function  $V_{a_0}$ for all states yields a $m_x\times 1$ vector:
% \begin{eqnarray}
% \mathbf{V}_0\equiv \left[\begin{array}{c} V_{a_0}(x_1)\\V_{a_0}(x_2)\\\vdots\\ V_{a_0}(x_{m_x})\end{array}\right]
% \end{eqnarray}
% and $\mathbf{T}$ is the transition matrix with entry $T(x_{j}|x_m,a_0)$ for the $m$th row and the $j$th column. 

% Also define the $m_x\times 1$ vector $\mathbf{\Gamma}$ as:
% \begin{align}
% \mathbf{\Gamma} \equiv \left[\begin{array}{c} 
% \ln(1+\sum_{a'\neq a_0}\sigma(a'|x_1)/\sigma(a_0|x_1))\\
% \vdots\\
% \ln(1+\sum_{a'\neq a_0}\sigma(a'|x_{m_x})/\sigma(a_0|x_{m_x}))\\
% \end{array}\right].
% \end{align}

% Then we can write system of equations in matrix form as:
% \begin{align}
% \mathbf{V}_0 = \beta \mathbf{T} [\mathbf{V}_0 + \mathbf{\Gamma}]
% \label{eq:v0_fp}
% \end{align}
% which leads to
% \begin{align}
% \mathbf{V}_0 = [I-\beta \mathbf{T}]^{-1}\beta \mathbf{T} \mathbf{\Gamma}.
% \label{eq:v0_inv}
% \end{align}

% The above is the unique fixed point. The value function for every state can then be estimated as:
% \begin{eqnarray}
% V_a(x_t)=\ln(\sigma(a|x_t))-\ln(\sigma(a_0|x_t))+V_{a_0}(x_t).\label{eq:v_backout}
% \end{eqnarray}



% \subsection{Backing Out the Reward Function}

% We can go further to estimate the reward values $r(a, x_t)$ for $a$ and every state $x_t$ without the need for a costly gradient descent algorithm. We can do this by simply backing out the reward values using equation \ref{eq:va}:
% \begin{align}\label{eq:reward}
% r(a,x_t)
% =V_a(x_t)- \nonumber\\ \beta\sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) 
% \left\{
% V_{a_0}(x_{t+1})
% +\ln
%  \left(
%     1 + \sum_{a'\in\mathcal{A}}\frac{\sigma(a|x_{t+1})}{\sigma(a_0|x_{t+1})}
%   \right)
% \right\}.
% \end{align}


% \newpage
% Return to ex-ante

% \begin{align} \label{eq:exantebellman}
% \begin{split}
% \overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
% & +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]\\
% &=\sum_{a\in\mathcal{A}} \sigma(a|x_t) \left[r(x_t,a)+\tilde{\epsilon}(a|x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}} T(x_{t+1}|x_t,a) \overline{V}(x_{t+1})\right]
% \end{split}
% \end{align}
% where 
% \[
% \tilde{\epsilon}(a|x_t)
% \]
% is the expected value of epsilon conditional on $a$ being optimal at state $x_t$. We know from Hotz Miller that this is equal to $\gamma-\ln\sigma(a|x_t)$ and is therefore only a function of the CCPs. Now stack the ex-ante value function over all states:
% \begin{align}
%     \begin{split}
%     \overline{V}=\sum_{a}S(a) *\left[ R(a)+\tilde{\epsilon}(a)+\beta T(a) \overline{V}\right]
%     \end{split}
% \end{align}
% where:
% \[
% \overline{V}=\left[\begin{array}{c}\overline{V}(x_1)\\\vdots\\\overline{V}(x_{|\mathcal{X}|}\end{array}\right]
% \]
% \[
% R(a)=\left[\begin{array}{c}r(x_1,a)\\\vdots\\ r(x_{|\mathcal{X}|},a)\end{array}\right]
% \]
% \[
% T(a)=\left[\begin{array}{ccc}
% T(x_1|x_1,a),\dots,T(x_{|\mathcal{X}|},x_1,a)\\
% \vdots\\
% T(x_1|x_{|\mathcal{X}|},a),\dots,T(x_{|\mathcal{X}|},x_{|\mathcal{X}|},a)\\
% \end{array}\right]
% \]
% \[
% S(a)=\left[\begin{array}{c}\sigma(a|x_1)\\ \vdots\\ \sigma(a|x_{|\mathcal{X}|})\end{array} \right]
% \]
% \[
% \tilde{\epsilon}(a)=\left[\begin{array}{c}\tilde{\epsilon}(a|x_1)\\ \vdots \\ \tilde{\epsilon}(a|x_{|\mathcal{X}|})\end{array}\right]
% \]
% Re-arranging :
% \begin{align}
%     \begin{split}
%     \overline{V}-\sum_{a}S(a) *\left[ \beta T(a) \overline{V}\right]=\sum_{a}S(a) *\left[ R(a)+\tilde{\epsilon}(a)\right]
%     \end{split}
% \end{align}
% which we can solve for $\overline{V}$
% \begin{align}
%     \begin{split}
%     \overline{V}=\left[I-\sum_{a}(S(a) \lambda) *\left[ \beta T(a)  \right]\right]^{-1}\left[\sum_{a}S(a) *\left[ R(a)+\tilde{\epsilon}(a)\right]\right]
%     \end{split}
% \end{align}
% where $\lambda$ is a $1\times|\mathcal{X}|$ vector of ones.

\clearpage

\subsection{Hotz-Miller's CCP Method}

Our aim in Inverse Reinforcement Learning is to find the parameterized reward function $r(\theta)$ for the given MDP/R. We will now show how we can leverage the non-parameteric estimates of choice conditional probabilities to efficiently estimate the parameters($\theta$) of the reward function for both linear and non-linear parameterization. 
Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
\begin{align}\label{eq:vamax}
\begin{split}
V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
%&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \\
%& \qquad \times E_{\vec{\epsilon}_{t+1}} \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\} \\
&=r(a, x_t) + \sum_{j=1}^{T-t}\beta^{j}E_{x_{t+j}|a_t,x_t} \\
& \qquad \big[r(x_{t+j}, a_{t+j})+ E_{\epsilon_{t+j}|a_t, x_t}\left[\epsilon(x_{t+j}, a_{t+j})\right] \big]
\end{split}
\end{align}

\textbf{Linear Parameters:} Many traditional IRL algorithms such as, \cite{abbeel2004apprenticeship} \cite{ziebart} assume that the reward is a linear combination of known features $z$. Assuming this we will now show how we can efficiently estimate $V_a(x_t)$ for different values of $r(\theta)$.

For linear parameterization we can write $r(a, x_t) = z(a, x_t)^T\theta$, where $z(a, x_t)$ are the set of basis functions (features) known to us.
Notice from \eqref{eq:vamax}, $V_a(x_t)$ is nothing but the expected and discounted sum of future rewards and shock values respectively. Thus we can split \eqref{eq:vamax} into two parts.
Define $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$ as the expected and discounted sum of current and future $z$ values and $\epsilon$ values respectively. Given this we can break \eqref{eq:vamax} into,
\begin{align}\label{eq:choice_specific_linear_param}
\begin{split}
V_a(x_t) & = \tilde{z}(a, x_t)^T \theta + \tilde{e}(a, x_t) \\
\tilde{z}(a, x_{t}, \theta) & = z(a, x_{t}) + \sum_{j=1}^{T-t}\beta^{j} E_{x_{t+j}|a,x_t} \\
& \qquad \times \left[ z\left(\pi^{*}\big( x_{t+j}, \epsilon_{t+j}, \theta \right), x_{t+j} \big) \right] \\
\end{split}
\end{align}
where $\pi^{*}(x_{t}, \epsilon_{t}, \theta)$ is the optimal policy. We will now show that both $\tilde{z}$ and $\tilde{e}$ need to be estimated only once.
To achieve this notice that, we can get consistent estimates of $\pi^{*}(x_t, \epsilon_t, \theta)$ from CCPs, hence we can write \eqref{eq:choice_specific_linear_param} as,
\begin{align}\label{eq:z_tilde_ccp}
\begin{split}
  \tilde{z}(a, x_{t}, \theta) & = z(a, x_t) + \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
  & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}) z(a_{t+j}, x_{t+j}) \right] \\
  \end{split}
\end{align}
We can very similarly write the above expression for $\tilde{e}(a, x_t, \theta)$ replacing $z(a, x_t)$ with $E[\epsilon_t(a)|x_t, a]$. For notational convenience we define $e(a, x_t) \equiv E[\epsilon_t(a)|x_t, a]$,
\begin{align}\label{eq:e_tilde}
\begin{split}
  \tilde{e}(a, x_{t}, \theta) & = \sum_{j=1}^{T-t}\beta^{j} E_{x_{t+j}|a,x_t} \\
  %& \qquad \times \left[ e\left(\pi^{*}\big( x_{t+j}, \epsilon_{t+j}, \theta \right), \epsilon_{t+j} \big) \right] \\
  %& = \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
  & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}) e(a_{t+j}, x_{t+j}) \right] \\
  \end{split}
\end{align}

\cite{hotz} show that $e(a, x_t)$ depends on conditional choice probabilities and distribution of $\epsilon$ only. Further, they prove that the mapping between CCPs and choice specific value function is invertible. Using this inverse mapping and assuming TIEV distribution for $\epsilon$, we get $e(a, x_t) = \gamma - \log P(a|x_t)$.

Thus, both $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$ depend on CCP values and transition probabilities only. As a result we can efficiently estimate $V_a(x_t)$ for different values of $\theta$ from \eqref{eq:choice_specific_linear_param} using simple matrix computations. The only real computation involved is in calculating $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$, both of which need to be done only once. Compare this to \cite{ziebart} where $V_a(x_t)$ needs to be calculated via expensive DP at every $\theta$ value. 
%These can be estimated by solving the DP problem using backwards recursion from \eqref{eq:z_tilde_ccp} and \eqref{eq:e_tilde} respectively, which is similar to Algorithm 9.1 in \cite{ziebart_phd}.

We will now show that the above estimation procedure can be simplified into a fixed point iteration algorithm. Let,
\begin{align}\label{eq:z_tilde_matrix}
Z(x) = \sum_{a}P(a|x)\tilde{z}(a, x)
\end{align}
this allows us to rewrite \eqref{eq:vamax} as,
\begin{align}\label{eq:z_tilde_from_Z}
\tilde{z}(a, x) = z(a, x) + \beta\sum_{x'}T(x'|a, x)Z(x')
\end{align}

Multiplying the above expression with $P(a|x)$ and summing over all actions we get,
\begin{align}\label{eq:z_tilde_recursion}
\begin{split}
\sum_{a}P(a|x)\tilde{z}(a|x) &= \sum_{a}P(a|x) \\
& \times \left\{z(a, x) + \beta\sum_{x'}T(x'|a,x)Z(x') \right\}
\end{split}
\end{align}

The LHS above is the definition of $Z(x)$ \eqref{eq:z_tilde_matrix}. To get a closed form expression we stack up \eqref{eq:z_tilde_recursion} for all different values of $x$. For this we define, 
$\mathbf{P}(a)$ as the vector for CCPs $\{P(a|x), x \in X\}$, similarly $\mathbf{z}(a)$ is $\{z(a, x), x \in X\}$ and $\mathbf{T}_{x'|x}(a)$ is the transition probability matrix for each 
action i.e., $T(x'|a, x) \equiv \mathbf{T}_{x'|x}(a)[x', x]$.

\begin{align}\label{eq:Z_defn}
\mathbf{Z} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{z}(a) + \beta \mathbf{T}_{x'|x}(a)\mathbf{Z}\right\}
\end{align}

We can get a closed form solution for $e(a, x)$ using the same formulation as above, which gives us,
\begin{align}\label{eq:E_defn}
\mathbf{E} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{e}(a) + \beta \mathbf{T}_{x'|x}(a)\mathbf{E}\right\}
\end{align}

For brevity, we combine $\mathbf{Z}$ and $\mathbf{E}$ together into $\mathbf{W} \equiv \{[\mathbf{Z}(x),  \mathbf{E}(x)], x \in X \}$. Thus we get,
\begin{align}\label{eq:w_recursion}
\mathbf{W} = \sum_{a}\mathbf{P}(a) \cdot \left\{ [\mathbf{z}(a), \mathbf{e}(a)] + \beta \mathbf{T}_{x'|x}(a)\mathbf{W}\right\}
\end{align}

In the above equation \eqref{eq:w_recursion} the only unknown is $\mathbf{W}$, which can be solved in closed form as,

\begin{align}\label{eq:w_inversion}
\begin{split}
\mathbf{W} &= \left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1} \\
& \qquad \times \sum_{a}\mathbf{P}(a) \cdot [\mathbf{z}(a), \mathbf{e}(a)]
\end{split}
\end{align}

We can solve for $\mathbf{W}$ either via fixed point iteration \eqref{eq:w_recursion} or via the inversion defined in \eqref{eq:w_inversion}.
Given $\mathbf{W}$ we can trivially estimate $\tilde{z}$ using \eqref{eq:z_tilde_from_Z} (and similarly $\tilde{e}$).  

\textbf{Non-Linear Parameters:} Many recent IRL algorithms have shown that linear parameterization is insufficient to learn complex reward functions \cite{levine2011nonlinear} \cite{wulfmeier2015maximum}. Thus in this section we focus on efficiently estimating $V_a(x_t)$ based on non-linear parameterization of $r$ from known set of features.
Our aim here is to show how to efficiently compute $\mathbf{W}$ \eqref{eq:w_inversion}, which can then be used to efficiently estimate $\tilde{z}$ and $\tilde{e}$ and hence $V_a(x_t)$, as shown before.

In the non-linear formulation, the rewards are defined as $r(a, x_t) = z(a, x_t, \theta)$ instead of $r(a, x_t) = z(a, x_t)^T\theta$. Notice that the only change is in the parameterization of $z$. Thus the choice specific value function \eqref{eq:choice_specific_linear_param} needs to be rewritten as,
\begin{align}\label{eq:choice_specific_non_linear_param}
V_a(x_t, \theta) = \tilde{z}(a, x_t, \theta) + \tilde{e}(a, x_t). 
\end{align}
Also, notice that the above change does not affect $\tilde{e}(a, x_t)$ and consequently the expression for $\mathbf{E}$ \eqref{eq:E_defn} remains unchanged. However, \eqref{eq:z_tilde_matrix} and \eqref{eq:z_tilde_from_Z} need to be rewritten to include the non-linear parameterization as,
\begin{align}
\begin{split}
Z(x, \theta) &= \sum_{a}P(a|x)\tilde{z}(a, x, \theta) \\
\tilde{z}(a, x, \theta) &= z(a, x, \theta) + \beta\sum_{x'}T(x'|a, x)Z(x', \theta)
\end{split}
\end{align}
%Notice, that the only difference between \eqref{eq:choice_specific_linear_param} and \eqref{eq:choice_specific_non_linear_param} is in terms of $\tilde{z}$. In the former expression it is a known set of features ($r(a, x_t) = z(a, x_t)^T\theta$) while in the latter case it is a scalar value($r(a, x_t) = z(a, x_t, \theta)$), estimated as a non-linear function of the features. 
%\begin{align}\label{eq:z_tilde_non_linear}
%\begin{split}
% \tilde{z}(a, x_{t}, \theta) & = r(a, x_t, \theta) + \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
%   & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}, \theta) \tilde{z}(a_{t+j}, x_{t+j}, \theta) \right] \\
%\end{split}
%\end{align}

%Thus $\tilde{z}(a, x)$ is a scalar quantity now since $r(a, x_t, \theta)$ is scalar. Apart from the above changes the other formulations are still valid for the non-linear case as well. Thus we can rewrite \eqref{eq:w_recursion} as
%\begin{align}
%W = \sum_{a}\overbar{P}(a) \cdot \left\{ [\overbar{r}(a, \theta), \overbar{e}(a)] + \beta \overbar{T}_{x'|x}(a)W \right\}
%\end{align}
%
%where we emphasize that $\overbar{r}(a, \theta)$ is the vector of rewards for each state and a function of $\theta$. From this we still get,
Thus following the same process as before for \eqref{eq:Z_defn} we get, 
\begin{align}\label{eq:Z_defn_non_linear}
\mathbf{Z}_{\theta} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{z}(a, \theta) + \beta \mathbf{T}_{x'|x}(a)\mathbf{Z}_{\theta}\right\}
\end{align}
where we have added a subscript $\theta$ to $\mathbf{Z}$ to indicate dependence. Combining $\mathbf{Z}_{\theta}$ and $\mathbf{E}$ together into $\mathbf{W}_{\theta}$ as before, we get \eqref{eq:w_recursion} and thus finally,
\begin{align}\label{eq:w_inversion_non_linear}
\begin{split}
\mathbf{W}_{\theta} &= \left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1} \\
& \qquad \times \sum_{a}\mathbf{P}(a) \cdot [\mathbf{z}(a, \theta), \mathbf{e}(a)]
\end{split}
\end{align}

Notice that to estimate $\mathbf{W}_{\theta}$ using \eqref{eq:w_inversion_non_linear} we need to calculate $\mathbf{z}(a, \theta)$ at every step of the iteration. But the inverse matrix $\left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations. This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}. 
Thus, similar to the linear case we can efficiently estimate $W_{\theta}$ for different values of $\theta$ by simple matrix computations.

\end{document}
