%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage[draft]{hyperref}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CCP-IRL}

% HACK REMOVE
%\usepackage[showframe]{geometry}% http://ctan.org/pkg/geometry

\usepackage[utf8]{inputenc}
\usepackage{authblk}

\usepackage{icml2018}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath,amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algorithmic}
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx

%PDF Info Is Required:
\icmltitlerunning{CCP-IRL}

% \author[1]{Mohit Sharma}
% \author[1]{Kris M. Kitani}
% \affil[1]{Robotics Institute, Carnegie Mellon University}
% \affil[1]{\texttt{\{mohits1, kkitani\}@cs.cmu.edu}}
% \author[]{Joachim Groeger}
% %\affil[2]{Amazon.com}
% \affil[2]{\texttt{jrg@joachimgroeger.com}}


% \pdfinfo{
% /Title (Inverse Reinforcement Learning with Conditional Choice Probabilities)
% /Author (
%     Mohit Sharma, 
%     %\texttt{mohits1@andrew.cmu.edu}
%     %\and
%     Kris M. Kitani, 
%     %\texttt{kkitani@cs.cmu.edu}
%     %\and
%     Joachim Groeger
%     %\texttt{joachimgroeger@gmail.com})
% }

% === Kris' Macros === %
\usepackage{mathtools}
\usepackage{bm}
\renewcommand{\vec}[1]{\mbox{\bm{$#1$}}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}

% === Mohit's Macros === %
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\def\MSHangBox#1{%^
\begin{minipage}[t]{\textwidth}% Top-hanging minipage, will align on
			       % bottom of first line
\begin{tabbing} % tabbing so that minipage shrinks to fit
~\\[-\baselineskip] % Make first line zero-height
#1 % Include user's text
\end{tabbing}%^
\end{minipage}} % can't allow } onto next line, as {WIDEBOX}~x will not tie.

\usepackage{color}
\definecolor{purple}{rgb}{0.58,0,0.83}

\begin{document}

\twocolumn[
\icmltitle{Inverse Reinforcement Learning with \\ Conditional Choice Probabilities}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{}
\icmlaffiliation{goo}{}
\icmlaffiliation{ed}{}

\icmlcorrespondingauthor{}{}
\icmlcorrespondingauthor{}{}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We make an important connection to existing results in econometrics to describe an alternative formulation of inverse reinforcement learning (IRL). In particular, we describe an algorithm using Conditional Choice Probabilities (CCP), which are maximum likelihood estimates of the policy estimated from expert demonstrations, to solve the IRL problem. Using the language of structural econometrics, we re-frame the optimal decision problem and introduce an alternative representation of value functions due to \cite{hotz}. In addition to presenting the theoretical connections that bridge the IRL literature between Economics and Robotics, the use of CCPs also has the practical benefit of reducing the computational cost of solving the IRL problem. Specifically, under the CCP representation, we show how one can avoid repeated calls to the dynamic programming subroutine typically used in model-based IRL. We show via extensive experimentation on standard IRL benchmarks that CCP-IRL is able to outperform MaxEnt-IRL, with as much as a 5x speedup and without compromising on the quality of the recovered reward function.
\end{abstract} 

\section{Introduction}

The problem of extracting the reward function of a task given observed optimal behavior has been studied in parallel in both robotics and economics. In robotics this literature is collected under the heading "Inverse Reinforcement Learning" (IRL), \cite{Ng2000, abbeel2004apprenticeship}. The aim here is to learn a reward function that best explains demonstrations of expert behavior so that a robotic system can reproduce expert like behavior. Alternatively, in economics it is referred to as "structural econometrics" \cite{miller, pakes, rust_gmc} and is used to help economists better understand human decision making. Both of the fields are similar in that both seek to uncover a \emph{latent} reward function of an underlying Markov Decision Process (MDP).
Although both fields developed in parallel there has been very limited knowledge transfer between them. To alleviate this we make the connection between these two fields more explicit. We uncover different problem formulations which give rise to similar algorithms. Specifically, we show how the softmax value function in \cite{ziebart} is exactly similar to the ex-ante value function in \cite{rust_gmc} under Type-1 extremum assumption.


Additionally, one of the main challenges in IRL applied to robotics is the large computational complexity of modern algorithms \cite{ziebart, Ratliff2006}. To infer the reward function of the underlying MDP, one must \textit{repeatedly} solve this MDP at every step of a reward parameter optimization scheme. The MDP solution, which is typically characterized by a value function, requires a computationally expensive Dynamic Programming (DP) procedure. Unfortunately, solving this DP step repeatedly makes IRL algorithms computationally prohibitive. In response, recent works have proposed  to find approximate IRL solution to deal with large environment spaces \cite{finn2016guided, levine2012continuous, huang2015approximate}.

The problem of computational complexity has also been studied in economics \cite{hotz, su2012constrained, aguirregabiria2002swapping}. Among the many works, Conditional Choice Probability (CCP) estimators \cite{hotz} are particularly interesting because of their computational efficiency.
CCP estimators use CCP values to estimate the reward function of the MDP.
The CCP values specify the optimal action for a state and are estimated from expert demonstrations. These estimators are computationally efficient since they avoid the repeated computation of the DP step by using an alternative representation of the MDP's value function. Specifically, it is shown that values functions can be decomposed into ratios of CCPs.

In this work we introduce Conditional Choice Probability Inverse Reinforcement Learning (CCP-IRL) by incorporating CCPs into IRL. We leverage results from \cite{rust_gmc, hotz, magnac} to formulate an estimation routine for the reward function with CCPs, that avoids repeated calls to the solver of the full dynamic decision problem. We test the CCP-IRL algorithm on multiple different IRL benchmarks and compare the results to the state-of-the-art IRL algorithm, MaxEnt-IRL \cite{ziebart}. In our experiments, we show that with CCP-IRL we can achieve up to 5$\times$ speedup without affecting the quality of the inferred reward function. Further, this speedup holds across large state spaces and increases for complex problems, such as, problems where value iteration takes much longer to converge. 

To summarize, in this work we make an important connection between economics and robotics to uncover different problem formulations that result in similar algorithms. We then use this connection to leverage the literature of conditional choice probabilities. For this, we propose a new IRL algorithm called CCP-IRL. We test the CCP-IRL algorithm on multiple different IRL benchmarks and show large speedup. We also discuss how the CCP approach is applicable to other important areas in IRL research.
Overall, we believe explicitly pointing out the similarities and differences in the two fields, will help researchers better leverage results from the other field. We hope this will lead to more knowledge transfer between them.


%==========================================================%
\section{IRL in Economics}

In this section, we first introduce the MDP formulation as used in the econometrics literature under the name "Dynamic Discrete Choice Model". Following this, we show how the optimality equation is formulated under these assumptions, and how the resulting optimization problems can be related to traditional IRL algorithms.

\subsection{Dynamic Discrete Choice Model}

A dynamic discrete choice (DDC) model (\emph{i.e.}, a discrete Markov decision process with action shocks) is defined as a tuple $(\mathcal{X,A}, T,r,\mathcal{E},F)$. 
We assume a discrete state space, although this is not strictly necessary (to avoid technical machinery out of the scope of this paper).
$\mathcal{X}$ is a countable set of states with a cardinality of $|\mathcal{X}|$. $\mathcal{A}$ is a finite set of actions with cardinality $|\mathcal{A}|$. $T$ is the transition function where $T(x'|x,a)$ is the probability of reaching state $x'$ given current state $x$ and action $a$. The reward function $r$ is a mapping $r:\mathcal{A}\times\mathcal{X}\rightarrow \mathbb{R}$.

Different from MDPs typically used in RL, each action also has a "payoff-shock" associated with it, that enters payoffs additively.
The motivation for this perturbation originates from a discrete choice literature \cite{mcfadden1973conditional} on static models as a way of reconciling why numerous economic agents with identical observed characteristics make different decisions to each other. Rather than assuming the agents are indifferent between all the decisions,
%that at least one of them takes (which might cover the choice set)
an alternative assumption is that there is an additional characteristic pertinent for decision making that the modeller does not observe. The simplest of way of modeling this unobserved characteristic is to treat it as an independent, identically distributed random variable, one for each element in the choice set. Adding a payoff-shock to each choice at every decision node in DDC models is the dynamic analogue to escape a similar lacuna.
This vector of shocks is denoted $\vec{\epsilon} = [ \epsilon_{1} \cdots \epsilon_{|\mathcal{A}|} ]$ and $\vec{\epsilon}\in \mathbb{R}^{|\mathcal{A}|}$. Total rewards for action $a \in \mathcal{A}$ in state $x \in \mathcal{X}$ are therefore given by:
\begin{eqnarray}
r(a,x)+\epsilon_a.
\end{eqnarray}
. A shock value $\epsilon_a\in\mathbb{R}$ is often assumed to be distributed according to a Gumbel or Type 1 Extreme Value (TIEV) distribution,
\begin{align}
F(\epsilon_a)=e^{-e^{-\epsilon_a}}
\end{align}
We will see that the use of a TIEV distribution is numerically convenient for the following derivations. However, alternative algorithms can be derived for other functional forms. Each shock $\epsilon_a$ is independently and identically drawn from $F(\epsilon_a)$. This ensures that state transitions are conditionally independent. All serial dependence between $\epsilon_{t}$ and $\epsilon_{t+1}$ is transmitted through $x_{t+1}$. \cite{rust_theory} proves the existence of optimal stationary policies in this setting.

\subsection{Bellman Optimality Equation Derivation}

Consider a system currently in state $(x_t,\vec{\epsilon}_t)$, where $\vec{\epsilon}_t$ is a vector of shock values. The decision problem is to select the action that maximizes the payoff:
\begin{align}
\begin{split}
V(x_t,\vec{\epsilon}_t) & = \max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& \qquad + \beta \cdot E_{x_{t+1},\vec{\epsilon}_{t+1}|x_t,a} \left[V(x_{t+1},\vec{\epsilon}_{t+1})\right] \big\} \\
\end{split}
\end{align} 
where $V$ is the value function, $\beta$ is the discount factor and $\epsilon_{at} \in \vec{\epsilon}_t$ is the shock value when selecting action $a$ at time $t$.


Given the conditional independence assumption of the shock variable described previously, we can separate the integration of $x_{t+1}$ and $\vec{\epsilon}$. Define the \emph{ex-ante} value function (i.e., $V$ prior to the revelation of the values of $\epsilon$) as:
\begin{eqnarray}\label{eq:def_exante}
\overline{V}(x_t)
\triangleq 
E_{\vec{\epsilon}_t} \left[ V(x_t, \vec{\epsilon}_t) \right],
\end{eqnarray}
that is, the expectation of the value function with respect to the shock distribution. Using this notation and conditional independence, we can write the original decision problem as:
% NOTE: 
% Cannot use \left \right to dynamically scale {} 
% This trick doesn't seem to work (https://tex.stackexchange.com/questions/49890/linebreak-between-left-and-right)
%
\begin{align}
\begin{split}
V(x_t,\vec{\epsilon}_t) & =\max_{a\in\mathcal{A}} \big\{ \vphantom{V} r(x_t,a)+\epsilon_{at}  \\
&  \, +\beta \cdot E_{x_{t+1}|x_t,a} \left[ \overline{V}(x_{t+1}) \right] \vphantom{r(x_t, a)} \big\}.
\nonumber
\end{split}
\end{align}


The \emph{ex-ante} value function also follows a Bellman-like equation:
\begin{align} \label{eq:exantebellman}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]
\end{split}
\end{align}

Assuming TIEV distribution for the shock values, one obtains the following expression for the \emph{ex-ante} value functions as shown by \cite{rust_gmc}:
\begin{align} \label{eq:exanterust}
\begin{split}
\overline{V}(x_t) &=\ln\left[\sum_{a\in\mathcal{A}} \exp\left(r(x_t,a)+\beta \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \right)\right] \\
& \qquad +\gamma,
\end{split}
\end{align}
where $\gamma$ is Euler's constant. The expectation of the maximum is equal to the average of expected value functions, conditional on choosing action $a$ with $\vec{\epsilon}$ integrated using the TIEV density. Weights in the average are given by the CCPs of choosing action $a$.

% E(max(a,b)) = Pr(a>b)E(a|a>b)+Pr(b>a) E(b|b>a)

Notice that the above is exactly the recursive representation of the Maximum Causal Entropy IOC algorithm as derived in Theorem 6.8 in \cite{ziebart_phd}. In our setting, the soft-max recursion is a consequence of Bellman's optimality principle in a setting with a separable stochastic payoff shock with a TIEV distribution, while in \cite{ziebart_phd} the authors derive the recursion from an information-theoretic perspective that enforces a maximum causal entropy distribution over trajectories.

%===============================================================%
\subsection{Conditional Choice Probability}

We will now show how it is possible to efficiently recover the optimal value function, and consequently the underlying reward function, using the DDC model. The key insight is that the optimal value function can be directly estimated from observed state-actions pairs (Conditional Choice probabilities), observed over a \textit{large} set of expert demonstrations. When this assumption holds the optimal value function can be represented as a linear function of the CCPs and efficiently computed for different parameter values without solving the DP problem iteratively.

Since an outside observer does not have access to the shock ($\vec{\epsilon}$), the underlying deterministic policy of the expert $\sigma(a | x,\epsilon)$ is not directly measurable. However, if we average decisions across trajectories conditioned on the same state variables we are able to identify the integrated policy. We denote this integrated policy by $\sigma(a|x)\in[0,1]$, the \emph{conditional choice probability} (CCP) of an action being chosen conditioned on state $x$: 
\begin{eqnarray}
\sigma(a|x_t)\triangleq E_{\vec{\epsilon}}\left[ \mathbf{1}\{a\ \textrm{is optimal in state }x_t\}\right],
\end{eqnarray}
where $\mathbf{1}\{\}$ is the indicator function. The event in the indicator function is equivalent to the event:
\begin{align}
\begin{split}
& \left\{r(x_t,a)+\epsilon_{at}+\beta E_{x_{t+1}|a,x_t} \overline{V}(x_{t+1})\geq \right. \\
& \quad \left. r(x_t,a')+\epsilon_{a't}+\beta E_{x_{t+1}|a',x_t} \overline{V}(x_{t+1}),\ \forall a'\neq a \right\}
\end{split}
\end{align}

Expanding the expectation under the TIEV assumption on the shock variable allows CCPs to be solved in closed-form:
\begin{align} \label{eq:ccps}
\begin{split}
\sigma(a|x_t)=\frac{\exp\left(r(x_t,a)+\beta E_{x_{t+1}|x_t,a} \overline{V}(x_{t+1})\right)}{\sum_{a'\in\mathcal{A}} \exp\left(r(x_t,a')+\beta E_{x_{t+1}|x_t,a'} \overline{V}(x_{t+1})\right)}
\end{split}
\end{align}
Notice that \eqref{eq:ccps} is identical to the definition of the policy of the MaxEnt formulation in \cite{ziebart_phd}, which is derived from an entropic prior on trajectories. The CCP is derived by integrating out the TIEV shock variable. 





\section{Conditional Choice Probability - Inverse Reinforcement Learning}

Our aim in Inverse Reinforcement Learning is to find the parameterized reward function $r(\theta)$ for the given MDP/R. We now show how we can leverage \textit{non-parameteric} estimates of choice conditional probabilities to efficiently estimate the parameters $\theta$ of the reward function.  
First, we look at the alternative representation of the \emph{ex-ante} value function which can be derived from CCP estimates. Using this alternative representation we see how to avoid solving the original MDP problem repeatedly.
For convenience, we will write the parameterized reward function as $r_{\theta}(x)$.

\subsection{Model-based Reinforcement Learning with CCPs}

Returning to the definition of the \emph{ex-ante} value function in Equation \eqref{eq:def_exante} and \eqref{eq:exantebellman}, notice that Equation \eqref{eq:exantebellman} is similar to the standard state-value function definition in reinforcement learning. Analogously, define the choice-specific value function $V_a(x_t)$  as:
\begin{align}
\begin{split}
V_a(x_t) &= E_{\vec{\epsilon}_t}\Big[r_{\theta}(x_t,a)+\epsilon_{at} \\
& \qquad +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \Big].\\
\end{split}
\end{align}
Hotz and Miller \yrcite{hotz} show that given \textit{consistent} CCP estimates from the data, equation \eqref{eq:exantebellman} can be estimated as,
\begin{align} \label{eq:exantebellman_2}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\sum_{a\in\mathcal{A}} \sigma(a|x_t) \big\{r_{\theta}(x_t,a)+\epsilon_{at} \\
& \qquad +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]\\
\end{split}
\end{align}
Now, defining the expected shock given that action $a$ is optimal as $\tilde{\epsilon}(a|x_t) = E_{\vec{\epsilon}}\left( \epsilon_{at} |a\ \textrm{is optimal in state } x_t\right)$, we can rewrite \eqref{eq:exantebellman_2} as,
\begin{align} \label{eq:exantebellman_3}
\begin{split}
\overline{V}(x_t) & = \sum_{a\in\mathcal{A}} \sigma(a|x_t) \Big[r_{\theta}(x_t,a)+\tilde{\epsilon}(a|x_t) \\
& \qquad +\beta \sum_{x_{t+1}\in\mathcal{X}} T(x_{t+1}|x_t,a) \overline{V}(x_{t+1})\Big]
\end{split}
\end{align}
Hotz and Miller \yrcite{hotz} further show that $\tilde{\epsilon}(a|x_t)$ depends on CCPs and distribution of $\epsilon$ only. They prove that the mapping between CCPs and choice specific value function is \emph{invertible}. Using this inverse mapping and assuming TIEV distribution for $\epsilon$, we get $\tilde{\epsilon}(a|x_t) = \gamma - \log \sigma(a|x_t)$.

Notice that \eqref{eq:exantebellman_3} is linear in ex-ante value function ($\overline{V}(x_t)$). Thus, we can write a closed form solution for it.
For this we stack the \emph{ex-ante} value function over all states and get,
\begin{align} \label{eq:exantebellman_4}
    \begin{split}
    \overline{\mathbf{V}}=\sum_{a}\mathbf{S}(a) \times \left[\mathbf{R}_{\theta}(a)+\tilde{\bm{\epsilon}}(a)+\beta \mathbf{T}(a) \overline{\mathbf{V}}\right]
    \end{split}
\end{align}
where:
\begin{align}
\overline{\mathbf{V}}=\left[\begin{array}{c}\overline{V}(x_1)\\\vdots\\\overline{V}(x_{|\mathcal{X}|})\end{array}\right]
\mathbf{R}_{\theta}(a)=\left[\begin{array}{c}r_{\theta}(x_1,a)\\\vdots\\ r_{\theta}(x_{|\mathcal{X}|},a)\end{array}\right]
\end{align}
\begin{align}
\mathbf{T}(a)=\left[\begin{array}{ccc}
T(x_1|x_1,a),\dots,T(x_{|\mathcal{X}|},x_1,a)\\
\vdots\\
T(x_1|x_{|\mathcal{X}|},a),\dots,T(x_{|\mathcal{X}|},x_{|\mathcal{X}|},a)\\
\end{array}\right]
\end{align}
Similarly, writing matrices $\mathbf{S}(a) = [\sigma(a|x); x\in\mathcal{X}]$ and $\tilde{\bm{\epsilon}}(a)=[\tilde{\epsilon}(a|x); x\in\mathcal{X}]$.
Rearranging the terms in \eqref{eq:exantebellman_4} we get,
\begin{align}
    \begin{split}
    \overline{\mathbf{V}}-\sum_{a}\mathbf{S}(a) *\left[ \beta \mathbf{T}(a) \overline{\mathbf{V}}\right]=\sum_{a}\mathbf{S}(a) *\left[ \mathbf{R}_{\theta}(a)+\tilde{\bm{\epsilon}}(a)\right]
    \end{split}
\end{align}

Defining $\lambda$ as a $1\times|\mathcal{X}|$ vector of ones, we can write the closed form solution for $\overline{\mathbf{V}}$ as,
\begin{align} \label{eq:exante_inversion}
\begin{split}
\overline{\mathbf{V}} &=\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1} \\
& \qquad \times \left[\sum_{a}\mathbf{S}(a) *\left[ \mathbf{R}_{\theta}(a)+\tilde{\bm{\epsilon}}(a)\right]\right]
\end{split}
\end{align}
Notice that the inverse matrix in \eqref{eq:exante_inversion} is similar to the inverse matrix required for value function estimation in value iteration (Theorem 9.2 \cite{ziebart_phd}). The computational complexity of solving the MDP is equivalent to known model-based RL algorithms.


\subsection{Linear Reward Parametrization}

The above closed form equation holds in the most general case of a non-parametric reward function. The equation can be further simplified for a linear reward parameterization $r_{\theta}(x,a) = \theta^{\top} z(x,a) $ where $z(x, a)$ is a feature representation for a given state-action pair. Stacking up the feature values we define $\mathbf{Z}(a) = [z(x, a); x \in \mathcal{X}]$. We can also stack the choice-specific value function to get $\mathbf{V}_a$. From \eqref{eq:exantebellman_4} we get
\begin{align} \label{eq:exante_linear}
    \begin{split}
    \mathbf{V}_a=\mathbf{R}_{\theta}(a)+\tilde{\bm{\epsilon}}(a)+\beta \mathbf{T}(a) \sum_{a'}\mathbf{S}(a')\mathbf{V}_a'
    \end{split}
\end{align}
Since $\mathbf{R}_{\theta}(a) = \mathbf{Z}(a)^T\theta$ for linear parameters, it is easy to show that \eqref{eq:exante_linear} can be written as $\mathbf{V}_a = \hat{\mathbf{Z}}(a)^T\theta + \hat{\bm{\epsilon}}(a)$, where we define
\begin{align} \label{eq:choice_specific_Z_tilde}
\begin{split}
    \hat{\mathbf{Z}}(a) = \mathbf{Z}(a) + \beta\mathbf{T}(a)\sum_{a'}\mathbf{S}(a')\hat{\mathbf{Z}}(a') \\
    \hat{\bm{\epsilon}}(a) = \tilde{\bm{\epsilon}}(a) + \beta\mathbf{T}(a)\sum_{a'}\mathbf{S}(a')\hat{\bm{\epsilon}}(a')
\end{split}    
\end{align}
Again both $\hat{\mathbf{Z}}(a)$ and $\hat{\bm{\epsilon}}(a)$ can be estimated via backwards recursion and is computationally similar to known model-based RL algorithm.

\begin{figure*}[ht]
\begin{minipage}[t]{0.45\textwidth}
  %\vspace{0pt}  
  \begin{algorithm}[H]
    \caption{CCP-IRL algorithm} \label{algo:ccp_irl_algorithm}
    \begin{algorithmic}[1]
        %\Procedure{CCP-IRL}{$\mu_D,f, S, A, T, \gamma$}
        \STATE {\bfseries Input:} expert features $\mu_D$, ccp $\mathbf{S}$, transition $\mathbf{T}$, discount $\gamma$
        \STATE $\theta^{(0)} \gets$ init\_weights
        \STATE $\mathbf{M} \gets \left[\mathbf{I}-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \gamma \mathbf{T}(a)  \right]\right]^{-1}$ 
        \STATE $\tilde{\bm{\epsilon}} \gets \gamma - \log \mathbf{S}(a)$
        \FOR{$i\gets 1, n$}
            \STATE $R^{(i)} \gets \mathbf{R}(\theta^{(i)}))$
            \STATE $V^{(i)} \gets \mathbf{M} \times \sum_{a}{\mathbf{S}(a) \times \left[ R^{(i)} +\tilde{\bm{\epsilon}}\right]}$
            \STATE $\pi_{\theta}^{(i)}(a|x) \gets e^{V^{(i)}(x_a) - V^{(i)}(x)}$
            \STATE $E[\mu^{(i)}] \gets $ FORWARD PASS()
            \STATE $\theta^{(i)} \gets \theta^{(i-1)} - \alpha \times (\mu_D - E[\mu^{(i)}])$
        \ENDFOR
    \end{algorithmic}
  \end{algorithm}
\end{minipage}%
\qquad
\begin{minipage}[t]{0.45\textwidth}
  %\vspace{0pt}
  \begin{algorithm}[H]
    \caption{Forward Pass} \label{algo:forward_pass_algorithm}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} policy $\pi_{\theta(a|x)}$
        \STATE $D^{(0)}(x) \gets P(x_i = x_{initial})$
        \FOR{$i\gets 1, n$}
            \STATE $D^{(i-1)}(x_{goal}) \gets 0$
            \STATE $D^{(i)}(x) \gets D^{(i)}(x) + \pi_{\theta}(a|x') D^{(i-1)}(x')$
        \ENDFOR
        \STATE $D(x) \gets \sum_{i}D^{(i)}(x)$ 
        \STATE \textbf{return} $D(x)$
    \end{algorithmic}
  \end{algorithm}
\end{minipage}
\end{figure*}


\subsection{Inverse Reinforcement Learning with CCPs}
We now look at how to efficiently solve the IRL problem using the above value function representation. 
We compare against MaxEnt-IRL, since, as shown above, the original ex-ante value function representation is exactly similar to MaxEnt-IRL.
MaxEnt-IRL involves estimating the parameters $\theta$ of the reward function using an iterative procedure.
At every step of the iteration we need to estimate the value function; this requires solving the MDP problem using a model-based RL algorithm. Since this happens repeatedly it can be prohibitively expensive.

In contrast, the above value function representations (\eqref{eq:exante_inversion} and \eqref{eq:exante_linear}) can be used to estimate the value function much more efficiently. To see this, notice that in \eqref{eq:exante_inversion} we can pre-calculate the inverse matrix once. Alternately, for linear parameters we can pre-calculate \eqref{eq:choice_specific_Z_tilde}. Both of these pre-computations are similar to solving the MDP problem \textit{once}. Given these pre-computed values we can calculate the value function, for all different values of $\theta$, using simple matrix operations only. Thus, in contrast to solving a full MDP problem using model-based RL algorithm, we only require simple matrix operations at every step of the iterative procedure. We call our method Conditional Choice Probability-Inverse Reinforcement Learning (CCP-IRL).

The pseudo-code for CCP-IRL for the most general case is given in Algorithm~\ref{algo:ccp_irl_algorithm}.
The inverse matrix $\mathbf{M}=\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1}$ is independent of $\theta$ and is pre-computed once for all iterations (Line 3).
At every step of the iteration, we calculate the reward for the given $\theta$ (Line 6). Given these, computing $\mathbf{\overline{V}}$ requires simple matrix operations only (Line 7). Thus, we avoid solving the complete MDP problem at every step of the optimization scheme.
Intuitively, the inverse matrix $\mathbf{M}$ computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}.

\subsection{Estimating CCPs}

We also note how to calculate the initial CCP estimates ($\mathbf{S}$). In their simplest form, the initial CCP estimates can be computed directly from $N$ expert trajectories each with $T_i$ time periods:  $\mathcal{D} = \{(a_{it},x_{it})_{t=0}^{T_i}:i=1,\dots,N\}$ in tabular form. An initial maximum likelihood estimate can be computed by maintaining a table over state-action pair occurrences. There is no restriction however on the representational form of CCPs and more complex functions (\emph{e.g.,} Deep Networks) can be used to represent CCPs. 

From the robotics perspective, the process of learning the CCPs is exactly Imitation Learning (IL), where the policy is learned directly from demonstrations without making the reward function explicit. The CCP-IRL framework can then be interpreted as, an imitation learning phase followed by a inverse reinforcement learning phase that makes use of a fully specified policy to estimate the reward function.


\subsection{Complexity Analysis}
The main computation in CCP-IRL is to estimate the inverse matrix in \eqref{eq:exante_inversion} or alternately estimate $\tilde{\mathbf{Z}}$ and $\tilde{\mathbf{E}}$ \eqref{eq:choice_specific_Z_tilde} for linear parameters. Both of these are computationally similar to solving the MDP problem \emph{once} \cite{?Putterman}. 
In contrast, the main computation in MaxEnt-IRL is solving the same MDP problem \emph{repeatedly}.
Thus, assuming a total of $N$ iterations till convergence and $T$ iterations for each backwards recursion; MaxEnt-IRL takes a total of $O(N\times T \times|A|\times|S|)$ \cite{ziebart_phd}. For CCP-IRL assuming the matrix inversion can be performed with state of the art matrix inversion method, we get a corresponding runtime of $O(|S|^{2.4}+T\times|A|\times|S|)$ for the general case and $O(T\times|A|\times|S|)$ for the linear reward formulation.

We also look at how for large state spaces, we can avoid using matrix inversion and rather estimate the inverse matrix \eqref{eq:exante_inversion} using successive approximations.
Defining $A \equiv \left(I- \beta \sum_{a}(\mathbf{S}(a) \lambda) *\left[ \mathbf{T}(a)  \right]\right)^{-1}$ we can write it as $A = (I - \beta F)^{-1}$ where $F$ is used as a shorthand for notational convenience. Premultiplying both sides with $(I - \beta F)$ we get $(I - \beta F)\times A = I$ which finally gives us $A = I + \beta F A$. We can now use this last equation to estimate the inverse matrix $A$ by successive approximations. Although, from a computational perspective sometimes estimating the inverse matrix can be fast itself, Zeibart \yrcite{ziebart_phd} discusses some of these considerations. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Experiments}

In this section we empirically validate,
(1) the computational efficiency of CCP-IRL and (2) the underlying assumptions of consistent CCP estimates \emph{i.e.,} we show the data requirement for CCP-IRL.
To this end we evaluate the performance of CCP-IRL on three standard IRL tasks. Since CCP-IRL is an extension of MaxEnt-IRL \cite{ziebart}, we use it as a baseline method to compare our results on the benchmark tasks. Previously, both linear \cite{ziebart} \cite{ziebart2010modeling} and non-linear \cite{wulfmeier2015maximum} formulations of MaxEnt-IRL have been used to estimate the reward functions. Hence, we discuss results for both formulations of CCP-IRL. For the former, we focus on problems of navigation in a traditional Gridworld setting with stochastic dynamics, while for the latter we choose the Objectworld task as described in \cite{Levine2013}. We will also like to note that given the additional complexity in applying MaxEnt-IRL to continuous domains we only focus on discrete tasks in our experiments.
% For each experimental scenario, we follow the pattern of: (1) Generate a few demonstration trajectories for the task; (2) Apply IRL (MaxEnt or CCP) algorithm to learn the reward function; and (3) Compare the learned reward function against the true reward. 
For comparative analysis, we use both qualitative and quantitative results.
For qualitative analysis, we directly compare the visualizations of the inferred reward functions for both CCP-IRL and MaxEnt-IRL.
For quantitative comparison, we use negative log likelihood (NLL) \cite{kitani2012activity} and expected value difference (EVD) \cite{levine2011nonlinear} as the evaluation criterion. NLL is a probabilistic comparison metric and evaluates the likelihood of a path under the predicted policy. For a policy ($\pi$), NLL is defined as,
\begin{align}
NLL(\pi) = E_{\pi(a|s)}\big[-\log \prod_{t} \pi(a_{t}|s_{t}) \big]
\end{align}
% Unfortunately, NLL cannot capture the underlying stochastic nature of the expert policy since it assumes deterministic transitions in expert trajectories.
As another metric of success, similar to related works \cite{Levine2013, wulfmeier2015maximum}, we use expected value difference (EVD). EVD measures the difference between the optimal and learned policy by comparing the value function obtained with each policy under the \textit{true reward} distribution.
Further, to verify the computational improvement using CCP-IRL, we observe the time taken by each algorithm as well as the number of iterations it takes for each algorithm to converge. We show that our algorithm is able to achieve similar qualitative and quantitative performance with \textit{much less} computational time.
%For computational analysis all experiments were run on a PC with an Intel Xeon CPU E5-2660 v3 2.60 GHz (10 cores) processor with an NVIDIA TITAN X (Pascal) GPU.


\subsection{Gridworld: Evaluating Linear Rewards} 

To show the computational efficiency of CCP-IRL for linear parameterizations we focus on the standard gridworld experiments, since it is easy to formulate linear reward function in this setting. We empirically validate the computational efficiency of CCP-IRL. Additionally, we qualitatively compare the reward functions inferred by both algorithms. Finally, we discuss how CCP-IRL requires consistent CCP estimates, which in turn depend on the amount of expert demonstrations available. Since CCP estimates are calculated from expert trajectories we observe how CCP-IRL's performance depends on the amount of expert trajectories.

\begin{table}[t]
\centering
\def\arraystretch{1.0}% 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
N & Cell Size & MaxEnt (sec) & CCP (sec) & Speedup \\\hline

32 & 8 & 635.63 & \textbf{266.18} & $3\times$ \\
32 & 4 & 584.30 & \textbf{283.81} & $2\times$ \\
64 & 8 & 3224.97 & \textbf{1024.42} & $3\times$ \\
\hline
\end{tabular}
\caption{\textbf{Computation time} (averaged over multiple runs) comparison between MaxEnt and CCP for gridworld settings. Each experiment was run for 50 iterations.}
\label{table:table_results_macro_cells}
\end{table}



\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/macro_cells/grid_16_macro_2_lr_05/ll_ccp_vs_maxent_per_traj.pdf}}&
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/macro_cells/grid_16_macro_2_lr_05/evd_multiple_tries.pdf}}
    \end{tabular}
    \caption{Results for gridworld of size 16 with macro-cells of size 2. Left: Minimum NLL results with varying number of trajectories. Right: Expected Value Difference results. For few trajectories CCP-IRL shows much larger variance as compared to MaxEnt-IRL.}
    \label{fig:img_maxent_vs_ccp_gridworld_macro_cell}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{ccc}
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/true_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/maxent_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/ccp_reward_2_trim.pdf}} \\
    True Reward & MaxEnt & CCP \\
    \end{tabular}
    \caption{ Reward distribution for macro cells with gridsize 8 and macro cell size 2 using 10 trajectories. Dark - high reward, Light - low reward. }
    \label{fig:img_reward_map_gridworld_macro_cell}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{c}
    \MSHangBox{\includegraphics[width=0.4\textwidth]{images/gridworld/fixed_target_wind_30/timeit_maxent_vs_ccp_grid_32_per_discount.pdf}}
  \end{tabular}
    \caption{Computation time (in seconds and averaged over multiple runs) comparison between MaxEnt and CCP for Gridworld with gridsize of 32. Each experiment was run for 50 iterations. }
    \label{fig:img_gridworld_maxent_vs_ccp_time_discount}
\end{figure}

For this we use the macro-cell Gridworld environment \cite{abbeel2004apprenticeship}. This is slightly more complex form of the standard RL gridworld navigation problem and provides a good basis to compare the two algorithms. 
In this setting, the $N \times N$ grid is divided into non-overlapping square regions (macro-cells). Each region contains multiple grid cells and each cell in a region shares the same reward. Similar to \cite{abbeel2004apprenticeship}, for every region we select a positive reward $r \in (0, 1)$ with probability of $0.1$ and $r = 0$ with probability $0.9$. This reward distribution leads to positive rewards in few macro cells which results in interesting policies to be learned and hence requires more precise CCP estimates to match expert behavior.

Since all the cells in the same region share the same reward, our feature representation is a one-hot encoding of the region that cell belongs to \emph{e.g.}, in a grid world of size $N=64$ and macro cell of size 8 we have 64 regions and thus each state vector is of size 64. As before, we assume a stochastic wind with probability $0.3$ and the agent can move only in four directions. 

We analyze the performance of both algorithms given different amounts of expert trajectories.
Figure \ref{fig:img_maxent_vs_ccp_gridworld_macro_cell} compares the NLL and EVD results against increasing number of expert trajectories.
As seen above, both algorithms show poor performance given very few trajectories (< 20 trajectories).
However, with moderate number of trajectories MaxEnt-IRL approaches expert behavior while CCP-IRL is still comparatively worse.
Finally, with sufficiently large number of trajectories (> 60) both algorithms converge to expert behavior. 
CCP-IRL's poor performance with few expert demonstrations reflect its dependence on sufficient amount of input data.
Since CCP estimates are calculated from input data CCP-IRL needs a sufficient (relatively larger than MaxEnt-IRL) amount of trajectories to get consistent CCP estimates.
% This is expected given that we need sufficient amount of trajectories to get consistent CCP estimates. 
% This requirement for more input trajectories can be solved for robotic tasks with autonomous data aquisition.

% These results show that CCP-IRL's performance on the input trajectories to get consistent CCP estimates.  since consistent CCP estimates require sufficient input trajectories 
%of CCPs which in turn are directly inferred from the data.
%which requires a sufficient number of trajectories to arrive at consistent estimates.
%Many robotic tasks use simulations or autonomous techniques to gather data, in which case the requirement for large number of trajectories can be easily satisfied. 

We also qualitatively compare the rewards inferred by both algorithms given few trajectories in 
Figure \ref{fig:img_reward_map_gridworld_macro_cell}. Notice that the darker regions in the true reward are similarly darker for both algorithms. Thus, both algorithms are able to infer the general reward distribution. However, MaxEnt-IRL is able to match the true reward distribution at a much finer level (since less discrepancy compared to the true reward) and hence the underlying policy more closely as compared to CCP-IRL.
Thus, given few input trajectories MaxEnt-IRL performs better than our proposed CCP-IRL algorithm.
% inferred reward function is worse as compared to MaxEnt-IRL.

%Finally we compare the computation cost for both algorithms across large state spaces ($|S| \in \{10^3, 10^4\}$) in Table \ref{table:table_results_macro_cells}.
%As before, CCP-IRL is almost 2x as fast as MaxEnt-IRL across state spaces. Additionally, for the last two rows the only difference in the experiments is in terms of the feature size, since we get similar performance improvement for both settings this indicates that the speedup in CCP-IRL is independent of the dimension of $z(a, x_t)$. This is important since in CCP-IRL the backwards recursion happens on the $z(a, x_t)$ vector while in MaxEnt-IRL it happens on the scalar reward which might give an impression of the former being much more computationally expensive.

We verify the computation advantage for CCP-IRL across large state spaces ($|S| \in \{10^3, 10^4\}$) in Table~\ref{table:table_results_macro_cells}. As seen before, CCP-IRL is atleast 2$\times$ faster than MaxEnt-IRL. Also, its computational efficiency increasing for larger state spaces.
%than MaxEnt-IRL across different sized state spaces.

% Additionally, we also observe the affect of feature dimension on computation complexity. Notice the last two rows of Table \ref{table:table_results_macro_cells} have same state space but different feature sizes. Given these differences we still get similar (~2x) performance improvement for CCP-IRL which indicates that this speedup is largely independent of the dimension of $z(a, x_t)$ \eqref{eq:choice_specific_linear_param}. This is important since in CCP-IRL the backwards recursion happens on the $z(a, x_t)$ vector while in MaxEnt-IRL it happens on the scalar reward which might give an impression of the former being much more computationally expensive.

\begin{table}[t]
\centering
\def\arraystretch{1.4}% 
\begin{tabular}{|c|c|c|c|}
\hline
Experiment Setting & MaxEnt & CCP & Speedup \\\hline

Grid size: 16, C = 2 & 1622.63 & \textbf{296.43} & $5\times$ \\
Grid size: 32, C = 2 & 9115.50 & \textbf{1580.22} & $6\times$ \\
Grid size: 16, C = 8 & 2535.38  & \textbf{545.95} & $5\times$ \\
Grid size: 32, C = 8 & 19445.66 & \textbf{4799.02} & $4\times$ \\
\hline
\end{tabular}
\caption{Computation time (in seconds and averaged over multiple runs) comparison between MaxEnt and CCP for Objectworld. Each experiment was run for the same number of iterations with similar settings. }
\label{table:table_results_objectworld}
\end{table}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.23\textwidth]{images/objectworld/grid_16_object_8/test_ll.pdf}}
    \MSHangBox{\includegraphics[width=0.23\textwidth]{images/objectworld/grid_16_object_8/evd_maxent_vs_ccp.pdf}}
  \end{tabular}
    \caption{Results on ObjectWorld with gridsize of 16 and 2 colors. Left: NLL results on test trajectories. Right: Expected Value Difference results. }
    \label{fig:img_objectworld_maxent_vs_ccp_lr_01}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/objectworld/transfer/ccp_no_var_maxent_no_var.pdf}}
    \MSHangBox{\includegraphics[width=0.24\textwidth]{images/objectworld/timeit_maxent_vs_ccp_grid_16_per_iter.pdf}}
  \end{tabular}
    \caption{Left: Results for transfer experiment using MaxEnt and CCP formulation on Objectworld with gridsize of 16 and 2 colors. Right: Time variance between MaxEnt-IRL and CCP-IRL with increasing number of iterations. As expected CCP-IRL shows little computation increase with larger number of iterations.}
    \label{fig:img_objectworld_maxent_vs_ccp_time_results}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{ccc}
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/true_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/maxent_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/ccp_reward_trim.pdf}} \\
    True Reward & MaxEnt & CCP \\
    \end{tabular}
    \caption{ Reward distribution for Objectworld with gridsize 8 using 30 trajectories and 5 colors. Dark - low reward, Light - high reward. We also plot the inner and outer color of each object. \textit{Pink} - color 1 \textit{Orange} - color 2, other colors are distractors. \textit{Figure best viewed in electronic version.} }
    \label{fig:img_reward_map_objectworld}
\end{figure}

\subsection{Objectworld: Evaluating Non-Linear Rewards}

We now look at CCP-IRL's performance when the true reward function is a non-linear parameterization of the feature vector.
For this, we use the Objectworld \cite{levine2011nonlinear} environment since the reward function is a non-linear function of state features.
Similar to related work \cite{wulfmeier2015maximum}, we use a Deep Neural Network (DNN) as the non-linear function approximator.
% Similar to previous works \cite{wulfmeier2015maximum} we use a Deep Neural Network as the non-linear function approximator for the reward function.
As before, we verify both (1) the computational advantage provided by CCP-IRL (DeepCCP-IRL) and (2) the data requirement for CCP-IRL in the above scenario.

The Objectworld environment consists of a grid of $N \times N$ states. At each state the agent can take 5 actions, including movement in 4 directions and staying in place. Spread through the grid are random objects, each with an inner and outer color. Each of these colors is chosen from a set of $C$ colors. The reward for each cell(state) is positive if the cell is within distance 3 of color 1 and distance 2 of color 2, negative if only within distance 3 of color 1 and zero in all other cases. For our feature vector we use a continuous set of values $x \in \mathbb{R}^{2C}$, where $x_i$ and $x_{i+1}$ is the shortest distance from the state to the \emph{i'th} inner and outer color respectively. Since the reward is only dependent on two colors, features for other colors act as distractors. 

We use DeepMaxEnt-IRL \cite{wulfmeier2015maximum} as the baseline, using similar deep neural network architecture for both algorithms. Precisely, we use a 2-layer feed-forward network with rectified linear units. We use the Adam \cite{kingma2014adam} optimizer with the initial learning rate set to $10^{-3}$.

We quantitatively analyze the performance of our proposed DeepCCP-IRL algorithm. 
Figure \ref{fig:img_objectworld_maxent_vs_ccp_lr_01} compares the NLL and EVD results for both algorithms. Notice that as observed before, with few expert trajectories both algorithms perform poorly. However, DeepMaxEnt-IRL matches expert performance with moderate number of trajectories ($\approx 20$), while DeepCCP-IRL requires relatively large number of trajectories ($\approx 40$).
This is expected since CCP-IRL requires larger number of expert trajectories to get consistent CCP estimates.

Also, we qualitatively look at the inferred reward to verify how well the DNN is able to approximate the non-linear reward. Figure \ref{fig:img_reward_map_objectworld} plots the inferred rewards against the true reward function. Notice that both algorithms capture the non-linearities in the underlying reward function and consequently match the expert behavior. Thus, a deep neural network suffices as a non-linear function approximator for CCP-IRL. 

We now analyze the computation gain in the non-linear case. Table \ref{table:table_results_objectworld} shows the computation time for different sized state spaces and different sized feature vectors. Notice that DeepCCP-IRL is almost 5$\times$ as fast as DeepMaxEnt-IRL across small and large state spaces. Thus we see that CCP-IRL provides a much larger computation advantage for the non-linear case, which we believe is because the objectworld MDP problem is more complex than the above grid world experiments. 
This results in both algorithms requiring larger number of iterations until convergence which leads to a large computational increase for DeepMaxEnt-IRL as compared to DeepCCP-IRL. 
This computational increase with larger number of iterations is also shown in Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} (Right).
Notice that as the number of iterations increase, our proposed DeepCCP-IRL algorithms shows minor computational increase as compared to DeepMaxEnt-IRL. 
Thus, for significantly complex MDP problems which require large number of iterations our proposed CCP-IRL algorithm should require much less computation time compared to MaxEnt-IRL.
% This results in each DP step taking longer for Objectworld which leads to large computation time for DeepMaxEnt-IRL.
%Also, as seen in Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} (Right), the computation gain for CCP-IRL holds across different number of iterations. This shows that the performance gain from CCP-IRL is largely independent of the number of iterations required to converge.

% Additionally, to better understand the above computation gain we plot the time taken against number of iterations in figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results}. As expected, DeepCCP-IRL shows negligible increase while DeepMaxEnt-IRL shows an almost exponential increase in computation time. Since DeepMaxEnt-IRL needs to solve the original MDP at every iteration step.

% We next look at the data requirement for DeepCCP-IRL. Notice Figure \ref{fig:img_objectworld_maxent_vs_ccp_lr_01}, as before we see that with fewer trajectories DeepCCP-IRL performs much worse compared to DeepMaxEnt-IRL. As expected this is a direct consequence of not having consistent CCP estimates given the insufficient amount of input trajectories.

% As seen above both DeepMaxent-IRL and DeepCCP-IRL show similar performance for both metrics with increasing number of trajectories. Thus both algorithms are able to recover the underlying reward function given sufficient trajectories. As before we see that with fewer trajectories DeepCCP-IRL performs much worse compared to DeepMaxEnt-IRL which is expected given that we need sufficient data for consistent CCP estimates. Also, Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} shows the results for the transfer experiment which reinforce the need for more data for better results.


%\textcolor{blue}{(KK: You mention the transfer experiment in the beginning but there is no discussion of it in this section... right?)}

%\textcolor{blue}{(KK: The entire section has almost no qualitative analysis. No visualization of the reward function that is learned. No visualization of transfer of the reward function. No visualization of trajectories. No visualization of the actual gridworld for object world.)}


%\textcolor{blue}{(KK: There should be a section on how the state space affects the results both in terms of accuracy and computational cost. I think that it is in the graphs but there is no explict section on it. The reader wants to know if we get the same speed up for large state spaces.)}

%\textcolor{blue}{(KK: Also, what about the entire discussion on the number of examples and coverage of the state space? What do the results say about this? Do we need examples from the entire space for CCP-IOC to work?)}

%\subsection{State and Action Representations}

%The traditional MaxEnt IOC model has taken the approach of state-based policy estimation using locations as states and directions as actions (N, NE, E, SE, and so on). In our CCP-based estimation, we instead choose to define states as the discrete feature vectors at each location $s = f(x,i)$, where $\vec{x}$ is the location vector $[x~y]$, and $i$ is a trajectory. Feature vectors change from trajectory to trajectory - as such, our state space contains a separate state for each trajectory-location pair; the number of states at which we estimate CCPs grows linearly with the number of trajectories. 

%Actions are feature vectors (states) that are exactly one grid position away from the current state. Note that the cardinality of the action space does not change as related to MaxEnt IOC but, instead, our interpretation (as a future state) of what an action is.
%
%We make the decision to redefine states and actions as above with the intent that rewards \textcolor{blue}{KK: SR why are we talking about rewards in the state and action section?}\textcolor{purple}{SR: KK wanting the reward to be a function of features is \emph{the reason} we choose to redefine states and actions.}\textcolor{blue}{KK: you don't have to argue with me on this point...I wrote the paper. It's the writing that needs to be fixed so that the connection is clear. I don't think that its obvious that rewards are functions of features in this paper.}, as in \cite{kitani2012activity}, are functions of feature vectors - \emph{not} locations; and that features of future states impact current agent choice - \emph{not} direction of movement \textcolor{blue}{KK: SR this is because the constant feature provides this implicitly when using value iteration. Direction does matter.}. This mirrors the underlying reasoning of \cite{kitani2012activity} but does so by explicitly defining the relationship between features and rewards \textcolor{blue}{KK: SR the explicit relationship is never made clear in terms of theory/implementation details.}. 
%
%\subsection{Action Equivalence and Base Action Representation}
%
%Because \textcolor{blue}{KK: don't start with 'because'} action space is an $F$-dimension real-valued vector \textcolor{blue}{KK: SR be careful about `space' and `vector' are being used}, we cannot practically use equivalence \textcolor{blue}{KK: SR what do you mean here by equivalence? why is it not practical?} in CCP estimation as in Equation \ref{eq:ccp-est}. Instead, we use a loosened version of equivalence \textcolor{blue}{KK: SR why is this ok?} where two actions are equivalent \textcolor{blue}{KK: SR why do we need this equivalence?} when the L1 distance \textcolor{blue}{KK: SR why L1?} between them is less than $a_{bin}$ \textcolor{blue}{KK: SR use a different variable, overloaded use with actions, sounds like this is a scalar threshold right?}. The consistency of this estimator \textcolor{blue}{KK: SR new term here `consistency of estimator' which is not defined} is guaranteed as long as $a_{bin}\rightarrow 0$ while the size of the sample increases without bound. This is identical to requirements all non-parametric estimators must fulfill for consistency.
%
%% \textcolor{purple}{(SR: JG, a couple sentences here about why this loosened version of equivalence does not compromise statistical soundness would be good)}
%
%We treat base action $a_0$ in a similar manner\textcolor{blue}{KK: SR remind the reader of what the base action is.}. In particular, we do not necessarily have an action at each state that is equivalent to $a_0$ \textcolor{blue}{KK: SR why?}; because a non-zero CCP is required for the action $a_0$ at every state \textcolor{blue}{KK: SR why?} for which we want a reward estimate \textcolor{blue}{KK: SR don't need to mention at this point? it breaks the logical flow}, we choose an arbitrary action $a_0$ that has non-zero CCP in a maximal number of states \textcolor{blue}{KK: SR the meaning of maximal number of states is not clear.}. But because \textcolor{blue}{KK: `But because' ... you're killing me!} we cannot rely on real-valued vector equivalence\textcolor{blue}{KK: SR unclear what this means?}, we use the most similar action (within an L1 bin size $a_{bin}$ of $a_0$), and call it $\tilde{a_0}$ \textcolor{blue}{KK: SR you explain what you are doing in a mechanical way but the reader is going to have a hard time understanding why this is the right thing to do. A research paper is in many ways an argumentative piece of writing, protocols and procedures need to be carefully motivated}.
%
%
%
%\subsection{CCP-based Reward Function Estimation}
%\label{sec:ccp-exp}
%
%\textcolor{blue}{KK: SR to be consistent, let's use CCP IOC.}
%
%For training our CCP-based model \textcolor{blue}{KK: SR what are we training exactly? remind the reader, we are trying to estimate the reward function right? That's the goal of IOC}, we use $N = 14$ ground truth observed trajectories across the state space \textcolor{blue}{KK: SR the state space has been redefined above. I think that you mean the locations of the scene. The state space is now all possible feature vectors right conditioned on the trajectory ID?} with $F = 34$ feature maps for each trajectory\textcolor{blue}{KK: SR is it for each trajectory or for each scene?} and compute the conditional choice probabilities for each state as follows:\textcolor{blue}{KK: SR ops, missing something here.}
%
%Instead of quantizing the space of $F$-dimensional real-valued states for computation of the conditional choice probabilities (which would cause the complexity to grow exponentially in state-space size), we instead compute point estimates of CCPs using Equation \ref{eq:ccp-est} at all present states \textcolor{blue}{KK: SR `present states' needs to be explained?} (assuming that each location-trajectory pair \textcolor{blue}{KK: SR another new term 'location-trajectory pair' that needs to be explained?} is a unique feature vector and, therefore, is a unique state \textcolor{blue}{(KK: SR not clear what this means? unique in what sense? you mean you can't visit the same state twice?)}), interpolating as necessary for other states.
%\textcolor{blue}{(KK: SR probably a more succint way to write this paragraph is to say that you are going to use a continuous distribution over the state space using KDE)}
%
%\textcolor{blue}{(KK: SR I think that this section needs to be fleshed out with mathematical notation because there is too much uncertainty in the text. It will be super helpful to the reader to be more concrete. I think that the state space for the CCP is actually location but internally you are using the feature vector representation...is that correct? You can make this concrete by writing out the actual KDE you use in terms of a feature vector.)}
%
%\textcolor{blue}{(KK: SR ok so this is going to confuse people. Above you say that the state representation is not location anymore but rather features vectors. This gives the impression that x= feature vector. Now we are talking about (x,y) locations again. This part needs to be clear. The state of the policy is still location but the internal representation for the KDE is a feature vector right?)} We do this \textcolor{blue}{(KK: SR this?)} computation for all locations $(x,y)$ in each trajectory $t$ (for each action possible for each state), which gives us an $N m_x \times A$ matrix of CCPs, with rows summing to 1 \textcolor{blue}{(KK: SR need to describe why N affects the size of the CCP table. From the theory section, CCP is just an integration of state-actions pairs over many trajectories. Nothing about storing a CCP for each trajectory)}. Because \textcolor{blue}{(KK: SR remove)} each state's CCP estimation is independent of other states, we can parallelize across state estimation; we do so by assigning one row of location space per thread\textcolor{blue}{(KK: SR this is too much detail)}. 
%
%\textcolor{blue}{(KK: SR actually now that I've read the entire section. This portion is actually about building the CCP table and the reward estimation. Consider new headings. At any rate, the reader at this point is thinking... so when are we getting to the actual experiments. There seems to be a lot of engineering involved in all of this.)}
%
%\textcolor{blue}{(KK: stopping here at the moment.)}
%
%We can then compute $\mathbf\Gamma$ and $\mathbf T$, the transition matrix. For computing $\mathbf\Gamma$, we use action $\tilde{a_0}$ \textcolor{blue}{(KK: this variable needs to be clarified above.)}. Because the transitions we are using are deterministic, we use an $m_x \times m_x$ matrix where element $a_{i,\tilde{a_0}}$ is 1 and all other elements are 0, where $\tilde{a_0}$ is the future feature vector (state) reached by choosing action $\tilde{a_0}$ from state $i$. \textcolor{blue}{(KK: mx is the cardinality of the state space right (number of pixels)? This is actually a big waste of memory if you are actually storing the mx x mx matrix since the transitions are sparse. minor point.)}
%
%To estimate the zero-value function \textcolor{blue}{(KK: this is a new term, I think this is called the choice-specific value function for the base action in the paper. Need to be consistent to avoid confusion.)}, we use Equation \ref{eq:v0_fp}. We choose to use this instead of the inversion in Equation \ref{eq:v0_inv} because of the size of the state space \textcolor{blue}{(KK: be explicit, you mean that the state space is too big right?.)} - it is faster and more practical to reach a fixed point on the zero-reward action value function. \cite{rust_theory} proves that this mapping has a unique fixed point. We use repeated matrix multiplication to reach an equivalence with an epsilon of L1-norm $\tau$ \textcolor{blue}{(KK: need to be explained. Write out the recursion equation for the reader to make it clear.)}.
%
%We can then simply compute the value function using Equation \ref{eq:v_backout}, for each state-action pair, and back out the reward function.
%
%\begin{table}[t]
%    \centering
%\begin{tabular}{cc}
%    \begin{tabular}{r|l}
%{\bf Parameter} & {\bf Value} \\
%\hline
%$h$ & 0.8 \\
%$a_0 ((x,y),t)$ & ((80,130),0) \\
%$\beta$ & 0.95 \\
%$\tau$ & $10 \times 10^{-10}$ \\
%$a_{bin}$ & 0.3
%    \end{tabular}
%    
%    \hspace{2.0cm}
%    
%    \begin{tabular}{|c|c|c|}
%    \hline
%    Forecasting & Max-Ent IOC & CCP-IOC \\
%    \hline
%    walk (A) & 1.623 & 2.032 \\
%    walk (B) & - & - \\
%    \hline
%    \end{tabular}
%    
%\end{tabular}
%    \caption{Caption??}
%    \label{tab:my_label}
%\end{table}
%
%
%
%%\subsection{Pedestrian Trajectory Forecasting Results}
%%
%\textcolor{blue}{(KK: Please fill out this part...)}
%
%VIRAT 1 dataset forecasting log loss (MaxEnt IOC vs CCP IOC)
%
%VIRAT 2 dataset forecasting log loss (MaxEnt IOC vs CCP IOC)
%
%
%
%\subsection{CCP-based Reward Function Subsampling Analysis}
%
%\textcolor{blue}{(KK: This experiment is not motivated well. Why do we need to do this experiment? explain to the reader for improved readability.)}
%We also perform subsample analysis \textcolor{blue}{(KK: please explain what you mean by subsample analysis.)} on the CCP-based reward function using random subsamples of state-action pairs. These pairs do not necessarily come from the same trajectory $t$ \textcolor{blue}{(KK: why is this important?)}. Using each subsample, we can determine an L2 distance to the "true" reward estimation function \textcolor{blue}{(KK: Why is L2 appropriate here.)}, derived from the entire dataset. We do so using the following specifications:
%\begin{enumerate}
%\item[-] $\eta = 1000$ size subsamples of state-action pairs
%\item[-] $N_\eta = 1000$ subsampling episodes
%\item[-] Parameters as in CCP-based Reward Function Estimation \textcolor{blue}{(KK: Not clear what this referring to.)}
%\end{enumerate}
%
%We begin subsampling analysis by first computing the ``true'' estimate for the reward function. We do this by running the algorithm specified in Section \ref{sec:ccp-exp} with all data and all states.
%
%After computing the ``true'' reward function estimate for the full set of states, we randomly subsample $\eta$ state-action pairs from the observed trajectories. It is worth noting that these state-action pairs do not necessarily consist a single trajectory.\textcolor{blue}{(KK: why is this worth noting...twice.)}
%
%Using the $\eta$ state-action pairs, we recompute an estimate for the reward function for present states (by using the algorithm specified in Section \ref{sec:ccp-exp}). This gives us a partial estimate of the ``true'' reward function $\hat R$. We can calculate an L2 norm approximation from $\hat R$ to $R$ by assuming that all non-present states in $\hat R$ have equivalent reward values to those states in $R$.
%
%Doing the above norm calculation over $N_\eta$ episodes gives us an estimate of the variation in the reward function estimator.\textcolor{blue}{(KK: Results are missing and discuss needs to follow.)}

\section{Related Work \& Discussion}

Past work in scaling IRL for large spaces has looked at discrete and continuous spaces separately. This is because applying Max-Ent IRL to continuous spaces is much more challenging and requires its own set of assumptions. Trivially, we could discretize the continuous space and then apply MaxEnt-IRL algorithm directly. However, this scales exponentially with respect to the state space and is thus infeasible in practice. Hence, more sophisticated algorithms are required to extend MaxEnt-IRL to continuous spaces. 

For discrete spaces, past work such as, \cite{huang2015approximate} has mainly focused on approximating the value iteration step in IRL. However, since the MDP problem is still being solved at every step of an iterative procedure this does not scale to very large spaces. For continuous domains, most algorithms use local approximation to estimate the partition function in MaxEnt-IRL. Giving up on global optimality allows them to scale their algorithm to high dimensional domains \cite{levine2012continuous, kalakrishnan2013learning, finn2016guided}.
%Similarly, Kalakrishnan et~al. \yrcite{kalakrishnan2013learning} use local optimality with path integral for policy improvement to scale IRL to manipulation tasks. Recently, Finn et~al. \yrcite{finn2016guided} combined local sample based approximation of MaxEnt-IRL with policy learning under unknown dynamics for high dimensional manipulation tasks.

Alternately, some IRL algorithms have focused on a smaller subset of MDP problems. Todorov \yrcite{todorov2007linearly} introduced a new class of MDPs which are linearly solvable. Dvijotham and Todorov \yrcite{dvijotham2010inverse} present an efficient IRL algorithm for these MDPs. However, this new class of MDP's only solve a restricted form of general MDP formulation and is thus not applicable in all scenarios. Particularly, it is not clear how well these MDPs would generalize to problems where the reward (and as a consequence the value function) is a non-linear function of the states.
%In contrast, our work makes no such approximation of the general MDP framework and is thus applicable everywhere.

% In contrast our work focuses on the simpler discrete setting but with much larger state space.

In contrast to previous work our CCP-IRL algorithm makes no assumption on the MDP framework and is thus generally applicable. 
Also, we focus on discrete problem settings to introduce the key insights from the CCP framework in a well understood paradigm.
Although, the CCP approach is applicable to situations with continuous state spaces, continuous actions or non-stationary settings, given the complexity of applying MaxEnt-IRL to continuous spaces, it is not yet clear how can CCPs be effectively applied to high-dimensional continuous control tasks in robotics. We leave this for future work. 

% CCP framework helps in the unidentification of the reward formulation
% CCP formulation helps in using other empirial models developed by economists e.g. what if we use a different form of human optimality.

\textbf{Discussion:} 
We now discuss some of extensions of the CCP framework which are relevant in robotics and AI research.  
The CCP framework provides methods to investigate the well known under-identification problem in IRL. Only recently, have researchers in AI focused on the unidentifiability problem in IRL. In \cite{amin2017repeated} the authors focus on identification guarantees when the agent can choose the task rewards in a fixed environment setting. In contrast, CCP approach clarifies necessary assumptions to ensure a ``unique" mapping from trajectories to the reward functions \cite{magnac}. CCP approach is also used in multi-agent settings where strategic situations involve multiple equilibria, this often necessitates some equilibrium selection rules. With CCPs and a dataset on a single path of play, it is possible to estimate rewards without having to solve the game and to correctly select the equilibrium in the data \cite{pese}.

Finally, we look at the data requirement for CCP-IRL. A complete theoretical analysis of sample complexity of CCPs is beyond the scope of our initial work. However, we would like to refer the reader to Aguirregabiria and Mira \yrcite{aguirregabiria2002swapping} which discuss some sample properties of CCP estimators.
More importantly, the authors show that one-step CCP estimators (such as CCP-IRL) can have large bias with insufficient data. 
To reduce this bias they introduce a successive iteration procedure which results in a sequence of estimators. They further show that this iteration until convergence will realize the original MaxEnt-IRL estimator.

\section{Conclusion}

% Edit later... place holder

We have described an alternative framework for inverse reinforcement learning (IRL) problems that avoids value function iteration or backward induction. In IRL problems, the aim is to estimate the reward function from observed trajectories of a Markov decision process (MDP). We first analyze the decision problem and introduce an alternative representation of value functions due to \cite{hotz}. These representations allow us to express value functions in terms of empirically estimable objects from action-state data and the unknown parameters of the reward function. We then show that it is possible to estimate reward functions with few parametric restrictions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{references}
\bibliographystyle{icml2018}




% \clearpage
% \section*{Appendix for "Conditional Choice Probability Inverse Optimal Control"}


% \section{Identification Of Reward Functions}
% In econometrics, ``identification" refers to the process of determining whether observed distributions in the data can be mapped into a unique set of parameters of a model. Identification assumes the investigator has access to an infinitely large data set and is not constrained by estimation error. The question in this setting is whether it is mathematically possible to construct a unique mapping from data to the parameters of interest. In the MDP setting, we can determine whether data on states and actions is sufficient to identify the reward function and to what extent parametric assumptions are required. \cite{pese} prove that the solution to a MDP can be represented as an equation system that is linear in the reward function (See Lemma 1 in that paper). Identification then reduces to determining whether a linear equation system has a unique solution. In particular, \cite{pese} provide conditions for identification in multi-agent settings, which are very similar to single-agent decision problems.

% We present one type of identification argument following \cite{bajari} where we assume that one action has a zero payoff in every state. 
% We will now show how the value function is related to the CCPs by introducing the choice-specific value function. Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
% \begin{align}
% V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
% &=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) E_{\vec{\epsilon}_{t+1}}
% \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\}\label{eq:vamax_id}
% \end{align}
% Define the value function difference for action $a$ and $a_0$ as:
% \begin{align}
% \Delta_a(x)\triangleq V_a(x)-V_{a_0}(x).
% \end{align}
% The base action $a_0$ is selected, one action for each state, for which the immediate reward is normalized to zero. Now add and subtract $V_{a_0}(x_{t+1})$ within the max operator in (\ref{eq:vamax}), introduce the short-hand $T^{x_{t+1}}_{x_t,a}$, and expanding the expectation with TIEV yields:
% \begin{eqnarray}\label{eq:cond_choice_v}
% V_a(x_t)=r(a,x_t)+\beta \sum_{x_{t+1}} T^{x_{t+1}}_{x_t,a}  \\ 
% \left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\exp(\Delta_{a'}(x_{t+1}))\right]+\gamma\right]\label{eq:vax}
% \end{eqnarray}
% We now have the choice-specific value function defined in terms of the value function difference $\Delta_{a}$.



% \subsection{Connecting Choice-Specific Value Function to CCPs}

% We will now proceed to expand the residual in terms of a ratio of CCPs. Recalling the definition of CCPs and choice-specific value function from Equations (\ref{eq:ccps}) and (\ref{eq:vax}):
% \begin{eqnarray}
% \sigma(a|x_t)=\frac{\exp\left(V_a(x_t)\right)}{\sum_{a'} \exp\left(V_{a'}(x_t)\right)}.
% \end{eqnarray}


% Multiplying and dividing by $\exp(-V_{a_0}(x_t))$ leads to
% \begin{eqnarray}
% \sigma(a|x_t)=\frac{\exp(V_a(x_t)-V_{a_0}(x_t))}{\sum_{a'} \exp(V_{a'}(x_t)-V_{a_0}(x_t))}=\frac{\exp(\Delta_a(x_t))}{\sum_{a'} \exp(\Delta_{a'}(x_t))}.
% \end{eqnarray}


% It is clear from the above that value function differences between action $a$ and $a'$ are equal to:
% \begin{eqnarray}
% \frac{\sigma(a'|x_t)}{\sigma(a_0|x_t)}=
% \left[
% \frac
%   {\exp(\Delta_{a'}(x_t))}
%   {\sum_{a''} \exp(\Delta_{a''}(x_t))}
% \right]
% \left[\frac{1}{\sum_{a''} \exp(\Delta_{a''}(x_t))}\right]^{-1}=\exp(\Delta_{a'}(x_t))
% \end{eqnarray}
% %
% %
% %
% Using the CCP ratio, the value function for the base action $a_0$ can be expressed as:
% \begin{eqnarray}
% V_{a_0}(x_t) 
% = 
% \beta\sum_{x_{t+1}}
% T(x_{t+1}|x_t,a_0) 
% \left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\frac{\sigma(a'|x_{t+1})}{\sigma(a_0|x_{t+1})}\right]+\gamma\right]
% \end{eqnarray}\label{eq:va0}

% \subsection{Solving for the Optimal Value Function}

% Since we have expressed the choice-specific (a base action) value function in terms of just the transition function and CCPs, we can now estimate $V_{a_0}$ as a fixed point of the above linear equation -- without the knowledge of the full value function or the reward function. In particular, stacking the value function  $V_{a_0}$ for all states yields a $m_x\times 1$ vector:
% \begin{eqnarray}
% \mathbf{V}_0\equiv \left[\begin{array}{c} V_{a_0}(x_1)\\V_{a_0}(x_2)\\\vdots\\ V_{a_0}(x_{m_x})\end{array}\right]
% \end{eqnarray}
% and $\mathbf{T}$ is the transition matrix with entry $T(x_{j}|x_m,a_0)$ for the $m$th row and the $j$th column. 

% Also define the $m_x\times 1$ vector $\mathbf{\Gamma}$ as:
% \begin{align}
% \mathbf{\Gamma} \equiv \left[\begin{array}{c} 
% \ln(1+\sum_{a'\neq a_0}\sigma(a'|x_1)/\sigma(a_0|x_1))\\
% \vdots\\
% \ln(1+\sum_{a'\neq a_0}\sigma(a'|x_{m_x})/\sigma(a_0|x_{m_x}))\\
% \end{array}\right].
% \end{align}

% Then we can write system of equations in matrix form as:
% \begin{align}
% \mathbf{V}_0 = \beta \mathbf{T} [\mathbf{V}_0 + \mathbf{\Gamma}]
% \label{eq:v0_fp}
% \end{align}
% which leads to
% \begin{align}
% \mathbf{V}_0 = [I-\beta \mathbf{T}]^{-1}\beta \mathbf{T} \mathbf{\Gamma}.
% \label{eq:v0_inv}
% \end{align}

% The above is the unique fixed point. The value function for every state can then be estimated as:
% \begin{eqnarray}
% V_a(x_t)=\ln(\sigma(a|x_t))-\ln(\sigma(a_0|x_t))+V_{a_0}(x_t).\label{eq:v_backout}
% \end{eqnarray}



% \subsection{Backing Out the Reward Function}

% We can go further to estimate the reward values $r(a, x_t)$ for $a$ and every state $x_t$ without the need for a costly gradient descent algorithm. We can do this by simply backing out the reward values using equation \ref{eq:va}:
% \begin{align}\label{eq:reward}
% r(a,x_t)
% =V_a(x_t)- \nonumber\\ \beta\sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) 
% \left\{
% V_{a_0}(x_{t+1})
% +\ln
%  \left(
%     1 + \sum_{a'\in\mathcal{A}}\frac{\sigma(a|x_{t+1})}{\sigma(a_0|x_{t+1})}
%   \right)
% \right\}.
% \end{align}


% \newpage
% Return to ex-ante

% \begin{align} \label{eq:exantebellman}
% \begin{split}
% \overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
% & +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]\\
% &=\sum_{a\in\mathcal{A}} \sigma(a|x_t) \left[r(x_t,a)+\tilde{\epsilon}(a|x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}} T(x_{t+1}|x_t,a) \overline{V}(x_{t+1})\right]
% \end{split}
% \end{align}
% where 
% \[
% \tilde{\epsilon}(a|x_t)
% \]
% is the expected value of epsilon conditional on $a$ being optimal at state $x_t$. We know from Hotz Miller that this is equal to $\gamma-\ln\sigma(a|x_t)$ and is therefore only a function of the CCPs. Now stack the ex-ante value function over all states:
% \begin{align}
%     \begin{split}
%     \overline{V}=\sum_{a}S(a) *\left[ R(a)+\tilde{\epsilon}(a)+\beta T(a) \overline{V}\right]
%     \end{split}
% \end{align}
% where:
% \[
% \overline{V}=\left[\begin{array}{c}\overline{V}(x_1)\\\vdots\\\overline{V}(x_{|\mathcal{X}|}\end{array}\right]
% \]
% \[
% R(a)=\left[\begin{array}{c}r(x_1,a)\\\vdots\\ r(x_{|\mathcal{X}|},a)\end{array}\right]
% \]
% \[
% T(a)=\left[\begin{array}{ccc}
% T(x_1|x_1,a),\dots,T(x_{|\mathcal{X}|},x_1,a)\\
% \vdots\\
% T(x_1|x_{|\mathcal{X}|},a),\dots,T(x_{|\mathcal{X}|},x_{|\mathcal{X}|},a)\\
% \end{array}\right]
% \]
% \[
% S(a)=\left[\begin{array}{c}\sigma(a|x_1)\\ \vdots\\ \sigma(a|x_{|\mathcal{X}|})\end{array} \right]
% \]
% \[
% \tilde{\epsilon}(a)=\left[\begin{array}{c}\tilde{\epsilon}(a|x_1)\\ \vdots \\ \tilde{\epsilon}(a|x_{|\mathcal{X}|})\end{array}\right]
% \]
% Re-arranging :
% \begin{align}
%     \begin{split}
%     \overline{V}-\sum_{a}S(a) *\left[ \beta T(a) \overline{V}\right]=\sum_{a}S(a) *\left[ R(a)+\tilde{\epsilon}(a)\right]
%     \end{split}
% \end{align}
% which we can solve for $\overline{V}$
% \begin{align}
%     \begin{split}
%     \overline{V}=\left[I-\sum_{a}(S(a) \lambda) *\left[ \beta T(a)  \right]\right]^{-1}\left[\sum_{a}S(a) *\left[ R(a)+\tilde{\epsilon}(a)\right]\right]
%     \end{split}
% \end{align}
% where $\lambda$ is a $1\times|\mathcal{X}|$ vector of ones.

\clearpage

\subsection{Hotz-Miller's CCP Method}

Our aim in Inverse Reinforcement Learning is to find the parameterized reward function $r(\theta)$ for the given MDP/R. We will now show how we can leverage the non-parameteric estimates of choice conditional probabilities to efficiently estimate the parameters($\theta$) of the reward function for both linear and non-linear parameterization. 
Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
\begin{align}\label{eq:vamax}
\begin{split}
V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
%&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \\
%& \qquad \times E_{\vec{\epsilon}_{t+1}} \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\} \\
&=r(a, x_t) + \sum_{j=1}^{T-t}\beta^{j}E_{x_{t+j}|a_t,x_t} \\
& \qquad \big[r(x_{t+j}, a_{t+j})+ E_{\epsilon_{t+j}|a_t, x_t}\left[\epsilon(x_{t+j}, a_{t+j})\right] \big]
\end{split}
\end{align}

\textbf{Linear Parameters:} Many traditional IRL algorithms such as, \cite{abbeel2004apprenticeship} \cite{ziebart} assume that the reward is a linear combination of known features $z$. Assuming this we will now show how we can efficiently estimate $V_a(x_t)$ for different values of $r(\theta)$.

For linear parameterization we can write $r(a, x_t) = z(a, x_t)^T\theta$, where $z(a, x_t)$ are the set of basis functions (features) known to us.
Notice from \eqref{eq:vamax}, $V_a(x_t)$ is nothing but the expected and discounted sum of future rewards and shock values respectively. Thus we can split \eqref{eq:vamax} into two parts.
Define $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$ as the expected and discounted sum of current and future $z$ values and $\epsilon$ values respectively. Given this we can break \eqref{eq:vamax} into,
\begin{align}\label{eq:choice_specific_linear_param}
\begin{split}
V_a(x_t) & = \tilde{z}(a, x_t)^T \theta + \tilde{e}(a, x_t) \\
\tilde{z}(a, x_{t}, \theta) & = z(a, x_{t}) + \sum_{j=1}^{T-t}\beta^{j} E_{x_{t+j}|a,x_t} \\
& \qquad \times \left[ z\left(\pi^{*}\big( x_{t+j}, \epsilon_{t+j}, \theta \right), x_{t+j} \big) \right] \\
\end{split}
\end{align}
where $\pi^{*}(x_{t}, \epsilon_{t}, \theta)$ is the optimal policy. We will now show that both $\tilde{z}$ and $\tilde{e}$ need to be estimated only once.
To achieve this notice that, we can get consistent estimates of $\pi^{*}(x_t, \epsilon_t, \theta)$ from CCPs, hence we can write \eqref{eq:choice_specific_linear_param} as,
\begin{align}\label{eq:z_tilde_ccp}
\begin{split}
  \tilde{z}(a, x_{t}, \theta) & = z(a, x_t) + \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
  & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}) z(a_{t+j}, x_{t+j}) \right] \\
  \end{split}
\end{align}
We can very similarly write the above expression for $\tilde{e}(a, x_t, \theta)$ replacing $z(a, x_t)$ with $E[\epsilon_t(a)|x_t, a]$. For notational convenience we define $e(a, x_t) \equiv E[\epsilon_t(a)|x_t, a]$,
\begin{align}\label{eq:e_tilde}
\begin{split}
  \tilde{e}(a, x_{t}, \theta) & = \sum_{j=1}^{T-t}\beta^{j} E_{x_{t+j}|a,x_t} \\
  %& \qquad \times \left[ e\left(\pi^{*}\big( x_{t+j}, \epsilon_{t+j}, \theta \right), \epsilon_{t+j} \big) \right] \\
  %& = \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
  & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}) e(a_{t+j}, x_{t+j}) \right] \\
  \end{split}
\end{align}

\cite{hotz} show that $e(a, x_t)$ depends on conditional choice probabilities and distribution of $\epsilon$ only. Further, they prove that the mapping between CCPs and choice specific value function is invertible. Using this inverse mapping and assuming TIEV distribution for $\epsilon$, we get $e(a, x_t) = \gamma - \log P(a|x_t)$.

Thus, both $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$ depend on CCP values and transition probabilities only. As a result we can efficiently estimate $V_a(x_t)$ for different values of $\theta$ from \eqref{eq:choice_specific_linear_param} using simple matrix computations. The only real computation involved is in calculating $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$, both of which need to be done only once. Compare this to \cite{ziebart} where $V_a(x_t)$ needs to be calculated via expensive DP at every $\theta$ value. 
%These can be estimated by solving the DP problem using backwards recursion from \eqref{eq:z_tilde_ccp} and \eqref{eq:e_tilde} respectively, which is similar to Algorithm 9.1 in \cite{ziebart_phd}.

We will now show that the above estimation procedure can be simplified into a fixed point iteration algorithm. Let,
\begin{align}\label{eq:z_tilde_matrix}
Z(x) = \sum_{a}P(a|x)\tilde{z}(a, x)
\end{align}
this allows us to rewrite \eqref{eq:vamax} as,
\begin{align}\label{eq:z_tilde_from_Z}
\tilde{z}(a, x) = z(a, x) + \beta\sum_{x'}T(x'|a, x)Z(x')
\end{align}

Multiplying the above expression with $P(a|x)$ and summing over all actions we get,
\begin{align}\label{eq:z_tilde_recursion}
\begin{split}
\sum_{a}P(a|x)\tilde{z}(a|x) &= \sum_{a}P(a|x) \\
& \times \left\{z(a, x) + \beta\sum_{x'}T(x'|a,x)Z(x') \right\}
\end{split}
\end{align}

The LHS above is the definition of $Z(x)$ \eqref{eq:z_tilde_matrix}. To get a closed form expression we stack up \eqref{eq:z_tilde_recursion} for all different values of $x$. For this we define, 
$\mathbf{P}(a)$ as the vector for CCPs $\{P(a|x), x \in X\}$, similarly $\mathbf{z}(a)$ is $\{z(a, x), x \in X\}$ and $\mathbf{T}_{x'|x}(a)$ is the transition probability matrix for each 
action i.e., $T(x'|a, x) \equiv \mathbf{T}_{x'|x}(a)[x', x]$.

\begin{align}\label{eq:Z_defn}
\mathbf{Z} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{z}(a) + \beta \mathbf{T}_{x'|x}(a)\mathbf{Z}\right\}
\end{align}

We can get a closed form solution for $e(a, x)$ using the same formulation as above, which gives us,
\begin{align}\label{eq:E_defn}
\mathbf{E} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{e}(a) + \beta \mathbf{T}_{x'|x}(a)\mathbf{E}\right\}
\end{align}

For brevity, we combine $\mathbf{Z}$ and $\mathbf{E}$ together into $\mathbf{W} \equiv \{[\mathbf{Z}(x),  \mathbf{E}(x)], x \in X \}$. Thus we get,
\begin{align}\label{eq:w_recursion}
\mathbf{W} = \sum_{a}\mathbf{P}(a) \cdot \left\{ [\mathbf{z}(a), \mathbf{e}(a)] + \beta \mathbf{T}_{x'|x}(a)\mathbf{W}\right\}
\end{align}

In the above equation \eqref{eq:w_recursion} the only unknown is $\mathbf{W}$, which can be solved in closed form as,

\begin{align}\label{eq:w_inversion}
\begin{split}
\mathbf{W} &= \left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1} \\
& \qquad \times \sum_{a}\mathbf{P}(a) \cdot [\mathbf{z}(a), \mathbf{e}(a)]
\end{split}
\end{align}

We can solve for $\mathbf{W}$ either via fixed point iteration \eqref{eq:w_recursion} or via the inversion defined in \eqref{eq:w_inversion}.
Given $\mathbf{W}$ we can trivially estimate $\tilde{z}$ using \eqref{eq:z_tilde_from_Z} (and similarly $\tilde{e}$).  

\textbf{Non-Linear Parameters:} Many recent IRL algorithms have shown that linear parameterization is insufficient to learn complex reward functions \cite{levine2011nonlinear} \cite{wulfmeier2015maximum}. Thus in this section we focus on efficiently estimating $V_a(x_t)$ based on non-linear parameterization of $r$ from known set of features.
Our aim here is to show how to efficiently compute $\mathbf{W}$ \eqref{eq:w_inversion}, which can then be used to efficiently estimate $\tilde{z}$ and $\tilde{e}$ and hence $V_a(x_t)$, as shown before.

In the non-linear formulation, the rewards are defined as $r(a, x_t) = z(a, x_t, \theta)$ instead of $r(a, x_t) = z(a, x_t)^T\theta$. Notice that the only change is in the parameterization of $z$. Thus the choice specific value function \eqref{eq:choice_specific_linear_param} needs to be rewritten as,
\begin{align}\label{eq:choice_specific_non_linear_param}
V_a(x_t, \theta) = \tilde{z}(a, x_t, \theta) + \tilde{e}(a, x_t). 
\end{align}
Also, notice that the above change does not affect $\tilde{e}(a, x_t)$ and consequently the expression for $\mathbf{E}$ \eqref{eq:E_defn} remains unchanged. However, \eqref{eq:z_tilde_matrix} and \eqref{eq:z_tilde_from_Z} need to be rewritten to include the non-linear parameterization as,
\begin{align}
\begin{split}
Z(x, \theta) &= \sum_{a}P(a|x)\tilde{z}(a, x, \theta) \\
\tilde{z}(a, x, \theta) &= z(a, x, \theta) + \beta\sum_{x'}T(x'|a, x)Z(x', \theta)
\end{split}
\end{align}
%Notice, that the only difference between \eqref{eq:choice_specific_linear_param} and \eqref{eq:choice_specific_non_linear_param} is in terms of $\tilde{z}$. In the former expression it is a known set of features ($r(a, x_t) = z(a, x_t)^T\theta$) while in the latter case it is a scalar value($r(a, x_t) = z(a, x_t, \theta)$), estimated as a non-linear function of the features. 
%\begin{align}\label{eq:z_tilde_non_linear}
%\begin{split}
% \tilde{z}(a, x_{t}, \theta) & = r(a, x_t, \theta) + \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
%   & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}, \theta) \tilde{z}(a_{t+j}, x_{t+j}, \theta) \right] \\
%\end{split}
%\end{align}

%Thus $\tilde{z}(a, x)$ is a scalar quantity now since $r(a, x_t, \theta)$ is scalar. Apart from the above changes the other formulations are still valid for the non-linear case as well. Thus we can rewrite \eqref{eq:w_recursion} as
%\begin{align}
%W = \sum_{a}\overbar{P}(a) \cdot \left\{ [\overbar{r}(a, \theta), \overbar{e}(a)] + \beta \overbar{T}_{x'|x}(a)W \right\}
%\end{align}
%
%where we emphasize that $\overbar{r}(a, \theta)$ is the vector of rewards for each state and a function of $\theta$. From this we still get,
Thus following the same process as before for \eqref{eq:Z_defn} we get, 
\begin{align}\label{eq:Z_defn_non_linear}
\mathbf{Z}_{\theta} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{z}(a, \theta) + \beta \mathbf{T}_{x'|x}(a)\mathbf{Z}_{\theta}\right\}
\end{align}
where we have added a subscript $\theta$ to $\mathbf{Z}$ to indicate dependence. Combining $\mathbf{Z}_{\theta}$ and $\mathbf{E}$ together into $\mathbf{W}_{\theta}$ as before, we get \eqref{eq:w_recursion} and thus finally,
\begin{align}\label{eq:w_inversion_non_linear}
\begin{split}
\mathbf{W}_{\theta} &= \left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1} \\
& \qquad \times \sum_{a}\mathbf{P}(a) \cdot [\mathbf{z}(a, \theta), \mathbf{e}(a)]
\end{split}
\end{align}

Notice that to estimate $\mathbf{W}_{\theta}$ using \eqref{eq:w_inversion_non_linear} we need to calculate $\mathbf{z}(a, \theta)$ at every step of the iteration. But the inverse matrix $\left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations. This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}. 
Thus, similar to the linear case we can efficiently estimate $W_{\theta}$ for different values of $\theta$ by simple matrix computations.

\end{document}
