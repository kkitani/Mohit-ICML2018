%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CCP-IRL}

% HACK REMOVE
%\usepackage[showframe]{geometry}% http://ctan.org/pkg/geometry

\usepackage[utf8]{inputenc}
\usepackage{authblk}

\usepackage{icml2018}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath,amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algorithmic}
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx

%PDF Info Is Required:
\icmltitlerunning{CCP-IRL}

% \author[1]{Mohit Sharma}
% \author[1]{Kris M. Kitani}
% \affil[1]{Robotics Institute, Carnegie Mellon University}
% \affil[1]{\texttt{\{mohits1, kkitani\}@cs.cmu.edu}}
% \author[]{Joachim Groeger}
% %\affil[2]{Amazon.com}
% \affil[2]{\texttt{jrg@joachimgroeger.com}}


% \pdfinfo{
% /Title (Inverse Reinforcement Learning with Conditional Choice Probabilities)
% /Author (
%     Mohit Sharma, 
%     %\texttt{mohits1@andrew.cmu.edu}
%     %\and
%     Kris M. Kitani, 
%     %\texttt{kkitani@cs.cmu.edu}
%     %\and
%     Joachim Groeger
%     %\texttt{joachimgroeger@gmail.com})
% }

% === Kris' Macros === %
\usepackage{mathtools}
\usepackage{bm}
\renewcommand{\vec}[1]{\mbox{\bm{$#1$}}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}

% === Mohit's Macros === %
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\def\MSHangBox#1{%^
\begin{minipage}[t]{\textwidth}% Top-hanging minipage, will align on
			       % bottom of first line
\begin{tabbing} % tabbing so that minipage shrinks to fit
~\\[-\baselineskip] % Make first line zero-height
#1 % Include user's text
\end{tabbing}%^
\end{minipage}} % can't allow } onto next line, as {WIDEBOX}~x will not tie.

\usepackage{color}
\definecolor{purple}{rgb}{0.58,0,0.83}

\begin{document}

\twocolumn[
\icmltitle{Inverse Reinforcement Learning with \\ Conditional Choice Probabilities}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{}
\icmlaffiliation{goo}{}
\icmlaffiliation{ed}{}

\icmlcorrespondingauthor{}{}
\icmlcorrespondingauthor{}{}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We make an important connection to existing results in econometrics to describe an alternative formulation of inverse reinforcement learning (IRL). In particular, we describe an algorithm using Conditional Choice Probabilities (CCP), which are maximum likelihood estimates of the policy estimated from expert demonstrations, to solve the IRL problem. Using the language of structural econometrics, we re-frame the optimal decision problem and introduce an alternative representation of value functions due to \cite{hotz}. In addition to presenting the theoretical connections that bridge the IRL literature between Economics and Robotics, the use of CCPs also has the practical benefit of reducing the computational cost of solving the IRL problem. Specifically, under the CCP representation, we show how one can avoid repeated calls to the dynamic programming subroutine typically used in model-based IRL. 
%In the most general case, we show that it is possible to uniquely estimate the optimal reward function, with no parametric restrictions.
%We also derive a version of the CCP-IRL algorithm that can directly estimate a parametric form of the reward function.
We show via extensive experimentation on standard IRL benchmarks that CCP-IRL is able to outperform MaxEnt-IRL, with as much as a 5x speedup and without compromising on the quality of the recovered reward function.

\end{abstract} 

\section{Introduction}

The problem of extracting the reward function of a task given observed optimal behavior has been studied in parallel in both robotics and economics. In robotics this literature is collected under the heading "Inverse Reinforcement Learning" (IRL), \cite{Ng2000, abbeel2004apprenticeship}. The aim here is to learn a reward function that best explains demonstrations of expert behavior so that a robotic system can reproduce expert-like behavior. Alternatively, in economics it is referred to as "structural econometrics" \cite{miller, pakes, rust_gmc} and is used to help economists better understand human decision making. 
Although both fields developed in parallel, they are similar in that both seek to uncover a latent reward function of an underlying Markov Decision Process (MDP).
%One key conceptual departure in the structural econometrics literature is that the modeller allows for random action-dependent reward, which is observed by the demonstrator prior to taking an action but unobserved by us. 
% In particular, a random reward shock is introduced that affects current choices but has no future effect other than through current action.
%The rationale for this is to ensure that the model is not immediately contradicted by the data.

% In IRL to learn a task given expert demonstrations we solve for the reward function of a MDP, under which the demonstrated trajectories are optimal. However, unfortunately most of the IRL algorithms are very expensive to run since they require solving the above MDP problem in every loop of their iteration. Thus recent works have looked at scaling IRL algorithms to large environment spaces \cite{finn2016guided} \cite{levine2012continuous}. 

One of the main challenges in IRL applied to robotics is the large computational complexity of modern algorithms \cite{ziebart, Ratliff2006}. 
To infer the reward function of the underlying MDP, one must \textit{repeatedly} solve this MDP at every step of a reward parameter optimization scheme.
% This is because in IRL we infer the reward function of the MDP by \textit{repeated} computation of the MDP solution for every loop of a reward parameter optimization scheme. 
The MDP solution, which is typically characterized by a value function, requires a computationally expensive Dynamic Programming (DP) procedure. The requirement of solving this DP repeatedly makes IRL algorithms computationally prohibitive.
%This MDP solution (\emph{i.e.}, the value function), is typically obtained by solving Bellman equations using dynamic programming (DP).
% Unfortunately, solving this DP problem is computationally expensive and thus solving it \textit{repeatedly} makes IRL algorithms computationally prohibitive.
% Given the large cost of solving the DP problem at each step of optimization vastly limits the range of the above methods.  Unfortunately, this makes most of the IRL algorithms computationally prohibitive for large environments. 
In response, recent works have proposed  to find approximate IRL solution to deal with large environment spaces \cite{finn2016guided, levine2012continuous, huang2015approximate}.

% This problem of large computational complexity has also been studied in economics, with the ensuing literature collected under the name "Conditional Choice Probabilities".
The problem of computational complexity has also been studied in economics \cite{hotz, su2012constrained, aguirregabiria2002swapping}. Among the many works, Conditional Choice Probability (CCP) estimators \cite{hotz} are particularly interesting because of their computational efficiency.
CCP estimators use CCP values to estimate the reward function of the MDP.
The CCP values specify the optimal action for a state and are estimated from expert demonstrations.
These estimators are computationally efficient since they avoid the repeated computation of the DP step by using an alternative representation of the MDP's value function. Specifically, it is shown that values functions can be decomposed into ratios of CCPs.
% This representation allows the MDP to be solved in closed form using CCPs.

%To avoid the repeated computation of the MDP, \cite{hotz} introduced an alternative representation of the value function such that it can be solved in closed form through the use of conditional choice probabilities (CCP).
In this paper we leverage results from econometrics \cite{rust_gmc, hotz, magnac} to formulate an estimation routine for the reward function with CCPs, that avoids repeated calls to the solver of the full dynamic decision problem.
% and (2) ensures a unique mapping from trajectories to rewards.
The key insight from \cite{hotz} is that differences in current reward and future values between actions can be calculated from CCPs. This allows us to express future value functions in terms of difference value functions and therefore CCPs. Since CCPs are directly observed in the data, we can use this function representation to estimate the value function of the MDP at each step of the optimization process without solving the expensive dynamic programming (DP) formulation. This results in an algorithm whose overall computational time is comparable to a single MDP computation of a traditional gradient-based IRL method. 

In this work we introduce Conditional Choice Probability Inverse Reinforcement Learning (CCP-IRL) by incorporating CCPs into IRL. We test the CCP-IRL algorithm on multiple different IRL benchmarks and compare the results to the state-of-the-art IRL algorithm, MaxEnt-IRL \cite{ziebart}. In our experiements, we show that with CCP-IRL we can achieve up to 5$\times$ speedup without affecting the quality of the inferred reward function. We also show that this speedup holds across large state spaces and increases for complex problems, such as, problems where value iteration takes much longer to converge. 


% ==== OLD INTRO ====


% The field of Inverse Reinforcement Learning (IRL) \cite{Ng2000} \cite{abbeel2004apprenticeship} in robotics or computer vision is concerned with the the problem of learning a task given expert demonstrations for it. IRL solves this problem by finding a reward function under which the demonstrated trajectories are optimal. However, unfortunately most of the IRL algorithms are very expensive to run since they require solving a MDP problem in every loop of their iteration.

% IRL has also been a topic of study in economics for a number of decades under the heading of "structural econometrics" \cite{miller}, \cite{pakes} and \cite{rust_gmc}. Structural econometrics differs little from IRL in robotics or computer vision, in that both seek to uncover a latent reward function to either understand, forecast, anticipate or mimic behavior. The key conceptual departure in the structural econometrics literature is that the modeller allows for random action-dependent reward, which is observed by the demonstrator prior to taking an action but unobserved by us. 
% In particular, a random reward shock is introduced that affects current choices but has no future effect other than through current action. The rationale for this is to ensure that the model is not immediately contradicted by the data.
%In a standard Dynamic Discrete Choice model (a MDP with action shocks) the policy is a usually a deterministic function of the state.
%This implies that if we observed two experts' behaviors, if both were characterized by identical state variables, they should take the same action. This rarely happens in real behavioral data and the additive shock allows for the possibility of an unobserved factor to determine choice. 
% The introduction of shocks means that policies can now not be directly observed in the data. However, it is still possible to measure integrated policy functions, \emph{i.e.}, policy functions that are conditional on the observable state averaged across the unobserved shock. These integrated policy functions yield a probability of selecting an action conditional on a state.
% We refer to these probabilities as conditional choice probabilities or CCPs.
%From the perspective of the expert, policies are deterministic, however from the outside observer they are random.

% ==== Related Work ====
% Attempt to connect the different strategies for IOC to put CCP in a new box
%The task of IOC or IRL is to infer the \emph{reward function}, usually of an underlying MDP, from a set of demonstrated behavior. The key differences across methods are in their choice of objective function for the MDP/R model and the behaviors observed in the data. Most techniques work with conditional feature expectation functions \emph{e.g.} \cite{abbeel} use the feature matching technique. However, a major drawback of direct feature matching techniques is the non-uniqueness of the inversion from trajectories to the reward function. Alternatively, \cite{ziebart} consider a metric based on an empirical analogue of the Kullback-Leibler deviation, whose formulation leads to a unique solution under the maximum entropy criteria. The method performs gradient descent on the parameters of a linearized reward function and requires the complete MDP solution for each descent step. Instead of matching feature counts, \cite{Ratliff2006} attempt to maximize the difference between the reward (a function of feature counts) of an expert trajectory versus that of all other trajectories using a max-margin framework. However, the number of constraints (potentially infinite) that must be evaluated is large.
% and requires the use of cutting plane or (sub)gradient methods to solve. One can also attempt to directly match \emph{trajectories} \cite{mombaur2009identifying} instead of features. Such an approach results in a bi-level problem; one loop to improve reward parameters and another loop to solve the forward optimal control problem. 
%One can also attempt to match policies \cite{neu}. This approach also requires computing the complete solution MDP for every (natural) gradient step to improve the policy. 

% As we have noted, the core of most value-based IRL approaches is the \textit{repeated} computation of the MDP solution for every loop of a reward parameter optimization scheme. The solution of the MDP (\emph{i.e.}, the value function), is typically solved using the Bellman equations with dynamic programming (DP), known as the Value Iteration algorithm. Given the large cost of solving the DP problem at each step of optimization vastly limits the range of the above methods.

% In econometrics, to avoid the repeated computation of the value function, \cite{hotz} introduced an alternative representation of the value function such that it can be solved in closed form through the use of conditional choice probabilities (CCP). In this paper we leverage results from \cite{rust_gmc}, \cite{hotz} and \cite{magnac} to formulate an estimation routine with CCPs that (1) avoids repeated calls to the solver of the full dynamic decision problem and (2) ensures a unique mapping from trajectories to rewards. The key insight from \citeauthor{hotz} is that differences in current reward and future values between actions can be inverted from CCPs. This allows us to express future value functions in terms of difference value functions and therefore CCPs. Since CCPs are directly observed in the data we can use this function representation to estimate the value function at each step of the optimization process without needing to solve the expensive DP problem.

% This results in an IRL algorithm whose overall computational time is comparable to a single MDP computation of a traditional gradient-based IRL method. We test the above CCP-IRL algorithm on multiple different RL benchmarks and compare the results to state of the art IRL algorithm such as MaxEnt-IRL \cite{ziebart}. We show that with CCP-IRL we can get upto 5x speedup without affecting the quality of the inferred reward function. We also show that this speedup holds across large state spaces and increases for complex problems, such as, problems where the value iteration takes much longer to converge. 

% The econometric approach also provides a systematic method of determining under what conditions a unique mapping can be guaranteed from the observed distribution in the data to the reward function. For example, it can be shown that the reward function can only be estimated when fixing the discount factor and the distribution of the random reward shock.

% Another benefit of the CCP approach is when considering multi-agent settings where agents are strategic. In these settings there are generally multiple equilibria. As a result, it is not possible to guarantee that the numerical solution to the game will match the data. The CCP approach side-steps this issue since strategies are estimated directly from a single-path of play.

% Not sure how to weave this in... \cite{Levine2013} use second order methods to initialize many optimal policies (each conditioned on the initial state) and combine them to achieve a good global policy.

%We can construct different metrics in order to match the data. For example, we could directly look for parameters of the reward function that minimize the distance between the non-parametrically estimated CCPs and the CCPs implied by our model. Stack the CCPs for all actions and all states into vector $\mathbf{P}$ and stack the model implied CCPs in function $\Lambda(\mathbf{P};\theta)$ where $\theta$ are the reward parameters. Then we could find estimates $\hat{\theta}$ by
%\[
%\hat{\theta}=\arg\min_{\theta} [\mathbf{P}-\Lambda(\mathbf{P};\theta)]'[\mathbf{P}-\Lambda(\mathbf{P};\theta)]
%\]
%The solution to the above minimizes the Euclidean distance between the estimated and model-implied policies. This approach is similar in spirit to \cite{neu2007} who consider directly matching policy functions of experts. 
%One drawback however of their approach, is that it requires the complete solution MDP. \cite{neu2007} regress value functions onto actions and they use a Boltzman probability distribution, which leads to expressions for the probability distribution of actions similar to the CCPs derived under TIEV payoff shocks.

% Contribution


% Points to make: 
% (1) Structural Econometrics pre-dates IRL work, with the exception of Kalman 1964.
% (2) Main paragdigm in robotic learning is feature matching
% (3) Main paradigm in econ is ...


% Problem of non-uniqueness...
%The task of inverse optimal control (IOC) or inverse reinforcement learning (IRL) problem is to infer the reward function, usually of an underlying MDP, from a set of demonstrated behavior. The key differences across IRL or IOC methods are in their choice of distance metric between the MDP/R model and the behaviors observed in the data. Most techniques work with conditional feature expectation functions. \cite{abbeel2004apprenticeship} use the feature matching technique. However, a major drawback of existing IRL methods is the non-uniqueness of the inversion from trajectories to the reward function.

% The MaxEnt solution to IOC
%\cite*{ziebart} considers a metric based on an empirical analogue of the Kullback-Leibler deviation, whose formulation leads to a unique policy under the maximum entropy criteria. \cite{ziebart_phd} derives a mapping of Maximum Entropy (MaxEnt) methods into the space of Bellman-like operators. The method performs gradient descent on the parameters of a linearized reward function and requires that complete solution of the MDP for each descent iteration.



%\subsection{CCP IOC} % AKA our solution to everything!

% Our proposed method (need to be improved)
%In this paper, we provide specific conditions that ensure a unique reward function can be recovered. Interestingly, the MaxEnt Bellman updates are identical to the setup we describe with Type 1 Extreme Value (TIEV) payoff shocks. Our method is more general however, as any other distributional assumption on shocks leads to different Bellman updates.


 

%In this paper we leverage results from \cite{rust_gmc}, \cite{hotz} and \cite{magnac} to formulate an estimation routine with CCPs that resembles policy matching and that ensures a unique mapping from trajectories to rewards. Moreover, this mapping avoids the solution to the full dynamic decision problem. In particular, we can construct different metrics in order to match the data. For example, we could directly look for parameters of the reward function that minimize the distance between the non-parametrically estimated CCPs and the CCPs implied by our model. Stack the CCPs for all actions and all states into vector $\mathbf{P}$ and stack the model implied CCPs in function $\Lambda(\mathbf{P};\theta)$ where $\theta$ are the reward parameters. Then we could find estimates $\hat{\theta}$ by
%\[
%\hat{\theta}=\arg\min_{\theta} %[\mathbf{P}-\Lambda(\mathbf{P};\theta)]'[\mathbf{P}-\Lambda(\mathbf{P};\theta)]
%\]
%The solution to the above minimizes the Euclidean distance between the estimated and model-implied policies. This approach is similar in spirit to \cite{neu} who consider directly matching policy functions of experts. One drawback however of their approach, is that it requires the complete solution MDP. \cite{neu} regress value functions onto actions and they use a Boltzman probability distribution, which leads to expressions for the probability distribution of actions similar to the CCPs derived under TIEV payoff shocks. 


%\subsection{Related Work}

%General overview of IOC, stress the importance of R, cite computational cost when we want to know R. Behavior Cloning versus IOC.


%\subsection{Inverse Optimal Control}

%The classical inverse optimal control (IOC) or inverse reinforcement learning (IRL) problem assumes a known MDP excluding the reward function $r$ and a set of trajectories $\mathcal{D}$ from an expert who behaves optimally according to $r$. An IOC algorithm then searches for an $r$ which best describes the expert's observed behavior, according to some criterion function. 

%The major challenge facing IOC problems is there exists multiple reward functions which can induce the observed optimal trajectories (\cite{Ng2000}). For example, any set of trajectories is optimal for the all-zero rewards. To achieve a unique solution, \cite{Ng2000} proposed also maximizing the difference between the optimal and second-best action for every state. Likewise, \cite{ziebart} placed an entropy based prior over trajectories. Most value-based IOC approaches require that a recursive optimization is solved as a sub-routine, which can be intractable to solve for large or continuous state spaces.

%\textcolor{blue}{(KK: Don't forget to mention Max-Margin Planning...)}

%Alternatively, one may choose to forego searching for $r$ and instead directly look for a policy which mimics the observed behavior $\mathcal{D}$. \cite{abbeel} finds a policy which is guaranteed to perform well with the unobserved $r$.

%The line of work from \cite*{Ross2011}, \cite*{Ross2014} and \cite*{Sun2017} address the issue of different train and test trajectory distributions, by interleaving an action or cost-to-go oracle during training. \cite{Levine2013} use second order methods to initialize many optimal policies (each conditioned on the initial state) and combine them to achieve a good global policy.

%\subsection{Structural Analysis} % unsure of the exact econ term here for IOC...

%In the economics literature, \cite{rust_non_id} provided the same non-uniqueness result as \cite{Ng2000}. \cite{magnac} provide conditions that ensure a unique mapping, which we describe in detail in the main text.  \textcolor{red}{I suspect earlier economics papers also noticed this, does someone know the first?\textcolor{green}{added the firs non uniqueness result and references to magnac}}. They propose assuming a known reward for a single action in every state, a reasonable assumptions for problems where the ``non-action'' has zero or constant reward. They also address the game-theoretic problem of multiple agents acting optimally in a single MDP -- which describes many businesses and trading markets.
%\textcolor{green}{I am alright dropping references to games here.}





%==========================================================%
\section{Preliminaries}

In this section, we first introduce the MDP formulation as used in the econometrics literature under the name "Dynamic Discrete Choice Model". Following this, we show how the optimality equation is formulated under these assumptions, and how the resulting optimization problems can be related to well-known IRL algorithms in robotics.


\subsection{Dynamic Discrete Choice Model}

A dynamic discrete choice (DDC) model (\emph{i.e.}, a discrete Markov decision process with action shocks) is defined as a tuple $(\mathcal{X,A}, T,r,\mathcal{E},F)$. $\mathcal{X}$ is a countable set of states with a cardinality of $|\mathcal{X}|$. We assume a discrete state space, although this is not strictly necessary. $\mathcal{A}$ is a finite set of actions with cardinality $|\mathcal{A}|$. $T$ is the transition function where $T(x'|x,a)$ is the probability of reaching state $x'$ given current state $x$ and action $a$. The reward function $r$ is a mapping $r:\mathcal{A}\times\mathcal{X}\rightarrow \mathbb{R}$.

Different from MDPs typically used in RL, each action also has a "payoff-shock" associated with it, that enters payoffs additively.
Intuitively, the shock variable accounts for the possibility that an agent takes a non-optimal behavior due to some unobserved factor of the environment or agent. In other words, the shock term allows the MDP to account for demonstrations of non-optimal behavior.
The vector of shocks is denoted $\vec{\epsilon} = [ \epsilon_{1} \cdots \epsilon_{|\mathcal{A}|} ]$ and $\vec{\epsilon}\in \mathbb{R}^{|\mathcal{A}|}$. The instantaneous reward for action $a \in \mathcal{A}$ in state $x \in \mathcal{X}$ is given by:
\begin{eqnarray}
r(a,x)+\epsilon_a.
\end{eqnarray}
The shock value $\epsilon_a\in\mathbb{R}$ is often assumed to be drawn or sampled according to a Gumbel or Type 1 Extreme Value (TIEV) distribution,
\begin{align}
F(\epsilon_a)=e^{-e^{-\epsilon_a}}.
\end{align}
We will see that the use of a TIEV distribution is numerically convenient for the following derivations. However, alternative algorithms can be derived for other functional forms. Each shock $\epsilon_a$ is independently and identically drawn from $F(\epsilon_a)$. This ensures that state transitions are conditionally independent. All serial dependence between $\epsilon_{t}$ and $\epsilon_{t+1}$ is transmitted through $x_{t+1}$. \cite{rust_theory} proves the existence of optimal stationary policies in this setting.


\subsection{Bellman Optimality Equation Derivation}

Consider a system currently in state $(x_t,\vec{\epsilon}_t)$, where $\vec{\epsilon}_t$ is a vector of shock values. The decision problem is to select the action that maximizes the payoff:
\begin{align}
\begin{split}
V(x_t,\vec{\epsilon}_t) & = \max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& \qquad + \beta \cdot E_{x_{t+1},\vec{\epsilon}_{t+1}|x_t,a} \left[V(x_{t+1},\vec{\epsilon}_{t+1})\right] \big\} \\
\end{split}
\end{align} 
where $V$ is the value function, $\beta$ is the discount factor and $\epsilon_{at} \in \vec{\epsilon}_t$ is the shock value when selecting action $a$ at time $t$.


Given the conditional independence assumption of the shock variable described previously, we can separate the integration of $x_{t+1}$ and $\vec{\epsilon}$. Define the \emph{ex-ante} value function (\emph{i.e.}, $V$ prior to the revelation of the values of $\epsilon$) as:
\begin{eqnarray}\label{eq:def_exante}
\overline{V}(x_t)
\triangleq 
E_{\vec{\epsilon}_t} \left[ V(x_t, \vec{\epsilon}_t) \right],
\end{eqnarray}
that is, the expectation of the value function with respect to the shock distribution. Using this notation and conditional independence, we can write the original decision problem as:
% NOTE: 
% Cannot use \left \right to dynamically scale {} 
% This trick doesn't seem to work (https://tex.stackexchange.com/questions/49890/linebreak-between-left-and-right)
%
\begin{align}
\begin{split}
V(x_t,\vec{\epsilon}_t) & =\max_{a\in\mathcal{A}} \big\{ \vphantom{V} r(x_t,a)+\epsilon_{at}  \\
&  \, +\beta \cdot E_{x_{t+1}|x_t,a} \left[ \overline{V}(x_{t+1}) \right] \vphantom{r(x_t, a)} \big\}.
\nonumber
\end{split}
\end{align}


The \emph{ex-ante} value function also follows a Bellman-like equation:
\begin{align} \label{eq:exantebellman}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
& +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big]
\end{split}
\end{align}

Assuming TIEV distribution for the shock values, one obtains the following expression for the \emph{ex-ante} value functions as shown by \cite{rust_gmc}:
\begin{align} \label{eq:exanterust}
\begin{split}
\overline{V}(x_t) &=\ln\left[\sum_{a\in\mathcal{A}} \exp\left(r(x_t,a)+\beta \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \right)\right] \\
& \qquad +\gamma,
\end{split}
\end{align}
where $\gamma$ is Euler's constant. The expectation of the maximum is equal to the average of expected value functions, conditioned on choosing action $a$ with $\vec{\epsilon}$ integrated using the TIEV density. Weights in the average are given by the CCPs of choosing action $a$.

% E(max(a,b)) = Pr(a>b)E(a|a>b)+Pr(b>a) E(b|b>a)

Notice that the above is exactly the recursive representation of the Maximum Causal Entropy IOC algorithm as derived in Theorem 6.8 in \cite{ziebart_phd}. In our setting, the soft-max recursion is a consequence of Bellman's optimality principle in a setting with a separable stochastic payoff shock with a TIEV distribution, while in \cite{ziebart_phd} the authors derive the recursion from an information-theoretic perspective that enforces a maximum causal entropy distribution over trajectories.


%===============================================================%
\section{Conditional Choice Probability Inverse Reinforcement Learning}

We will now show how it is possible to efficiently recover the optimal value function, and consequently the underlying reward function, using the DDC model. The key insight is that the optimal value function can be directly estimated from observed state-actions pairs (Conditional Choice probabilities), observed over a \textit{large} set of expert demonstrations. When this assumption holds, the optimal value function can be represented as a linear function of the CCPs and efficiently computed for different parameter values without repeatedly solving the DP.

\subsection{Conditional Choice Probabilities}

Since an outside observer does not have access to the shock ($\vec{\epsilon}$), the underlying deterministic policy of the expert $\sigma(a | x,\epsilon)$ is not directly measurable. However, if we average decisions across trajectories conditioned on the same state variables, we are able to identify the integrated policy. We denote this integrated policy by $\sigma(a|x)\in[0,1]$. This quantity describes the \emph{conditional choice probability} (CCP) of an action being chosen conditioned on state $x$: 
\begin{eqnarray}
\sigma(a|x_t)\triangleq E_{\vec{\epsilon}}\left[ \mathbf{1}\{a\ \textrm{is optimal in state }x_t\}\right],
\end{eqnarray}
where $\mathbf{1}\{\}$ is the indicator function. The event in the indicator function is equivalent to the event:
\begin{align}
\begin{split}
& \left\{r(x_t,a)+\epsilon_{at}+\beta E_{x_{t+1}|a,x_t} \overline{V}(x_{t+1})\geq \right. \\
& \quad \left. r(x_t,a')+\epsilon_{a't}+\beta E_{x_{t+1}|a',x_t} \overline{V}(x_{t+1}),\ \forall a'\neq a \right\}.
\end{split}
\end{align}

Expanding the expectation under the TIEV assumption on the shock variable allows CCPs to be solved in closed-form:
\begin{align} \label{eq:ccps}
\begin{split}
\sigma(a|x_t)=\frac{\exp\left(r(x_t,a)+\beta E_{x_{t+1}|x_t,a} \overline{V}(x_{t+1})\right)}{\sum_{a'\in\mathcal{A}} \exp\left(r(x_t,a')+\beta E_{x_{t+1}|x_t,a'} \overline{V}(x_{t+1})\right)}
\end{split}
\end{align}
Notice that \eqref{eq:ccps} is identical to the definition of the policy of the MaxEnt formulation in \cite{ziebart_phd}, which is derived from an entropic prior on trajectories. The CCP is derived by integrating out the TIEV shock variable. 
% In its simplest form, the CCPs can be computed directly from $N$ expert trajectories each with $T_i$ time periods:  $\mathcal{D} = \{(a_{it},x_{it})_{t=0}^{T_i}:i=1,\dots,N\}$ in tabular form. An initial maximum likelihood estimate can be computed by maintaining a table over state-action pair occurrences. 


\begin{figure*}[ht]
\begin{minipage}[t]{0.45\textwidth}
  %\vspace{0pt}  
  \begin{algorithm}[H]
    \caption{CCP-IRL algorithm} \label{algo:ccp_irl_algorithm}
    \begin{algorithmic}[1]
        %\Procedure{CCP-IRL}{$\mu_D,f, S, A, T, \gamma$}
        \STATE {\bfseries Input:} $\mu_D,f, S, A, T, \gamma$
        \STATE $\theta^{(0)} \gets$ init\_weights
        \STATE $M \gets \left[I-\sum_{a}(S(a) \lambda) *\left[ \beta T(a)  \right]\right]^{-1}$ 
        \STATE $\tilde{\epsilon} \gets \gamma - \log S(a)$
        \FOR{$i\gets 1, n$}
            \STATE $R^{(i)} \gets \theta^T f$
            \STATE $V^{(i)} \gets M \times \sum_{a}{S(a) \times \left[ R^{(i)} +\tilde{\epsilon}\right]}$
            \STATE $\pi_{\theta}^{(i)}(a|x) \gets e^{V^{(i)}(x_a) - V^{(i)}(x)}$
            \STATE $E[\mu^{(i)}] \gets $ FORWARD PASS()
            \STATE $\theta^{(i)} \gets \theta^{(i-1)} - \alpha \times (\mu_D - E[\mu^{(i)}])$
        \ENDFOR
    \end{algorithmic}
  \end{algorithm}
\end{minipage}%
\qquad
\begin{minipage}[t]{0.45\textwidth}
  %\vspace{0pt}
  \begin{algorithm}[H]
    \caption{Forward Pass} \label{algo:forward_pass_algorithm}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} policy $\pi_{\theta(a|x)}$
        \STATE $D^{(0)}(x) \gets P(x_i = x_{initial})$
        \FOR{$i\gets 1, n$}
            \STATE $D^{(i-1)}(x_{goal}) \gets 0$
            \STATE $D^{(i)}(x) \gets D^{(i)}(x) + \pi_{\theta}(a|x') D^{(i-1)}(x')$
        \ENDFOR
        \STATE $D(x) \gets \sum_{i}D^{(i)}(x)$ 
        \STATE \textbf{return} $D(x)$
    \end{algorithmic}
  \end{algorithm}
\end{minipage}
\end{figure*}


\section{Hotz-Miller's CCP Method}

Our aim in Inverse Reinforcement Learning is to find the parameterized reward function $r(\theta)$ for the given MDP/R. We now show how we can leverage \textit{non-parameteric} estimates of choice conditional probabilities to efficiently estimate the parameters $\theta$ of the reward function. 

First, we look at the alternative representation of the \emph{ex-ante} value function which can be derived from the CCP representation. Using this alternative representation, we will see how we can avoid solving the original MDP using the expensive dynamic programming formulation for every update of $\theta$.
%
%Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
%\begin{align}\label{eq:vamax}
%\begin{split}
%V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
%&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \\
%& \qquad \times E_{\vec{\epsilon}_{t+1}} \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\} \\
%&=r(a, x_t) + \sum_{j=1}^{T-t}\beta^{j}E_{x_{t+j}|a_t,x_t} \\
%& \qquad \big[r(x_{t+j}, a_{t+j})+ E_{\epsilon_{t+j}|a_t, x_t}\left[\epsilon(x_{t+j}, a_{t+j})\right] \big]
%\end{split}
%\end{align}
%
% Previously, we have seen that given the ex-ante value function we can derive the optimal policy estimates using \eqref{eq:ccps}.
% We now derive an alternate representation for the ex-ante value function.
Returning to the definition of ex-ante value function in Equation \eqref{eq:def_exante} and \eqref{eq:exantebellman},
%we know that,
%
%\begin{align} \label{eq:exantebellman_1}
%\begin{split}
%\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\max_{a\in\mathcal{A}} \big\{r(x_t,a)+\epsilon_{at} \\
%& \qquad +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big].\\
%\end{split}
%\end{align}
\cite{hotz} show that if one can obtain \textit{consistent} CCP estimates from the data, the Equation \eqref{eq:exantebellman} can be estimated by,
\begin{align} \label{eq:exantebellman_2}
\begin{split}
\overline{V}(x_t) & = E_{\vec{\epsilon}_t}\Big[\sum_{a\in\mathcal{A}} \sigma(a|x_t) \big\{r(x_t,a)+\epsilon_{at} \\
& \qquad +\beta  \cdot E_{x_{t+1}|a,x_t} \left[ \overline{V}(x_{t+1}) \right] \big\}\Big].\\
\end{split}
\end{align}
Now, defining the expected shock given optimal action $a$ as $\tilde{\epsilon}(a|x_t) = E_{\vec{\epsilon}}\left( \epsilon_{at} |a\ \textrm{is optimal in state } x_t\right)$, we can rewrite \eqref{eq:exantebellman_2} as,
\begin{align} \label{eq:exantebellman_3}
\begin{split}
\overline{V}(x_t) & = \sum_{a\in\mathcal{A}} \sigma(a|x_t) \Big[r(x_t,a)+\tilde{\epsilon}(a|x_t) \\
& \qquad +\beta \sum_{x_{t+1}\in\mathcal{X}} T(x_{t+1}|x_t,a) \overline{V}(x_{t+1})\Big]
\end{split}
\end{align}

It was shown further that $\tilde{\epsilon}(a|x_t)$ depends on CCPs and distribution of $\epsilon$ only. They prove that the mapping between CCPs and choice specific value function is invertible. Using this inverse mapping and assuming TIEV distribution for $\epsilon$, we get $\tilde{\epsilon}(a|x_t) = \gamma - \log \sigma(a|x_t)$. Now that the relationship between the value function and the CCPs has been established, we describe an algorithm for solving for the value function,

\subsection{Reinforcement Learning with CCPs}

\textcolor{blue}{(Very rough transition here with no motivation. What is the goal of the inference, what is the strategy and how will we get their?)}
We can now use Monte Carlo approximations of the expected value functions. We use the CCPs, and transition probabilities to simulate $S$ trajectories each with $T$ decision nodes. The steps are:
\begin{enumerate}
\setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
    \item Draw an initial state $x_0$
    \item Draw an action using CCPs associated with realized state $x_0$, call this $a_0$
    \item Given action $a_0$ and state $x_0$ draw future state $x_1$. 
    \item Continue this process until we have $T$ decision nodes.
    \item Repeat this $S$ times
    \item Store all trajectories.
\end{enumerate}
We can then approximate the expected value functions, conditional on a parameter guess on the reward function, as:
\begin{eqnarray}
\frac{1}{S}\sum_{s=1}^S\sum_{t=0}^T\beta^t [r(a_t,x_t)+\gamma-\ln \sigma(a_t|x_t)]
\end{eqnarray}


From \eqref{eq:exantebellman_3} we can see that, excluding the unknown reward function, all other terms can be estimated from CCPs. We can now stack the ex-ante value function over all states,
\begin{align} \label{eq:exantebellman_4}
    \begin{split}
    \overline{\mathbf{V}}=\sum_{a}\mathbf{S}(a) \times \left[\mathbf{R}(a)+\tilde{\bm{\epsilon}}(a)+\beta \mathbf{T}(a) \overline{\mathbf{V}}\right]
    \end{split}
\end{align}
where:
\[
\overline{\mathbf{V}}=\left[\begin{array}{c}\overline{V}(x_1)\\\vdots\\\overline{V}(x_{|\mathcal{X}|}\end{array}\right]
\]
\[
\mathbf{R}(a)=\left[\begin{array}{c}r(x_1,a)\\\vdots\\ r(x_{|\mathcal{X}|},a)\end{array}\right]
\]

\[
\mathbf{T}(a)=\left[\begin{array}{ccc}
T(x_1|x_1,a),\dots,T(x_{|\mathcal{X}|},x_1,a)\\
\vdots\\
T(x_1|x_{|\mathcal{X}|},a),\dots,T(x_{|\mathcal{X}|},x_{|\mathcal{X}|},a)\\
\end{array}\right]
\]

\[
\mathbf{S}(a)=\left[\begin{array}{c}\sigma(a|x_1)\\ \vdots\\ \sigma(a|x_{|\mathcal{X}|})\end{array} \right]
\]

\[
\tilde{\bm{\epsilon}}(a)=\left[\begin{array}{c}\tilde{\epsilon}(a|x_1)\\ \vdots \\ \tilde{\epsilon}(a|x_{|\mathcal{X}|})\end{array}\right]
\]



Notice that \eqref{eq:exantebellman_4} is linear in ex-ante value function ($\overline{\mathbf{V}}$). Thus we can write a closed form solution for it.
First, rearranging the terms we get,

\begin{align}
    \begin{split}
    \overline{\mathbf{V}}-\sum_{a}\mathbf{S}(a) *\left[ \beta \mathbf{T}(a) \overline{\mathbf{V}}\right]=\sum_{a}\mathbf{S}(a) *\left[ \mathbf{R}(a)+\tilde{\bm{\epsilon}}(a)\right]
    \end{split}
\end{align}

Defining $\lambda$ as a $1\times|\mathcal{X}|$ vector of ones.
We can now write the closed form solution for $\overline{\mathbf{V}}$ as,

\begin{align} \label{eq:exante_inversion}
\begin{split}
\overline{\mathbf{V}} &=\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1} \\
& \qquad \times \left[\sum_{a}\mathbf{S}(a) *\left[ \mathbf{R}(a)+\tilde{\bm{\epsilon}}(a)\right]\right]
\end{split}
\end{align}

The above is the value function representation used by \cite{pese} and discussed in \cite{arcidiacono}.

% Note that if the reward function is assumed to be parameterized by $\theta$ we can make this parameterization explicit by replacing, $\mathbf{R}(a)$ in \eqref{eq:exante_inversion} with $\mathbf{R}(a, \theta)$.

We now discuss the CCP-IRL algorithm and how it avoids repeatedly solving the original MDP problem.
The pseudo-code for CCP-IRL is given in Algorithm~\ref{algo:ccp_irl_algorithm}, where $\mu_D$ is expert's feature expectations and $f$ are features at every state.
Notice that the only quantity dependent on $\theta$ in \eqref{eq:exante_inversion} is $\mathbf{R}(a, \theta)$.
Thus, to estimate $\mathbf{\overline{V}}$ using \eqref{eq:exante_inversion} we calculate $\mathbf{R}(a, \theta)$ for every $\theta$ value \emph{i.e.}, at every step of the iteration (Line 6).
But the inverse matrix $\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations (Line 3).
This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}.
Given this inverse matrix computing $\mathbf{\overline{V}}$ at any $\theta$ requires simple matrix operations (Line 7), which allows us to avoid solving the MDP using dynamic programming at every step of the iteration.
Lines 8-10 calculate the gradient for the reward parameters and are explained in \cite{kitani2012activity}.

We also note how to calculate the initial CCP estimates ($\mathbf{S}$). In their simplest form, the initial CCP estimates can be computed directly from $N$ expert trajectories each with $T_i$ time periods:  $\mathcal{D} = \{(a_{it},x_{it})_{t=0}^{T_i}:i=1,\dots,N\}$ in tabular form. An initial maximum likelihood estimate can be computed by maintaining a table over state-action pair occurrences.

%Lines 8-10 we calculate the gradient for the rewad parameters using the IRL policy, which is estimated from the choice specific value function under maximum entropy distribution. These lines and Algorithm~\ref{algo:forward_pass_algorithm} are explained in \cite{kitani2012activity}.

% We will now look at how using the above formulation for the ex-ante value function \eqref{eq:exante_inversion} avoids us from repeatedly solving the original MDP problem.
% The pseudo-code for CCP-IRL is given in Algorithm~\ref{algo:ccp_irl_algorithm}.
% Notice that the only quantity dependent on $\theta$ in \eqref{eq:exante_inversion} is $\mathbf{R}(a, \theta)$.
% Thus, to estimate $\mathbf{\overline{V}}$ using \eqref{eq:exante_inversion} we calculate $\mathbf{R}(a, \theta)$ for every $\theta$ value \emph{i.e.}, at every step of the iteration (Line 6).
% But the inverse matrix $\left[I-\sum_{a}(\mathbf{S}(a) \lambda) *\left[ \beta \mathbf{T}(a)  \right]\right]^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations.
% This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}. 
% Given this inverse matrix computing the ex-ante value function at any $\theta$ requires simple matrix operations which allows us to avoid solving the MDP using dynamic programming at every step of the iteration.\footnote{
% \cite{hotz2} derive a simulation approach that can avoid matrix inversions. Specifically, it is possible to simulate $S$ action state pairs with $\tau$ periods using the CCPs and the transition probabilities. These paths together with the fact that $\tilde{\epsilon}(a|x_t)$ is only a function of the CCPs, allows us to estimate the value function for a candidate value $\theta$. The simualation results must only be computed once and can be used for every $\theta$ candidate. The value function estimates can then be used to find $\theta$ that matches the data.}
%\textbf{Calculate CCP:} As mentioned previously we can get conditional choice probability estimates using a simple frequency estimator or a Kernel estimator for large state spaces. Given these estimates we can find the reward parameters ($\theta$) using an MLE estimator.

%The main advantage of the above estimation procedure lies in its computation complexity. Recall, that in Maximum Entropy IRL \cite{ziebart2010modeling} we have to solve the value iteration (backwards Dynamic Programming) problem at every iteration. Comparatively in the above formulation we only need to estimate $\tilde{z}(a, x_t)$ once and we can use different estimates of $\theta$ at each iteration. Thus, the above algorithm greatly reduces the computational burden of Maximum Entropy formulation by removing the backwards pass from each iteration.


% =============== CCP (Non-parameteric) ==============

%\subsection{Choice-Specific Value Function Estimation}

%We will now show how the value function is related to the CCPs by introducing the choice-specific value function. Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
%\begin{align}
%V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
%&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) E_{\vec{\epsilon}_{t+1}}
%\max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\}\label{eq:vamax}
%\end{align}
%Define the value function difference for action $a$ and $a_0$ as:
%\begin{align}
%\Delta_a(x)\triangleq V_a(x)-V_{a_0}(x).
%\end{align}
%The base action $a_0$ is selected, one action for each state, for which the immediate reward is normalized to zero. Now add and subtract $V_{a_0}(x_{t+1})$ within the max operator in (\ref{eq:vamax}), introduce the short-hand $T^{x_{t+1}}_{x_t,a}$, and expanding the expectation with TIEV yields:
%\begin{eqnarray}\label{eq:cond_choice_v}
%V_a(x_t)=r(a,x_t)+\beta \sum_{x_{t+1}} T^{x_{t+1}}_{x_t,a} \left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\exp(\Delta_{a'}(x_{t+1}))\right]+\gamma\right]\label{eq:vax}
%\end{eqnarray}
%We now have the choice-specific value function defined in terms of the value function difference $\Delta_{a}$.



%\subsection{Connecting Choice-Specific Value Function to CCPs}
%
%We will now proceed to expand the residual in terms of a ratio of CCPs. Recalling the definition of CCPs and choice-specific value function from Equations (\ref{eq:ccps}) and (\ref{eq:vax}):
%\begin{eqnarray}
%\sigma(a|x_t)=\frac{\exp\left(V_a(x_t)\right)}{\sum_{a'} \exp\left(V_{a'}(x_t)\right)}.
%\end{eqnarray}
%

%Multiplying and dividing by $\exp(-V_{a_0}(x_t))$ leads to
%\begin{eqnarray}
%\sigma(a|x_t)=\frac{\exp(V_a(x_t)-V_{a_0}(x_t))}{\sum_{a'} \exp(V_{a'}(x_t)-V_{a_0}(x_t))}=\frac{\exp(\Delta_a(x_t))}{\sum_{a'} \exp(\Delta_{a'}(x_t))}.
%\end{eqnarray}


%It is clear from the above that value function differences between action $a$ and $a'$ are equal to:
%\begin{eqnarray}
%\frac{\sigma(a'|x_t)}{\sigma(a_0|x_t)}=
%\left[
%\frac
%  {\exp(\Delta_{a'}(x_t))}
%  {\sum_{a''} \exp(\Delta_{a''}(x_t))}
%\right]
%\left[\frac{1}{\sum_{a''} \exp(\Delta_{a''}(x_t))}\right]^{-1}=\exp(\Delta_{a'}(x_t))
%\end{eqnarray}
%%
%
%
%Using the CCP ratio, the value function for the base action $a_0$ can be expressed as:
%\begin{eqnarray}
%V_{a_0}(x_t) 
%= 
%\beta\sum_{x_{t+1}}
%T(x_{t+1}|x_t,a_0) 
%\left[V_{a_0}(x_{t+1})+\ln\left[1+ \sum_{a'\neq a_0}\frac{\sigma(a'|x_{t+1})}{\sigma(a_0|x_{t+1})}\right]+\gamma\right]
%\end{eqnarray}\label{eq:va0}
%
%\subsection{Solving for the Optimal Value Function}

%Since we have expressed the choice-specific (a base action) value function in terms of just the transition function and CCPs, we can now estimate $V_{a_0}$ as a fixed point of the above linear equation -- without the knowledge of the full value function or the reward function. In particular, stacking the value function  $V_{a_0}$ for all states yields a $m_x\times 1$ vector:
%\begin{eqnarray}
%\mathbf{V}_0\equiv \left[\begin{array}{c} V_{a_0}(x_1)\\V_{a_0}(x_2)\\\vdots\\ V_{a_0}(x_{m_x})\end{array}\right]
%\end{eqnarray}
%and $\mathbf{T}$ is the transition matrix with entry $T(x_{j}|x_m,a_0)$ for the $m$th row and the $j$th column. 

%Also define the $m_x\times 1$ vector $\mathbf{\Gamma}$ as:
%\begin{align}
%\mathbf{\Gamma} \equiv \left[\begin{array}{c} 
%\ln(1+\sum_{a'\neq a_0}\sigma(a'|x_1)/\sigma(a_0|x_1))\\
%\vdots\\
%\ln(1+\sum_{a'\neq a_0}\sigma(a'|x_{m_x})/\sigma(a_0|x_{m_x}))\\
%\end{array}\right].
%\end{align}
%
%Then we can write system of equations in matrix form as:
%\begin{align}
%\mathbf{V}_0 = \beta \mathbf{T} [\mathbf{V}_0 + \mathbf{\Gamma}]
%\label{eq:v0_fp}
%\end{align}
%which leads to
%\begin{align}
%\mathbf{V}_0 = [I-\beta \mathbf{T}]^{-1}\beta \mathbf{T} \mathbf{\Gamma}.
%\label{eq:v0_inv}
%\end{align}

%The above is the unique fixed point. The value function for every state can then be estimated as:
%\begin{eqnarray}
%V_a(x_t)=\ln(\sigma(a|x_t))-\ln(\sigma(a_0|x_t))+V_{a_0}(x_t).\label{eq:v_backout}
%\end{eqnarray}
%

%\subsection{Computational Complexity}
%In this way, we are able to avoid the need for dynamic programming and solve for the value function directly by solving the linear system.
%\textcolor{red}{(Matt: Need to explain how this is different than typical linear system formulation for policy evaluation (see http://incompleteideas.net/sutton/book/ebook/node41.html). Usually DP methods are faster than solving the system of $|X|$ linear equations for large $|X|$.)} \textcolor{blue}{(Empirically the CCP IOC is much faster. What is happening here? The state space is not big enough to see the computational cost of the linear system overtake the DP or is the GD outer loop killing MaxEnt IOC? Matt, please feel free to add your analysis and thoughts here.)}
%\textcolor{green}{I think the cost is coming re-solution of the DP at every possible parameter guess. Is that getting down by GD?}
%\textcolor{red}{Matt: MaxEnt IOC essentially has to perform value iteration for each gradient calculation. If I'm interpreting CCP IOC correctly, Eq 27 only needs to be evaluated once for every state (i.e. it avoids the need for an outer loop)?} 
%\textcolor{green}{Matt exactly, no iterations on the value function for every parameter value only once.} 
%\textcolor{red}{(Matt: Got it. Then the computational complexity should be $\mathcal{O}(|X|^3 + |X||A|)$ (first term for solving the linear system in eq 27, second term for eq 28). In CCP IOC, wouldn't using DP for Eq 27 reduce the $|X|^3$ term?)}
%\textcolor{purple}{SR: So this makes sense to me, but doesn't take into account the actual CCP computation -- this assumes we have already computed CCPs and are using those to compute the value function. We need this, plus what it takes to compute CCPs (which should be $\Omega(|X||A|^2NT)$ from the top of my head, because of the kernel smoothing in eq 10)}

%\subsection{Backing Out the Reward Function}

%We can go further to estimate the reward values $r(a, x_t)$ for $a$ and every state $x_t$ without the need for a costly gradient descent algorithm. We can do this by simply backing out the reward values using equation \ref{eq:va}:
%\begin{eqnarray}\label{eq:reward}
%r(a,x_t)
%=
%V_a(x_t)
%- 
%\beta\sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) 
%\left\{
%V_{a_0}(x_{t+1})
%+
%\ln
%  \left(
%    1 + \sum_{a'\in\mathcal{A}}\frac{\sigma(a|x_{t+1})}{\sigma(a_0|x_{t+1})}
%  \right)
%\right\}.
%\end{eqnarray}
%
%This proves that reward functions can be recovered without any further assumptions aside from the existence of one choice generating zero payoffs. 
%
%\textcolor{blue}{(Add an algorithm box !!!)}

%\subsection{Reward Function Transfer}
%The reward function is a vector of payoffs associated with every action at every state. This function can then be transferred to new settings. In order to determine optimal actions in new scenes we must re-sove the optimization problem with the new features and the recovered reward function. Denote the estimated reward function as $\hat{r}(x,a)$ The solution can be found using a slightly modified value function iteration approach using the smoothed value function:
%\begin{eqnarray} \label{eq:exanterust}
%\hat{\overline{V}}(x_t)=\ln\left[\sum_{a\in\mathcal{A}} \exp\left(\hat{r}(x_t,a)+\beta \cdot E_{x_{t+1}|a} \left[
%~\hat{\overline{V}}(x_{t+1})~
%\right]
%\right)\right]+\gamma,
%\end{eqnarray}
%where $\hat{\overline{V}}(x_t)$ is defined as in equation (\ref{eq:def_exante}) substituting the estimated reward function $\hat{r}(x,a)$.
%which defines a smoothed Bellman operator:
%\begin{eqnarray} \label{eq:exanterust}
%\Psi_\rho(\hat{\overline{V}})(x_t)=\rho\ln\left[\frac{1}{\rho}\sum_{a\in\mathcal{A}} \exp\left(\hat{r}(x_t,a)+\beta \cdot E_{x_{t+1}|a} \left[
%~\hat{\overline{V}}(x_{t+1})~
%\right]
%\right)\right]+\gamma,
%\end{eqnarray}
%where $\rho$ is a smoothing parameter. \cite{rust_theory} proves that this mapping is a contraction. Using the above with the estimated reward function it is possible to derive optimal policies for new settings. 
%\subsection{Relaxing Base Action Assumption}
%The CCP approach does not rely on the existence of a base action. There are a number of alternative assumptions that allow us to leverage the CCP representations. For example, if we are willing to make parametric assumptions on the reward function, e.g. linear or non-linear function of state variables, we can relax the base action assumption and allow for non-zero reward. These results are discussed in detail in \cite{magnac}. 
%


\subsection{Complexity Analysis}
The main computation in CCP-IRL is to estimate the inverse matrix in \eqref{eq:exante_inversion}.
% As noted above, estimating this inverse matrix is similar to finding the state visitation frequency for each state. 
In contrast the main computation in MaxEnt-IRL is solving the MDP using dynamic programming.
%Similar computation is also involved when solving the MDP problem using dynamic programming.
However, note that unlike MaxEnt-IRL where we need to \textit{repeatedly} solve the MDP using dynamic programming we only need to estimate the inverse matrix \textit{once}.
Once the matrix inverse has been found estimating the MDP in CCP-IRL involves simple matrix computations and hence involve no significant computation overhead.

Thus, assuming a total of $N$ iterations for the entire MaxEnt-IRL convergence and $T$ iterations for each backwards recursion, MaxEnt-IRL takes a total of $O(N\times T \times|A|\times|S|)$ \cite{ziebart_phd}. For CCP-IRL assuming the matrix inversion can be performed with state of the art matrix inversion method, we get a corresponding runtime of $O(|S|^{2.4}+T\times|A|\times|S|)$. This complexity can be further reduced to $O(T\times|A|\times|S|)$ for linear reward formulations. We derive this formulation in the supplementary section.

We also look at how for large state spaces, we can avoid using matrix inversion and rather estimate the inverse matrix \eqref{eq:exante_inversion} using successive approximations.
Defining $A \equiv \left(I- \beta \sum_{a}(\mathbf{S}(a) \lambda) *\left[ \mathbf{T}(a)  \right]\right)^{-1}$ we can write it as $A = (I - \beta F)^{-1}$ where $F$ is used as a shorthand for notational convenience. Premultiplying both sides with $(I - \beta F)$ we get $(I - \beta F)\times A = I$ which finally gives us $A = I + \beta F A$. We can now use this last equation to estimate the invese matrix $A$ by successive approximations. From a computational perspective this can be much more efficient compared to estimating the inverse directly. Next, we will empirically show the above computational gains in CCP-IRL as well as discuss the expert data requirements for CCP-IRL.


% The main computation in CCP-IRL is to estimate $\tilde{Z}(a, x_t)$ and $\tilde{E}(a, x_t)$. For a finite or infinite horizon model with linear parameters for the reward function, we can implement this using backward recursion similar to Algorithm 2 in \cite{kitani2012activity}. Once $\tilde{Z}$ and $\tilde{E}$ have been estimated, we can calculate the $V(a, x_t, \theta)$ at different parameter values using simple matrix computations. Thus during the entire gradient descent we perform the expensive backwards recursion process only once. Assuming a total of $N$ iterations for the entire MaxEnt-IRL convergence and $T$ iterations for each backwards recursion MaxEnt-IRL takes a total of $O(N\times T \times|A|\times|S|)$ \cite{ziebart_phd}. These are reduced to $O(T\times|A|\times|S|)$ for linearly parameterized CCP-IRL. For non-linear parameterization, to avoid estimating $\tilde{Z}(a, x_t, \theta)$ at every $\theta$ value, we pre-compute the inverse matrix in \eqref{eq:w_inversion_non_linear} once. Thus the main computation again reduces to a non-linear function approximation at every step and a one time matrix inversion. Thus we get a corresponding runtime of $O(|S|^{2.4}+T\times|A|\times|S|)$ as compared to $O(N\times T \times(|A|\times|S|)$ for MaxEnt-IRL.

% Let $m_v$ be the number of steps for the value iteration to converge. Then, \emph{each gradient computation} requires

% Once the MLE is computed, CCP IOC requires a single policy evaluation (Eq. \ref{eq:v_backout}) and a single reward function backout (Eq. \ref{eq:reward}). Policy evaluation can be approached naively by solving the system of $|\mathcal{X}|$ linear equations directly (requiring $\mathcal{O}(|\mathcal{X}|^{2.4})$ time). More typically, this would be solved with fixed point iteration, which requires $\mathcal{O}(|\mathcal{X}|m_p)$, where $m_{p}$ is the number of steps for policy evaluation to converge.  We refer the reader to \cite{Sutton2017} for more details. The reward function backout in Eq. \ref{eq:reward} requires $\mathcal{O}(|\mathcal{X}||\mathcal{A}|)$ time.

% The CCP maximum likelihood estimation depends on the nature of the state and action space. For discrete actions and states, the tabular frequency estimation is linear in the number of observed state-action pairs $\mathcal{O}(NT)$. For the continuous case, this depends on the choice of likelihood function, though for a naive non-parametric MLE, this may require $\mathcal{O}((NT)^2)$ for all observed state-action pairs.

% Compare this to MaxEnt IOC in the discrete case, a form of policy gradient descent for imitation learning, which requires a procedure similar to value iteration for every gradient computation. Let $m_v$ be the number of steps for the value iteration to converge. Then, \emph{each gradient computation} requires $\mathcal{O}(|\mathcal{X}|^2|\mathcal{A}|m_v)$ operations. As we expect $m_v$ and $m_p$ to be similar, a single gradient update of MaxEnt IOC is more computationally complex than the entire CCP IOC algorithm.
% \textcolor{red}{(Someone else familiar with MaxEnt IOC: This is a key take-home message, could you verify it?)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Experiments}

%In this section our aim is to empirically validate the above theoretical results \textcolor{blue}{(KK: To vague. Be clear and upfront.)}. 
In this section we empirically validate,
(1) the computational efficiency of CCP-IRL and (2) the underlying assumptions of consistent CCP estimates \emph{i.e.,} we show the data requirement for CCP-IRL.
To this end we evaluate the performance of CCP-IRL on three standard IRL tasks. Since CCP-IRL is most closely related with traditional MaxEnt-IRL \cite{ziebart}, we use it as a baseline method to compare our results on the benchmark tasks. Previously, both linear \cite{ziebart} \cite{ziebart2010modeling} and non-linear \cite{wulfmeier2015maximum} formulations of MaxEnt-IRL have been used to estimate the reward functions. Hence, we discuss results for both linear and non-linear parameterization of CCP-IRL. For the former, we focus on problems of navigation in a traditional Gridworld setting with stochastic dynamics, while for the latter we choose the Objectworld task as described in \cite{Levine2013}.
% For each experimental scenario, we follow the pattern of: (1) Generate a few demonstration trajectories for the task; (2) Apply IRL (MaxEnt or CCP) algorithm to learn the reward function; and (3) Compare the learned reward function against the true reward. 

For comparative analysis, we use both qualitative and quantitative results.
For qualitative analysis, we directly compare the visualizations of the inferred reward functions for both CCP-IRL and MaxEnt-IRL.
For quantitative comparison, we use negative log likelihood (NLL) \cite{kitani2012activity} and expected value difference (EVD) \cite{levine2011nonlinear} as the evaluation criterion. NLL is a probabilistic comparison metric and evaluates the likelihood of a path under the predicted policy. For a policy ($\pi$), NLL is defined as,
\begin{align}
NLL(\pi) = E_{\pi(a|s)}\big[-\log \prod_{t} \pi(a_{t}|s_{t}) \big]
\end{align}
% Unfortunately, NLL cannot capture the underlying stochastic nature of the expert policy since it assumes deterministic transitions in expert trajectories.
As another metric of success, similar to related works \cite{Levine2013} \cite{wulfmeier2015maximum}, we use expected value difference (EVD). EVD measures the difference between the optimal and learned policy by comparing the value function obtained with each policy under the \textit{true reward} distribution.
Further, to verify the computational improvement using CCP-IRL, we observe the time taken by each algorithm as well as the number of iterations it takes for each algorithm to converge. We show that our algorithm is able to achieve similar qualitative and quantitative performance with \textit{much less} computational time.
%For computational analysis all experiments were run on a PC with an Intel Xeon CPU E5-2660 v3 2.60 GHz (10 cores) processor with an NVIDIA TITAN X (Pascal) GPU.


\subsection{Gridworld: Evaluating Linear Rewards} 

We use the Gridworld experiments to show the computational efficiency of CCP-IRL assuming linear parameterization. We use the Gridworld problem because the reward function is approximately linear. We test with two increasingly difficult settings in Gridworld \emph{i.e.}, Fixed Target and Macro Cells (described below) to show how CCP-IRL provides computational advantage across both tasks. Also, for the more complex Macro-cell task, we show how CCP-IRL requires consistent CCP estimates, which in turn depend on the amount of expert demonstrations available.

\subsubsection{Fixed Target Gridworld}

For our initial experiment we focus on the standard RL task of navigation in a $N \times N$ grid world. We show that CCP-IRL provides a significant computation advantage when compared to MaxEnt-IRL. Additionally, we also show that similar to MaxEnt-IRL, CCP-IRL is able to extract the underlying reward function across large state spaces.

In this setting, the agent is required to move to a specific target location given some obstacles. The initial start location is randomly distributed through the grid. The agent gets a large positive reward at the target location. For states with obstacles, the agent gets a large negative reward. At all other states, the agent gets $0$ reward. The agent can only move in four directions (North, South, East, West) \emph{i.e.}, no diagonal movement is allowed. To make the environment more challenging, we assume stochastic wind, which forces the agent to move to a random neighboring location with a certain probability, $p = 0.3$.
For our feature representation, we use distance to the target location along with the state of each grid cell \emph{i.e.}, whether the grid cell contains an obstacle or not.

First, we compare the EVD performance of our proposed CCP-IRL algorithm against the MaxEnt IRL baseline in Figure \ref{fig:img_maxent_vs_ccp_gridworld} (Right).
As seen in the above plot, both algorithms converge to the expert behavior with similar amount of data. 
Hence our proposed CCP-IRL algorithm is correctly able to infer the underlying reward distribution.% with few expert trajectories.

We now observe the computational gain provided by CCP-IRL. The first three rows in Table \ref{table:table_results_macro_cells} compare the amount of time between CCP-IRL and MaxEnt-IRL for increasing state spaces.
Notice that CCP-IRL is atleast 2$\times$ faster compared to MaxEnt-IRL, for both small and large state spaces. This is expected given that we do not use backwards recursion to solve the MDP problem at every iteration.
% Also recall that feature dimension for each experiment is $N^{2}$. Thus the above results show that the computational advantage of CCP-IRL holds across different size of state spaces as well as feature dimension. 
    
Next, we look at the convergence rate for both algorithms. This is important since CCP-IRL provides much larger computational advantage with increasing number of iterations. 
Figure \ref{fig:img_maxent_vs_ccp_gridworld} (Left) shows the NLL values for both algorithms against increasing number of iterations.
Notice that both algorithms converge to similar result with same number of iterations for different amount of input trajectories. This shows that both algorithms have a similar rate of convergence.
% Figure \ref{fig:img_maxent_vs_ccp_gridworld} (Left) shows that both algorithms converge to same result with similar number of iterations which shows that both algorithms have a similar rate of convergence   . 

%As can be seen in the first figure both algorithms perform similarly (NLL results) across different number of trajectories which is expected given that the task is not very challenging and hence both algorithms are able to . In the next figure \ref{fig:img_maxent_vs_ccp_gridworld}, we show the expected value difference results against the number of trajectories. The EVD value approaches zero with increasing number of demonstrations which indicates that both the algorithms are able to learn the underlying reward function quite well as number trajectories increase.
We also compare the computation time for each algorithm against the discount factor ($\beta$) of the underlying MDP. By varying $\beta$ we are able to vary the complexity of the original MDP since a large $\beta$ value gives more weight to future actions and thus each solution of the value iteration DP takes longer.
Figure \ref{fig:img_gridworld_maxent_vs_ccp_time_discount} shows the computation time for both algorithms against different $\beta$ values. As expected, we see an almost exponential rise in the computation time for MaxEnt-IRL while CCP-IRL shows a negligible time increase which indicates that CCP-IRL provides much larger gains for more complex MDP problems. 

% This shows that for navigation in a gridworld task both CCP-IRL and MaxEnt-IRL are able to infer the underlying reward function with a few expert trajectories.

%As seen above both algorithms perform worse with very little data \textcolor{blue}{(KK: This point about using very little data doesn't fit here. The main point is performance. You can mention this as a secondary point but you need to be careful about the logical structure. Think: What is the  main point I am trying to make through this experiment. And stick to that poin.)} but are able to quickly converge to expert behavior as the number of trajectories increase. This is expected given that the task is not very challenging \textcolor{blue}{(KK: Saying this makes it sound like what you did was a meaningless experiment. Be careful with how you phrase this.)} both CCP-IRL and MaxEnt-IRL are able to infer the underlying reward distribution with few trajectories. However, comparatively CCP-IRL takes larger number of trajectories to converge which is expected since with few trajectories our inital CCP estimates will not be consistent. \textcolor{blue}{(KK: The analysis of this paragraph is so scattered. Select one main point, stick with it and use the data to support your point.)}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/mazeworld/log_likelihood/test_ll_vs_iterations_traj_20_80.pdf}}&
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/mazeworld/evd_maxent_vs_ccp_grid_16_complex_obstacle.pdf}}
    \end{tabular}
    \caption{Left: Log likelihood results for MaxEnt and CCP on fixed target with gridsize of 16 with 20 and 80 trajectories respectively. Right: Expected Value Difference results on fixed target with grid size 16.}
    \label{fig:img_maxent_vs_ccp_gridworld}
\end{figure}
%\begin{figure}[t]
%\centering
  %\begin{tabular}{ccc}
    %\MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/fixed_target_wind_30/reward_map/true_reward.pdf}}&
    %\MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/fixed_target_wind_30/reward_map/maxent_reward.pdf}}&
    %\MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/fixed_target_wind_30/reward_map/ccp_reward.pdf}} \\
    %True Reward & MaxEnt & CCP \\
    %\end{tabular}
    %\caption{ Reward distribution for grid 16 with 20 trajectories. Dark - high reward, Light - low reward. }
    %\label{fig:img_reward_map_gridworld_fixed_target}
%\end{figure}

\begin{table}[t]
\centering
\def\arraystretch{1.0}% 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
N & Cell Size & MaxEnt (sec) & CCP (sec) & Speedup \\\hline

32 & - & 584.31 & \textbf{270.52} & $2\times$ \\
64 & - & 1812.94 & \textbf{552.18} & $3\times$\\
128& - & 15062.24 & \textbf{3119.20} & $5\times$ \\
32 & 8 & 635.63 & \textbf{266.18} & $3\times$ \\
32 & 4 & 584.30 & \textbf{283.81} & $2\times$ \\
64 & 8 & 3224.97 & \textbf{1024.42} & $3\times$ \\
\hline
\end{tabular}
\caption{\textbf{Computation time} (averaged over multiple runs) comparison between MaxEnt and CCP for gridworld settings. Each experiment was run for 50 iterations.}
\label{table:table_results_macro_cells}
\end{table}



\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/macro_cells/grid_16_macro_2_lr_05/ll_ccp_vs_maxent_per_traj.pdf}}&
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/gridworld/macro_cells/grid_16_macro_2_lr_05/evd_multiple_tries.pdf}}
    \end{tabular}
    \caption{Results for gridworld of size 16 with macro-cells of size 2. Left: Minimum NLL results with varying number of trajectories. Right: Expected Value Difference results. For few trajectories CCP-IRL shows much larger variance as compared to MaxEnt-IRL.}
    \label{fig:img_maxent_vs_ccp_gridworld_macro_cell}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{ccc}
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/true_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/maxent_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/gridworld/macro_cells/grid_8_macro_2/reward_map/ccp_reward_2_trim.pdf}} \\
    True Reward & MaxEnt & CCP \\
    \end{tabular}
    \caption{ Reward distribution for macro cells with gridsize 8 and macro cell size 2 using 10 trajectories. Dark - high reward, Light - low reward. }
    \label{fig:img_reward_map_gridworld_macro_cell}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{c}
    \MSHangBox{\includegraphics[width=0.4\textwidth]{images/gridworld/fixed_target_wind_30/timeit_maxent_vs_ccp_grid_32_per_discount.pdf}}
  \end{tabular}
    \caption{Computation time (in seconds and averaged over multiple runs) comparison between MaxEnt and CCP for Gridworld with gridsize of 32. Each experiment was run for 50 iterations. }
    \label{fig:img_gridworld_maxent_vs_ccp_time_discount}
\end{figure}

\subsubsection{Macro Cells}

We use the more complex macro-cell Gridworld environment \cite{abbeel2004apprenticeship} to demonstrate how CCP-IRL's performance depends on expert trajectories.
As discussed above, CCP-IRL requires consistent CCP estimates for reward function estimation. Since CCP estimates are calculated from expert trajectories we observe how CCP-IRL's performance depends on the amount of expert trajectories.
% To highlight the dependence of CCP-IRL on consistent CCP estimates we test its performance in a more complicated Gridworld setting using the

In this setting, the $N \times N$ grid is divided into non-overlapping square regions (macro-cells). Each region contains multiple grid cells and each cell in a region shares the same reward. Similar to \cite{abbeel2004apprenticeship}, for every region we select a positive reward $r \in (0, 1)$ with probability of $0.1$ and $r = 0$ with probability $0.9$. This reward distribution leads to positive rewards in few macro cells which results in interesting policies to be learned and hence requires more precise CCP estimates to match expert behavior.

Since all the cells in the same region share the same reward, our feature representation is a one-hot encoding of the region that cell belongs to \emph{e.g.}, in a grid world of size $N=64$ and macro cell of size 8 we have 64 regions and thus each state vector is of size 64. As before, we assume a stochastic wind with probability $0.3$ and the agent can move only in four directions. 

We analyze the performance of both algorithms given different amounts of expert trajectories.
Figure \ref{fig:img_maxent_vs_ccp_gridworld_macro_cell} compares the NLL and EVD results against increasing number of expert trajectories.
As seen above, both algorithms show poor performance given very few trajectories (< 20 trajectories).
However, with moderate number of trajectories MaxEnt-IRL approaches expert behavior while CCP-IRL is still comparatively worse.
Finally, with sufficiently large number of trajectories (> 60) both algorithms converge to expert behavior. 
CCP-IRL's poor performance with few expert demonstrations reflect its dependence on sufficient amount of input data.
Since CCP estimates are calculated from input data CCP-IRL needs a sufficient (relatively larger than MaxEnt-IRL) amount of trajectories to get consistent CCP estimates.
% This is expected given that we need sufficient amount of trajectories to get consistent CCP estimates. 
% This requirement for more input trajectories can be solved for robotic tasks with autonomous data aquisition.

% These results show that CCP-IRL's performance on the input trajectories to get consistent CCP estimates.  since consistent CCP estimates require sufficient input trajectories 
%of CCPs which in turn are directly inferred from the data.
%which requires a sufficient number of trajectories to arrive at consistent estimates.
%Many robotic tasks use simulations or autonomous techniques to gather data, in which case the requirement for large number of trajectories can be easily satisfied. 

We also qualitatively compare the rewards inferred by both algorithms given few trajectories in 
Figure \ref{fig:img_reward_map_gridworld_macro_cell}. Notice that the darker regions in the true reward are similarly darker for both algorithms. Thus, both algorithms are able to infer the general reward distribution. However, MaxEnt-IRL is able to match the true reward distribution at a much finer level (since less discrepancy compared to the true reward) and hence the underlying policy more closely as compared to CCP-IRL.
Thus, given few input trajectories MaxEnt-IRL performs better than our proposed CCP-IRL algorithm.
% inferred reward function is worse as compared to MaxEnt-IRL.

%Finally we compare the computation cost for both algorithms across large state spaces ($|S| \in \{10^3, 10^4\}$) in Table \ref{table:table_results_macro_cells}.
%As before, CCP-IRL is almost 2x as fast as MaxEnt-IRL across state spaces. Additionally, for the last two rows the only difference in the experiments is in terms of the feature size, since we get similar performance improvement for both settings this indicates that the speedup in CCP-IRL is independent of the dimension of $z(a, x_t)$. This is important since in CCP-IRL the backwards recursion happens on the $z(a, x_t)$ vector while in MaxEnt-IRL it happens on the scalar reward which might give an impression of the former being much more computationally expensive.

We verify the computation advantage for CCP-IRL across large state spaces ($|S| \in \{10^3, 10^4\}$) in Table~\ref{table:table_results_macro_cells}. As seen before, CCP-IRL is atleast 2$\times$ faster than MaxEnt-IRL. Also, its computational efficiency increasing for larger state spaces.
%than MaxEnt-IRL across different sized state spaces.

% Additionally, we also observe the affect of feature dimension on computation complexity. Notice the last two rows of Table \ref{table:table_results_macro_cells} have same state space but different feature sizes. Given these differences we still get similar (~2x) performance improvement for CCP-IRL which indicates that this speedup is largely independent of the dimension of $z(a, x_t)$ \eqref{eq:choice_specific_linear_param}. This is important since in CCP-IRL the backwards recursion happens on the $z(a, x_t)$ vector while in MaxEnt-IRL it happens on the scalar reward which might give an impression of the former being much more computationally expensive.

\begin{table}[t]
\centering
\def\arraystretch{1.4}% 
\begin{tabular}{|c|c|c|c|}
\hline
Experiment Setting & MaxEnt & CCP & Speedup \\\hline

Grid size: 16, C = 2 & 1622.63 & \textbf{296.43} & $5\times$ \\
Grid size: 32, C = 2 & 9115.50 & \textbf{1580.22} & $6\times$ \\
Grid size: 16, C = 8 & 2535.38  & \textbf{545.95} & $5\times$ \\
Grid size: 32, C = 8 & 19445.66 & \textbf{4799.02} & $4\times$ \\
\hline
\end{tabular}
\caption{Computation time (in seconds and averaged over multiple runs) comparison between MaxEnt and CCP for Objectworld. Each experiment was run for the same number of iterations with similar settings. }
\label{table:table_results_objectworld}
\end{table}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.23\textwidth]{images/objectworld/grid_16_object_8/test_ll.pdf}}
    \MSHangBox{\includegraphics[width=0.23\textwidth]{images/objectworld/grid_16_object_8/evd_maxent_vs_ccp.pdf}}
  \end{tabular}
    \caption{Results on ObjectWorld with gridsize of 16 and 2 colors. Left: NLL results on test trajectories. Right: Expected Value Difference results. }
    \label{fig:img_objectworld_maxent_vs_ccp_lr_01}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{cc}
    \MSHangBox{\includegraphics[width=0.22\textwidth]{images/objectworld/transfer/ccp_no_var_maxent_no_var.pdf}}
    \MSHangBox{\includegraphics[width=0.24\textwidth]{images/objectworld/timeit_maxent_vs_ccp_grid_16_per_iter.pdf}}
  \end{tabular}
    \caption{Left: Results for transfer experiment using MaxEnt and CCP formulation on Objectworld with gridsize of 16 and 2 colors. Right: Time variance between MaxEnt-IRL and CCP-IRL with increasing number of iterations. As expected CCP-IRL shows little computation increase with larger number of iterations.}
    \label{fig:img_objectworld_maxent_vs_ccp_time_results}
\end{figure}

\begin{figure}[t]
\centering
  \begin{tabular}{ccc}
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/true_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/maxent_reward_trim.pdf}}&
    \MSHangBox{\includegraphics[width=0.13\textwidth]{images/objectworld/grid_8_object_20_color_5/reward_map/ccp_reward_trim.pdf}} \\
    True Reward & MaxEnt & CCP \\
    \end{tabular}
    \caption{ Reward distribution for Objectworld with gridsize 8 using 30 trajectories and 5 colors. Dark - low reward, Light - high reward. We also plot the inner and outer color of each object. \textit{Pink} - color 1 \textit{Orange} - color 2, other colors are distractors. \textit{Figure best viewed in electronic version.} }
    \label{fig:img_reward_map_objectworld}
\end{figure}

\subsection{Objectworld: Evaluating Non-Linear Rewards}

We now look at CCP-IRL's performance when the true reward function is a non-linear parameterization of the feature vector.
For this, we use the Objectworld \cite{levine2011nonlinear} environment since the reward function is a non-linear function of state features \cite{levine2011nonlinear}.
Similar to related work \cite{wulfmeier2015maximum}, we use a Deep Neural Network (DNN) as the non-linear function approximator.
% Similar to previous works \cite{wulfmeier2015maximum} we use a Deep Neural Network as the non-linear function approximator for the reward function.
As before, we verify both (1) the computational advantage provided by CCP-IRL (DeepCCP-IRL) and (2) the data requirement for CCP-IRL in the above scenario.

The Objectworld environment consists of a grid of $N \times N$ states. At each state the agent can take 5 actions, including movement in 4 directions and staying in place. Spread through the grid are random objects, each with an inner and outer color. Each of these colors is chosen from a set of $C$ colors. The reward for each cell(state) is positive if the cell is within distance 3 of color 1 and distance 2 of color 2, negative if only within distance 3 of color 1 and zero in all other cases. For our feature vector we use a continuous set of values $x \in \mathbb{R}^{2C}$, where $x_i$ and $x_{i+1}$ is the shortest distance from the state to the \emph{i'th} inner and outer color respectively. Since the reward is only dependent on two colors, features for other colors act as distractors. 

We use DeepMaxEnt-IRL \cite{wulfmeier2015maximum} as the baseline, using similar deep neural network architecture for both algorithms. Precisely, we use a 2-layer feed-forward network with rectified linear units. We use the Adam \cite{kingma2014adam} optimizer with the initial learning rate set to $10^{-3}$.

We quantitatively analyze the performance of our proposed DeepCCP-IRL algorithm. 
Figure \ref{fig:img_objectworld_maxent_vs_ccp_lr_01} compares the NLL and EVD results for both algorithms. Notice that as observed before, with few expert trajectories both algorithms perform poorly. However, DeepMaxEnt-IRL matches expert performance with moderate number of trajectories ($\approx 20$), while DeepCCP-IRL requires relatively large number of trajectories ($\approx 40$).
This is expected since CCP-IRL requires larger number of expert trajectories to get consistent CCP estimates.

Also, we qualitatively look at the inferred reward to verify how well the DNN is able to approximate the non-linear reward. Figure \ref{fig:img_reward_map_objectworld} plots the inferred rewards against the true reward function. Notice that both algorithms capture the non-linearities in the underlying reward function and consequently match the expert behavior. Thus, a deep neural network suffices as a non-linear function approximator for CCP-IRL. 

We now analyze the computation gain in the non-linear case. Table \ref{table:table_results_objectworld} shows the computation time for different sized state spaces and different sized feature vectors. Notice that DeepCCP-IRL is almost 5$\times$ as fast as DeepMaxEnt-IRL across small and large state spaces. Thus we see that CCP-IRL provides a much larger computation advantage for the non-linear case, which we believe is because the objectworld MDP problem is more complex than the above grid world experiments. 
This results in both algorithms requiring larger number of iterations until convergence which leads to a large computational increase for DeepMaxEnt-IRL as compared to DeepCCP-IRL. 
This computational increase with larger number of iterations is also shown in Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} (Right).
Notice that as the number of iterations increase, our proposed DeepCCP-IRL algorithms shows minor computational increase as compared to DeepMaxEnt-IRL. 
Thus, for significantly complex MDP problems which require large number of iterations our proposed CCP-IRL algorithm should require much less computation time compared to MaxEnt-IRL.
% This results in each DP step taking longer for Objectworld which leads to large computation time for DeepMaxEnt-IRL.
%Also, as seen in Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} (Right), the computation gain for CCP-IRL holds across different number of iterations. This shows that the performance gain from CCP-IRL is largely independent of the number of iterations required to converge.

% Additionally, to better understand the above computation gain we plot the time taken against number of iterations in figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results}. As expected, DeepCCP-IRL shows negligible increase while DeepMaxEnt-IRL shows an almost exponential increase in computation time. Since DeepMaxEnt-IRL needs to solve the original MDP at every iteration step.

% We next look at the data requirement for DeepCCP-IRL. Notice Figure \ref{fig:img_objectworld_maxent_vs_ccp_lr_01}, as before we see that with fewer trajectories DeepCCP-IRL performs much worse compared to DeepMaxEnt-IRL. As expected this is a direct consequence of not having consistent CCP estimates given the insufficient amount of input trajectories.

% As seen above both DeepMaxent-IRL and DeepCCP-IRL show similar performance for both metrics with increasing number of trajectories. Thus both algorithms are able to recover the underlying reward function given sufficient trajectories. As before we see that with fewer trajectories DeepCCP-IRL performs much worse compared to DeepMaxEnt-IRL which is expected given that we need sufficient data for consistent CCP estimates. Also, Figure \ref{fig:img_objectworld_maxent_vs_ccp_time_results} shows the results for the transfer experiment which reinforce the need for more data for better results.


\section{Related Work \& Discussion}

Most existing work on scaling IRL to large spaces has focused on high dimensional continuous spaces. 
% Continuous spaces
To apply Max-Ent IRL to continuous spaces is challenging. Trivially, we could discretize the continuous space and then apply the traditional MaxEnt-IRL algorithm directly. However, this scales exponentially with respect to the state space and is thus infeasible in practice. More sophisticated algorithms are required to extend MaxEnt-IRL to continuous spaces. 
In \cite{levine2012continuous} the authors use local approximation to the reward function. Giving up on global optimality allows them to scale their algorithm to high dimensional domains. Similarly, Kalakrishnan et~al. \yrcite{kalakrishnan2013learning} use local optimality and path integral for policy improvement to scale IRL to manipulation tasks. Recently, Finn et~al. \yrcite{finn2016guided} combined sample based approximation of MaxEnt-IRL with policy learning under unknown dynamics for high dimensional manipulation tasks.

Alternately, some IRL algorithms have focused on a smaller subset of MDP problems. Todorov \yrcite{todorov2007linearly} introduced a new class of MDPs which are linearly solvable. In \cite{dvijotham2010inverse}, the authors present an efficient IRL algorithm for these MDPs. However, this new class of MDP's only solve a restricted form of general MDP formulation and thus might not be applicable in all scenarios. 
%In contrast, our work makes no such approximation of the general MDP framework and is thus applicable everywhere.

% In contrast our work focuses on the simpler discrete setting but with much larger state space.

In contrast to previous work our CCP-IRL algorithm makes no assumption on the MDP framework and is thus generally applicable.
Also, in this work we focus on discrete problem settings, to introduce the key insights from the CCP framework in a well understood paradigm.
However, the approach is applicable to situations with continuous state spaces, continuous actions, non-stationary or finite horizon settings.
% complexity is based on giving up on global optimality and rather focusing on local optimality.
Further, given the complexity of applying MaxEnt-IRL to continuous spaces it is not yet clear how can CCP-IRL be effectively applied to high-dimensional continuous control tasks in robotics. We leave this for future work. 

% CCP framework helps in the unidentification of the reward formulation
% CCP formulation helps in using other empirial models developed by economists e.g. what if we use a different form of human optimality.

We believe our work provides an important connection between economics and robotics.
We now discuss some of extensions of the CCP framework which are relevant in robotics.  
The CCP framework provides methods to investigate the well known under-identification problem in IRL. CCP approach clarifies necessary assumptions to ensure a ``unique" mapping from trajectories to the reward functions \cite{magnac}. CCP approach is also used in multi-agent settings where strategic situations involve multiple equilibria, this often necessitates some equilibrium selection rules. With CCPs and a dataset on a single path of play, it is possible to estimate rewards without having to solve the game and to correctly select the equilibrium in the data \cite{pese}.

Finally, we look at the data requirement for CCP-IRL. A complete theoretical analysis of sample complexity of CCPs is beyond the scope of our initial work. However, we would like to refer the reader to Aguirregabiria and Mira \yrcite{aguirregabiria2002swapping} which discuss some sample properties of CCP estimators.
More importantly, the authors show that one-step CCP estimators (such as CCP-IRL) can have large bias with insufficient data. 
To reduce this bias they introduce a successive iteration procedure which results in a sequence of estimators. They further show that this iteration until convergence will realize the original MaxEnt-IRL estimator.

\section{Conclusion}

% Edit later... place holder

We have described an alternative framework for inverse reinforcement learning (IRL) problems that avoids value function iteration or backward induction. In IRL problems, the aim is to estimate the reward function from observed trajectories of a Markov decision process (MDP). We first analyze the decision problem and introduce an alternative representation of value functions due to \cite{hotz}. These representations allow us to express value functions in terms of empirically estimable objects from action-state data and the unknown parameters of the reward function. We then show that it is possible to estimate reward functions with few parametric restrictions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \clearpage


\bibliography{references}
\bibliographystyle{icml2018}

% \clearpage

% ==== Mohit's initial derivation ====

% \subsection{Hotz-Miller's CCP Method}

% Our aim in Inverse Reinforcement Learning is to find the parameterized reward function $r(\theta)$ for the given MDP/R. We will now show how we can leverage the non-parameteric estimates of choice conditional probabilities to efficiently estimate the parameters($\theta$) of the reward function for both linear and non-linear parameterization. 
% Denote the choice-specific value function as the value associated with action $a$ excluding $\epsilon_a$ as:
% \begin{align}\label{eq:vamax}
% \begin{split}
% V_a(x_t) &\triangleq r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \overline{V}(x_{t+1}) \\
% %&=r(a,x_t)+\beta \sum_{x_{t+1}\in\mathcal{X}}T(x_{t+1}|x_t,a) \\
% %& \qquad \times E_{\vec{\epsilon}_{t+1}} \max_{a'}\left\{V_{a'}(x_{t+1})+\epsilon_{t+1})\right\} \\
% &=r(a, x_t) + \sum_{j=1}^{T-t}\beta^{j}E_{x_{t+j}|a_t,x_t} \\
% & \qquad \big[r(x_{t+j}, a_{t+j})+ E_{\epsilon_{t+j}|a_t, x_t}\left[\epsilon(x_{t+j}, a_{t+j})\right] \big]
% \end{split}
% \end{align}

% \textbf{Linear Parameters:} Many traditional IRL algorithms such as, \cite{abbeel2004apprenticeship} \cite{ziebart} assume that the reward is a linear combination of known features $z$. Assuming this we will now show how we can efficiently estimate $V_a(x_t)$ for different values of $r(\theta)$.

% For linear parameterization we can write $r(a, x_t) = z(a, x_t)^T\theta$, where $z(a, x_t)$ are the set of basis functions (features) known to us.
% Notice from \eqref{eq:vamax}, $V_a(x_t)$ is nothing but the expected and discounted sum of future rewards and shock values respectively. Thus we can split \eqref{eq:vamax} into two parts.
% Define $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$ as the expected and discounted sum of current and future $z$ values and $\epsilon$ values respectively. Given this we can break \eqref{eq:vamax} into,
% \begin{align}\label{eq:choice_specific_linear_param}
% \begin{split}
% V_a(x_t) & = \tilde{z}(a, x_t)^T \theta + \tilde{e}(a, x_t) \\
% \tilde{z}(a, x_{t}, \theta) & = z(a, x_{t}) + \sum_{j=1}^{T-t}\beta^{j} E_{x_{t+j}|a,x_t} \\
% & \qquad \times \left[ z\left(\pi^{*}\big( x_{t+j}, \epsilon_{t+j}, \theta \right), x_{t+j} \big) \right] \\
% \end{split}
% \end{align}
% where $\pi^{*}(x_{t}, \epsilon_{t}, \theta)$ is the optimal policy. We will now show that both $\tilde{z}$ and $\tilde{e}$ need to be estimated only once.
% To achieve this notice that, we can get consistent estimates of $\pi^{*}(x_t, \epsilon_t, \theta)$ from CCPs, hence we can write \eqref{eq:choice_specific_linear_param} as,
% \begin{align}\label{eq:z_tilde_ccp}
% \begin{split}
%   \tilde{z}(a, x_{t}, \theta) & = z(a, x_t) + \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
%   & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}) z(a_{t+j}, x_{t+j}) \right] \\
%   \end{split}
% \end{align}
% We can very similarly write the above expression for $\tilde{e}(a, x_t, \theta)$ replacing $z(a, x_t)$ with $E[\epsilon_t(a)|x_t, a]$. For notational convenience we define $e(a, x_t) \equiv E[\epsilon_t(a)|x_t, a]$,
% \begin{align}\label{eq:e_tilde}
% \begin{split}
%   \tilde{e}(a, x_{t}, \theta) & = \sum_{j=1}^{T-t}\beta^{j} E_{x_{t+j}|a,x_t} \\
%   %& \qquad \times \left[ e\left(\pi^{*}\big( x_{t+j}, \epsilon_{t+j}, \theta \right), \epsilon_{t+j} \big) \right] \\
%   %& = \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
%   & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}) e(a_{t+j}, x_{t+j}) \right] \\
%   \end{split}
% \end{align}

% \cite{hotz} show that $e(a, x_t)$ depends on conditional choice probabilities and distribution of $\epsilon$ only. Further, they prove that the mapping between CCPs and choice specific value function is invertible. Using this inverse mapping and assuming TIEV distribution for $\epsilon$, we get $e(a, x_t) = \gamma - \log P(a|x_t)$.

% Thus, both $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$ depend on CCP values and transition probabilities only. As a result we can efficiently estimate $V_a(x_t)$ for different values of $\theta$ from \eqref{eq:choice_specific_linear_param} using simple matrix computations. The only real computation involved is in calculating $\tilde{z}(a, x_t)$ and $\tilde{e}(a, x_t)$, both of which need to be done only once. Compare this to \cite{ziebart} where $V_a(x_t)$ needs to be calculated via expensive DP at every $\theta$ value. 
% %These can be estimated by solving the DP problem using backwards recursion from \eqref{eq:z_tilde_ccp} and \eqref{eq:e_tilde} respectively, which is similar to Algorithm 9.1 in \cite{ziebart_phd}.

% We will now show that the above estimation procedure can be simplified into a fixed point iteration algorithm. Let,
% \begin{align}\label{eq:z_tilde_matrix}
% Z(x) = \sum_{a}P(a|x)\tilde{z}(a, x)
% \end{align}
% this allows us to rewrite \eqref{eq:vamax} as,
% \begin{align}\label{eq:z_tilde_from_Z}
% \tilde{z}(a, x) = z(a, x) + \beta\sum_{x'}T(x'|a, x)Z(x')
% \end{align}

% Multiplying the above expression with $P(a|x)$ and summing over all actions we get,
% \begin{align}\label{eq:z_tilde_recursion}
% \begin{split}
% \sum_{a}P(a|x)\tilde{z}(a|x) &= \sum_{a}P(a|x) \\
% & \times \left\{z(a, x) + \beta\sum_{x'}T(x'|a,x)Z(x') \right\}
% \end{split}
% \end{align}

% The LHS above is the definition of $Z(x)$ \eqref{eq:z_tilde_matrix}. To get a closed form expression we stack up \eqref{eq:z_tilde_recursion} for all different values of $x$. For this we define, 
% $\mathbf{P}(a)$ as the vector for CCPs $\{P(a|x), x \in X\}$, similarly $\mathbf{z}(a)$ is $\{z(a, x), x \in X\}$ and $\mathbf{T}_{x'|x}(a)$ is the transition probability matrix for each 
% action i.e., $T(x'|a, x) \equiv \mathbf{T}_{x'|x}(a)[x', x]$.

% \begin{align}\label{eq:Z_defn}
% \mathbf{Z} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{z}(a) + \beta \mathbf{T}_{x'|x}(a)\mathbf{Z}\right\}
% \end{align}

% We can get a closed form solution for $e(a, x)$ using the same formulation as above, which gives us,
% \begin{align}\label{eq:E_defn}
% \mathbf{E} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{e}(a) + \beta \mathbf{T}_{x'|x}(a)\mathbf{E}\right\}
% \end{align}

% For brevity, we combine $\mathbf{Z}$ and $\mathbf{E}$ together into $\mathbf{W} \equiv \{[\mathbf{Z}(x),  \mathbf{E}(x)], x \in X \}$. Thus we get,
% \begin{align}\label{eq:w_recursion}
% \mathbf{W} = \sum_{a}\mathbf{P}(a) \cdot \left\{ [\mathbf{z}(a), \mathbf{e}(a)] + \beta \mathbf{T}_{x'|x}(a)\mathbf{W}\right\}
% \end{align}

% In the above equation \eqref{eq:w_recursion} the only unknown is $\mathbf{W}$, which can be solved in closed form as,

% \begin{align}\label{eq:w_inversion}
% \begin{split}
% \mathbf{W} &= \left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1} \\
% & \qquad \times \sum_{a}\mathbf{P}(a) \cdot [\mathbf{z}(a), \mathbf{e}(a)]
% \end{split}
% \end{align}

% We can solve for $\mathbf{W}$ either via fixed point iteration \eqref{eq:w_recursion} or via the inversion defined in \eqref{eq:w_inversion}.
% Given $\mathbf{W}$ we can trivially estimate $\tilde{z}$ using \eqref{eq:z_tilde_from_Z} (and similarly $\tilde{e}$).  

% \textbf{Non-Linear Parameters:} Many recent IRL algorithms have shown that linear parameterization is insufficient to learn complex reward functions \cite{levine2011nonlinear} \cite{wulfmeier2015maximum}. Thus in this section we focus on efficiently estimating $V_a(x_t)$ based on non-linear parameterization of $r$ from known set of features.
% Our aim here is to show how to efficiently compute $\mathbf{W}$ \eqref{eq:w_inversion}, which can then be used to efficiently estimate $\tilde{z}$ and $\tilde{e}$ and hence $V_a(x_t)$, as shown before.

% In the non-linear formulation, the rewards are defined as $r(a, x_t) = z(a, x_t, \theta)$ instead of $r(a, x_t) = z(a, x_t)^T\theta$. Notice that the only change is in the parameterization of $z$. Thus the choice specific value function \eqref{eq:choice_specific_linear_param} needs to be rewritten as,
% \begin{align}\label{eq:choice_specific_non_linear_param}
% V_a(x_t, \theta) = \tilde{z}(a, x_t, \theta) + \tilde{e}(a, x_t). 
% \end{align}
% Also, notice that the above change does not affect $\tilde{e}(a, x_t)$ and consequently the expression for $\mathbf{E}$ \eqref{eq:E_defn} remains unchanged. However, \eqref{eq:z_tilde_matrix} and \eqref{eq:z_tilde_from_Z} need to be rewritten to include the non-linear parameterization as,
% \begin{align}
% \begin{split}
% Z(x, \theta) &= \sum_{a}P(a|x)\tilde{z}(a, x, \theta) \\
% \tilde{z}(a, x, \theta) &= z(a, x, \theta) + \beta\sum_{x'}T(x'|a, x)Z(x', \theta)
% \end{split}
% \end{align}
% %Notice, that the only difference between \eqref{eq:choice_specific_linear_param} and \eqref{eq:choice_specific_non_linear_param} is in terms of $\tilde{z}$. In the former expression it is a known set of features ($r(a, x_t) = z(a, x_t)^T\theta$) while in the latter case it is a scalar value($r(a, x_t) = z(a, x_t, \theta)$), estimated as a non-linear function of the features. 
% %\begin{align}\label{eq:z_tilde_non_linear}
% %\begin{split}
% % \tilde{z}(a, x_{t}, \theta) & = r(a, x_t, \theta) + \sum_{j=1}^{T-t} \beta^{j} E_{x_{t+j}|a_t=a, x_t} \\
% %   & \qquad \left[ \sum_{a_{t+j}} P(a_{t+j}|x_{t+j}, \theta) \tilde{z}(a_{t+j}, x_{t+j}, \theta) \right] \\
% %\end{split}
% %\end{align}

% %Thus $\tilde{z}(a, x)$ is a scalar quantity now since $r(a, x_t, \theta)$ is scalar. Apart from the above changes the other formulations are still valid for the non-linear case as well. Thus we can rewrite \eqref{eq:w_recursion} as
% %\begin{align}
% %W = \sum_{a}\overbar{P}(a) \cdot \left\{ [\overbar{r}(a, \theta), \overbar{e}(a)] + \beta \overbar{T}_{x'|x}(a)W \right\}
% %\end{align}
% %
% %where we emphasize that $\overbar{r}(a, \theta)$ is the vector of rewards for each state and a function of $\theta$. From this we still get,
% Thus following the same process as before for \eqref{eq:Z_defn} we get, 
% \begin{align}\label{eq:Z_defn_non_linear}
% \mathbf{Z}_{\theta} = \sum_{a}\mathbf{P}(a) \cdot \left\{ \mathbf{z}(a, \theta) + \beta \mathbf{T}_{x'|x}(a)\mathbf{Z}_{\theta}\right\}
% \end{align}
% where we have added a subscript $\theta$ to $\mathbf{Z}$ to indicate dependence. Combining $\mathbf{Z}_{\theta}$ and $\mathbf{E}$ together into $\mathbf{W}_{\theta}$ as before, we get \eqref{eq:w_recursion} and thus finally,
% \begin{align}\label{eq:w_inversion_non_linear}
% \begin{split}
% \mathbf{W}_{\theta} &= \left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1} \\
% & \qquad \times \sum_{a}\mathbf{P}(a) \cdot [\mathbf{z}(a, \theta), \mathbf{e}(a)]
% \end{split}
% \end{align}

% Notice that to estimate $\mathbf{W}_{\theta}$ using \eqref{eq:w_inversion_non_linear} we need to calculate $\mathbf{z}(a, \theta)$ at every step of the iteration. But the inverse matrix $\left( \mathbf{I} - \beta \sum_{a}\mathbf{P}(a) \cdot \mathbf{T}_{x'|x}(a) \right)^{-1}$ is independent of $\theta$ and hence can be pre-computed once for all iterations. This inverse matrix computes the state visitation frequency for each state, weighted by the appropriate discount factor and hence encompasses a large part of calculations involved in MaxEnt \cite{ziebart_phd}. 
% Thus, similar to the linear case we can efficiently estimate $W_{\theta}$ for different values of $\theta$ by simple matrix computations.

\end{document}
